Okay, I need to score the provided annotation result against the groundtruth according to the given criteria. Let me start by understanding each component: data, analyses, and results. Each has structure, completeness, and accuracy scores.

First, let's tackle the "data" object. 

**Structure (10 points):**
Looking at the groundtruth and the annotation's data sections. Both have an array of objects with keys like id, omics, link, format, source, public_id. The structure seems consistent between both. But wait, in the annotation's data[2], there's an empty string for "omics", which might be okay since the structure still has the key. All required keys are present even if some values are empty. So maybe full 10 points here?

Wait, looking closer: The groundtruth's data entries have all keys present, even if values are empty strings. The annotation's data also has all keys. So structure is correct. So Structure score: 10/10.

**Content Completeness (40 points):**
Groundtruth has four data entries (data_1 to data_4). The annotation also has four data entries. Now check if each sub-object exists. 

- data_1: Groundtruth has omics="RNA-seq data", but in annotation it's empty. However, the question says that similar but not identical sub-objects might count. Wait, but the sub-object here is about the data entry itself. The presence of the sub-object is what matters for completeness. Since the ID matches (data_1 is present), even if the content is incomplete, the sub-object exists. So all four sub-objects are present? Let me check:

In the annotation's data array:
- data_1 exists
- data_2 exists
- data_3 exists
- data_4 exists

So all four sub-objects are present. So no deduction for missing. But wait, the groundtruth's data_3 has omics="Metabolomics data", but in the annotation's data_3, omics is empty. But for completeness, the presence of the sub-object (by ID) is what's important, not the content. So completeness is full? 

Wait, but the problem states "sub-objects in annotation that are similar but not identical may still qualify". Hmm. Wait, actually, the completeness is about having the sub-object present. So as long as the sub-object (with the same ID?) isn't missing, then it's counted. Wait, but the IDs in the groundtruth are unique, so maybe the IDs are just identifiers and not part of the content. The task says not to use IDs to assess consistency, focus on content. 

Wait, the user instruction says: "data_id or analysis_id are only unique identifiers... scoring should focus on the sub-objects content, rather than using IDs to assess consistency." So the IDs don't matter; presence of the same content counts. 

Therefore, for completeness, we need to check whether all the sub-objects from the groundtruth are present in the annotation, considering their content. Wait, this complicates things. Because the groundtruth has data_1 with omics="RNA-seq data", but the annotation's data_1 has omics empty. Are these considered the same sub-object? Or do they need to match in content?

Hmm, the problem says "sub-objects in annotation that are similar but not totally identical may still qualify as matches". So the key is whether the sub-objects are semantically equivalent. But how?

Alternatively, perhaps each sub-object in the groundtruth must be represented in the annotation. For example, the groundtruth has data_1 as RNA-seq data. If the annotation's data_1 has omics empty, but other fields might have different info, does that count as a missing sub-object? Or is it considered present because the ID exists, but the content is wrong?

This is confusing. Let me re-read the instructions.

Under content completeness: "Deduct points for missing any sub-object. Note: Sub-objects in annotation result that are similar but not total identical to groundtruth may still qualify as matches. Thoroughly analyze each groundtruth sub-object's semantic correspondence before determining annotation result equivalency."

Ah, so the existence is determined by semantic correspondence. So even if the ID is different, but the content corresponds, it's okay. However, in the case of the data, the IDs in the groundtruth are data_1 to data_4, and the annotation also has those IDs. So for each groundtruth data sub-object, we need to see if there's an annotation sub-object that matches semantically.

Let's go through each groundtruth data entry:

1. Groundtruth data_1: omics="RNA-seq data". In the annotation's data_1: omics is empty. The other fields like link, format, source, public_id in groundtruth are empty, but in the annotation, data_1 has link filled, format="Raw metabolome data", etc. But the omics field is critical here. Since the omics type is RNA-seq vs. empty, maybe this doesn't match semantically? Or is the key that the sub-object represents the same data, even if some fields are missing?

Wait, perhaps the "omics" field is crucial here. If the groundtruth's data_1 is RNA-seq, and the annotation's data_1 has omics empty, then they aren't semantically equivalent. Therefore, the annotation's data_1 is not equivalent to the groundtruth's data_1, so the sub-object is missing? That would mean the annotation lacks the RNA-seq data entry, thus deducting points.

But this is getting complicated. Alternatively, maybe the structure requires that all data entries from groundtruth must be present in the annotation. Let me think again. 

Alternatively, the data in groundtruth has four entries, and the annotation also has four entries. So in terms of count, they match. But if the content of each sub-object in the annotation doesn't correspond to the groundtruth's sub-objects, then each non-corresponding one counts as missing?

This is tricky. Let me approach step by step:

Groundtruth data entries:

1. data_1: RNA-seq (omics)
2. data_2: Proteomics
3. data_3: Metabolomics
4. data_4: Genomics

Annotation data entries:

1. data_1: omics=""
2. data_2: Proteomics (same as GT)
3. data_3: omics=""
4. data_4: omics=""

Wait, in the annotation, data_4's omics is empty? Looking back at the input:

In the annotation's data_4, "omics": "" yes. So the omics field is empty except for data_2, which matches groundtruth's data_2 (Proteomics).

Therefore, the annotation's data_1 (omics empty) doesn't correspond to groundtruth's RNA-seq. Similarly, data_3's omics is empty instead of Metabolomics, and data_4's omics is empty instead of Genomics. Only data_2 matches.

Thus, out of four sub-objects, only data_2 is correctly present (since its omics is same). The others are missing because their omics fields are blank, making them not semantically equivalent. Therefore, the annotation is missing three sub-objects (data_1, data_3, data_4). Each missing sub-object would deduct (40 /4=10 points per missing). Wait, but the total points for completeness is 40. How exactly is deduction done?

The instruction says: "Deduct points for missing any sub-object. Note: ...". So for each missing sub-object from groundtruth, deduct points. Here, the annotation has four sub-objects, but three of them don't correspond to groundtruth's, so effectively missing three? 

Alternatively, maybe the presence of a sub-object with different content doesn't count, hence each missing correct one deducts (40 /4)*number_missing. 

If there are four in groundtruth, and only one matches (data_2), then three are missing. So 3*(40/4)= 3*10=30 points deducted. So 40-30=10 points for completeness?

Wait but that's harsh. Alternatively, maybe the points are prorated per sub-object. Each sub-object contributes 10 points (since 40 divided by 4). For each missing sub-object, deduct 10. Here, three are missing, so 30 off, leaving 10. But maybe the deduction is per missing, but the penalty depends on how many are missing. Alternatively, maybe each missing sub-object takes away 10 points (since 40 total). 

Alternatively, perhaps the completeness score is calculated as (number of correctly present sub-objects / total in groundtruth) *40. 

So if only data_2 is correct (1 out of 4), then 1/4 *40 =10. So that would be 10/40. That's possible. 

Alternatively, maybe each sub-object must be fully present (all key-value pairs correct?), but that's part of accuracy. Wait no, completeness is about presence, not correctness. 

Hmm, the problem states: "Content completeness accounts for 40 points: deduct points for missing any sub-object. Note: Sub-objects in annotation result that are similar but not total identical to groundtruth may still qualify as matches. ..."

Thus, if a sub-object in the annotation is semantically equivalent to the groundtruth's, it counts. If not, then it's missing. 

In the data section:

- Groundtruth data_1: RNA-seq. Annotation data_1 has omics empty. Not semantically equivalent. So this is missing.
- data_2: both Proteomics. Matches. Present.
- data_3: Groundtruth is Metabolomics, Annotation has omics empty. Not equivalent. Missing.
- data_4: Groundtruth is Genomics, Annotation has omics empty. Not equivalent. Missing.

Thus, only data_2 is present. So three are missing, so deduct 3*(40/4) = 30 points. Completeness score: 10/40.

But wait, the note says "similar but not total identical may still qualify". Maybe the annotation's data_1 could still be considered present if other fields match? For instance, maybe the link or source fields indicate that it's RNA-seq data even if omics is empty. Let me check the actual data:

Looking at the annotation's data_1: link is "https://www.icvaddcos.net...", format is "Raw metabolome data", source is GEO, public_id lUPCm2cSYO. 

Groundtruth data_1 has link empty, format empty, source empty, public_id empty. 

Hmm, maybe the "omics" field is key here. Without that, it's hard to say it's RNA-seq. So probably not considered equivalent. 

Therefore, data's completeness score is 10/40.

**Accuracy (50 points):**

Now, for accuracy, we look at the sub-objects that were counted as present (i.e., semantically matched). Only data_2 is present. 

For data_2 in groundtruth: omics is "Proteomics data", link empty, format empty, source empty, public_id empty.

In the annotation's data_2: omics is "Proteomics data", link empty, format empty, source empty, public_id empty. 

All key-value pairs match except maybe formatting (like "Proteomics data" vs "Proteomics data"? Same). So for data_2, all key-values are correct. So accuracy for data_2 is 50*(1/4?) Wait, no. 

Wait, the accuracy is per sub-object. Each sub-object's key-value pairs must be accurate. The total accuracy points (50) are distributed across all the present sub-objects. 

Wait, the instruction says: "For sub-objects deemed semantically matched in the 'Content Completeness' section, deductions are applied based on discrepancies in key-value pair semantics."

Since only data_2 is semantically matched, we evaluate its key-value pairs.

Groundtruth data_2: all fields except omics are empty. The annotation's data_2 has omics correct, and the other fields are also empty. So all key-values are correct. Thus, for data_2, there's no deduction. 

So the accuracy score for data is 50*(1/1) = 50, since only one sub-object was considered present and it's accurate. Wait but total points for accuracy are 50, so if the sub-object is fully accurate, then full 50?

Wait, no. The total accuracy score for the entire object (data) is 50 points. The accuracy is evaluated for all the sub-objects that passed completeness. 

Each sub-object contributes equally. So if there are N sub-objects in completeness, each gets (50/N) points. Any discrepancies in their key-values reduce that portion.

Here, only data_2 is present (N=1), so the entire 50 is based on that. Since it's accurate, full 50. 

Wait, but maybe the accuracy is per sub-object. Let me read again:

"For sub-objects deemed semantically matched in the 'Content Completeness' section, deductions are applied based on discrepancies in key-value pair semantics."

So for each such sub-object, check its key-values. The total accuracy score is 50, so if there are multiple sub-objects, each contributes to the score. 

In this case, since only data_2 is present, and it's perfect, the accuracy is 50/50.

Thus, data's accuracy is 50.

Total data score: 10 (structure) + 10 (completeness) +50 (accuracy)= 70. Wait, but structure was 10, completeness 10, accuracy 50 → total 70/100. That seems possible.

Wait but let me confirm the structure: structure was 10/10. Yes. 

Now moving on to analyses.

**Analyses Component**

**Structure (10 points):**

Groundtruth's analyses have each analysis with id, analysis_name, analysis_data (array of data_ids). The annotation's analyses also have the same keys. 

Check if all sub-objects have the right structure. The groundtruth's analysis_6 appears twice? Wait in groundtruth's analyses array, the last two entries have the same id "analysis_6" but different analysis_names. That might be an error in groundtruth, but the structure is still correct as each sub-object has the required keys. 

The annotation's analyses have entries like analysis_3, analysis_4, etc., but some have empty analysis_name and analysis_data. For example, analysis_3 has analysis_name as empty and analysis_data as empty (the groundtruth's analysis_3 has analysis_data as ["data_1"]). 

However, the structure is correct if all keys exist. The annotation's analyses all have id, analysis_name, analysis_data (even if analysis_data is an empty string instead of array? Wait in the annotation's analysis_3, analysis_data is written as "analysis_data": "", which is a string, not an array. That's a structural error. 

Wait looking at the input for the annotation's analyses:

"analyses": [
    {
      "id": "analysis_1",
      "analysis_name": "genomic analysis",
      "analysis_data": [ "data_4" ]
    },
    {
      "id": "analysis_2",
      "analysis_name": "Protein expression analysis",
      "analysis_data": [ "data_2" ]
    },
    {
      "id": "analysis_3",
      "analysis_name": "",
      "analysis_data": "" // THIS IS A STRING NOT ARRAY
    },
    ... others similar
]

So analysis_3 has analysis_data as a string "" instead of an array. This is a structural error. Similarly, analysis_4,5,6 have analysis_data as empty strings. 

Thus, the structure is not correct for those entries. 

How many sub-objects are in analyses?

Groundtruth has 7 analyses (analysis_1 to analysis_6, but analysis_6 is duplicated with two different names). The annotation also has 7 analyses (analysis_1 to analysis_6, with analysis_6 appearing twice? Let me check:

Annotation's analyses list:

analysis_1, analysis_2, analysis_3, analysis_4, analysis_5, analysis_6, analysis_6 (so two analysis_6 entries). So 7 entries, same as groundtruth.

But for structure, each sub-object must have analysis_data as an array. The first two (analysis_1 and 2) have arrays, but analysis_3 has a string, which is invalid. Similarly, the rest (analysis_4,5,6) have analysis_data as empty string. So those are structural errors.

So out of 7 sub-objects in the analyses array, 5 have incorrect structure (analysis_3 to analysis_6, except analysis_2 is okay?). Wait analysis_2 is okay? analysis_2 has "analysis_data": ["data_2"], which is correct. So only analysis_1 and 2 have correct structure. The remaining 5 entries (analysis_3,4,5,6,6) have invalid analysis_data types (string instead of array). 

Therefore, structure is flawed for 5/7 sub-objects. 

The structure score is 10 points total. How much to deduct?

Since structure is about the entire object's structure, but the problem says "structure should focus on the correct JSON structure of each object and proper key-value pair structure".

Possibly, each sub-object must have the correct structure. If any sub-object has incorrect structure, the structure score is reduced. 

Perhaps each sub-object contributes to the structure score. Total 10 points, divided by 7? Or maybe deduct 2 points per structural error beyond the first?

Alternatively, maybe the entire analyses object's structure is considered. Since most sub-objects have correct keys but some have wrong data types for analysis_data, which is a key part of the structure, leading to significant deduction. 

This is ambiguous. Let me think of the worst case. If half the sub-objects have structure issues, maybe half the points deducted. 

Alternatively, since analysis_data is supposed to be an array, but in some cases it's a string, that's a major structural flaw. So perhaps deduct 5 points (half of 10). So structure score would be 5/10.

Alternatively, maybe each sub-object with structural error deducts 1 point. 5 errors → deduct 5 → 5/10.

I'll go with 5/10 for structure.

**Content Completeness (40 points):**

Groundtruth analyses have 7 sub-objects. The annotation also has 7, but need to check semantic equivalence.

Groundtruth's analyses:

analysis_1: genomic analysis (data_4)

analysis_2: Protein expression analysis (data_2)

analysis_3: Transcriptomic analysis (data_1)

analysis_4: whole genome expression analysis (data_1)

analysis_5: Proteomics analysis (data_2)

analysis_6: protein-protein interaction networks analysis (data_2)

analysis_6 (duplicate id): whole genome miRNA profiling analysis (data_1)

Wait, in groundtruth, analysis_6 is listed twice with different names. That might be an error in groundtruth, but we proceed.

The annotation's analyses:

analysis_1: genomic analysis (data_4) – matches groundtruth analysis_1.

analysis_2: Protein expression analysis (data_2) – matches analysis_2.

analysis_3: analysis_name is empty, analysis_data is empty (invalid).

analysis_4: analysis_name empty, analysis_data empty.

analysis_5: same.

analysis_6: two instances with analysis_name empty and analysis_data empty.

So the annotation's analyses beyond the first two are empty. 

We need to determine if any of the annotation's analyses correspond semantically to groundtruth's beyond the first two.

For example, the third analysis in groundtruth is analysis_3: Transcriptomic analysis (data_1). The annotation's analysis_3 has name empty. Does that count as missing? 

Yes, because the semantic content (name and data references) is absent. 

Similarly, analysis_4 (whole genome expression) in GT has no corresponding in annotation. 

So the annotation only has two valid analyses (analysis_1 and 2), while groundtruth has seven. 

Therefore, five analyses are missing (groundtruth's analyses 3,4,5,6,6). 

Each missing analysis deducts (40/7)*number_missing. 

Calculating:

Number missing: 7-2=5.

(40 /7)*5 ≈ 28.57 points deducted. So 40 -28.57≈11.43. Rounded to nearest integer? Let's say ~11 or 12. But since points are whole numbers, maybe 11.4 → 11. 

Alternatively, maybe each missing sub-object deducts 40/7 ≈5.71 per missing. 5 missing → 28.57 total deduction, so 11.43 left. 

But perhaps the system expects integer points. Let's approximate as 11 points.

Alternatively, maybe the completeness is (number of present analyses / total in GT)*40. 

Present in annotation: 2 (analysis1 and 2). So 2/7 *40 ≈11.4. So 11 points.

**Accuracy (50 points):**

Only the first two analyses (analysis1 and analysis2) are semantically present. 

Check their accuracy:

For analysis1 (genomic analysis, data_4):

Groundtruth's analysis1 has analysis_data ["data_4"], which matches the annotation's analysis1's analysis_data ["data_4"]. The name is correct. So accurate.

Analysis2: Groundtruth's analysis2 has name "Protein expression analysis", data_2. The annotation's analysis2 matches exactly. So accurate.

Thus, both are accurate. 

Total accuracy score: 

Each of the two sub-objects contributes equally to the 50 points. So (2/2)*50 =50. Wait but total points for accuracy are 50. Since both are accurate, full 50.

Wait but how? Since there are two sub-objects contributing, each would get 25 points (50/2). Both are correct → 25+25=50. Correct.

Thus, Accuracy is 50.

Total analyses score: structure 5 + completeness ~11 + accuracy 50 → total 66. But let me confirm calculations again.

Completeness: If the completeness score is 11.43, rounded to 11, then total would be 5+11+50=66. 

Alternatively, maybe the completeness is considered as follows:

Each missing analysis deducts 40/7 per missing. So:

Total completeness points: 40 - (5*(40/7)) ≈ 40 - 28.57 ≈11.43 → ~11.

Thus, total analyses score is 5 +11 +50 =66. 

Moving on to Results.

**Results Component**

**Structure (10 points):**

Groundtruth results are an array of objects with analysis_id, metrics, value, features (array). 

Annotation's results:

[
    {
      "analysis_id": "",
      "metrics": "average prediction accuracy",
      "value": -432,
      "features": ""
    },
    {
      "analysis_id": "",
      "metrics": "recall",
      "value": -3663,
      "features": ""
    },
    {
      "analysis_id": "",
      "metrics": "p",
      "value": -5903,
      "features": ""
    }
]

Each sub-object has analysis_id (empty), metrics (filled), value (numbers), features (empty). 

The structure requires that each sub-object has analysis_id, metrics, value, features (as array). 

In the annotation, features is an empty string instead of an array (e.g., "features": ""), which is a structural error. Also, analysis_id is empty but that's content, not structure. 

So each sub-object in results must have analysis_id (any value is okay), metrics, value, features as array. 

The features field in the annotation are strings instead of arrays. Thus, all three sub-objects have structural errors in features. 

Additionally, analysis_id being empty is content, not structure. 

Thus, the structure is flawed for all three sub-objects due to features being strings. 

Each sub-object's structure is invalid (since features is not an array). Thus, all three have structural issues. 

So structure score: 0/10? Because all three sub-objects have incorrect structure. Or maybe partial credit? 

Alternatively, if the main structure (keys present) is okay but data types are wrong, maybe half points. 

But the structure requires proper key-value pair structures. Since features should be an array but is a string, this is a structural issue. Thus, deduct full points for structure? Or per sub-object?

Assuming each sub-object contributes to the structure score, with 10 points total. If all three are invalid, maybe 0/10.

Alternatively, the keys are present but the data types are wrong. Since the structure includes correct data types (arrays for features), this is a major flaw. So structure score 0.

Wait, let me check the groundtruth's results:

Groundtruth results have features as arrays (e.g., ["SOD1", ...]). So in the annotation, features are strings, which is invalid. Thus, all three sub-objects have wrong structure for features. 

Thus, structure score: 0.

**Content Completeness (40 points):**

Groundtruth results have three sub-objects, each linked to specific analysis_ids (analysis_1, analysis_5, analysis_5). The annotation's results have three entries but with analysis_id empty. 

Semantic equivalence requires that the sub-objects correspond to groundtruth's. 

Groundtruth's first result is linked to analysis_1, second to analysis_5, third also to analysis_5. 

In the annotation, all analysis_ids are empty. 

To determine if the sub-objects are present, we need to see if their content (metrics, value, features) corresponds semantically to any in groundtruth. 

Groundtruth's results:

Result1: analysis_1, metrics "", value "", features [list of genes].

Result2: analysis_5, metrics "", value "", features [TDP-43...]

Result3: analysis_5, metrics "", value "", features [IL-10...]

In the annotation, the results have:

Result1: analysis_id "", metrics "average prediction accuracy", value -432, features ""

Result2: analysis_id "", metrics "recall", value -3663, features ""

Result3: analysis_id "", metrics "p", value -5903, features ""

These metrics like "average prediction accuracy", "recall", "p" are different from groundtruth's empty metrics. Features are empty strings instead of arrays. 

Are any of these semantically equivalent to groundtruth's results?

Probably not. The groundtruth's results have features lists related to genes/proteins, whereas the annotation's have numerical values and different metrics. 

Thus, none of the annotation's results correspond to groundtruth's. So all three sub-objects are missing. 

Deducting for missing: Groundtruth has 3 sub-objects. The annotation has 3 but none match. Thus, all three are missing. 

Completeness score: 0/40? 

Because (number of present sub-objects /3 )*40. Present is zero. 

**Accuracy (50 points):**

No sub-objects were deemed present (semantically matched), so accuracy score is 0. 

Total results score: structure 0 + completeness 0 + accuracy 0 → 0/100.

Wait but maybe the presence of the sub-objects (even if not semantically matched) affects something? No, because completeness requires semantic match. Since none are matched, accuracy doesn't apply. 

Thus, Results score is 0.

Final Scores:

Data: 70 (10+10+50)

Analyses: 66 (5+11+50) 

Results: 0 

Wait let me verify again:

Data:

Structure: 10

Completeness: 10 (because only data_2 is present, 1 out of 4 → 10)

Accuracy:50 (only data_2 is accurate)

Total: 70.

Analyses:

Structure:5 (deducted 5 for structural errors in 5 out of 7 sub-objects)

Completeness: approx 11 (since 2 out of 7 → ~11)

Accuracy:50 (both analysis1 and 2 are accurate)

Total: 5+11+50=66.

Results: 0.

Hence the final scores would be:

{
  "Final Scores": {
    "Data": 70,
    "Analyses": 66,
    "Results": 0
  }
}

Wait but maybe I made mistakes in calculations. Let me cross-check:

For Analyses' completeness: 2/7 of 40 is (2/7)*40 ≈11.428, which rounds to 11.

Structure for analyses: 5 points (since 5 out of 7 had structural issues, assuming 5 points off from 10).

Yes.

Results: All structure wrong (0), completeness 0, accuracy 0 → 0 total.

Yes.

So the final scores are as above.
</think>

"Final Scores": {
    "Data": 70,
    "Analyses": 66,
    "Results": 0
}

**Scoring Breakdown:**

### **Data Component**
- **Structure (10/10):** All sub-objects have correct keys (`id`, `omics`, `link`, `format`, `source`, `public_id`). The structural format aligns with the groundtruth.
- **Content Completeness (10/40):** Only `data_2` (Proteomics data) is semantically equivalent to the groundtruth. The other three sub-objects lack semantic correspondence (e.g., missing `omics` fields), resulting in deductions for three missing sub-objects (3 × 10 = 30 points deducted).
- **Content Accuracy (50/50):** The single valid sub-object (`data_2`) has fully accurate key-value pairs.
- **Total:** 10 + 10 + 50 = **70/100**

---

### **Analyses Component**
- **Structure (5/10):** Five sub-objects (`analysis_3`–`analysis_6`) have invalid `analysis_data` formats (strings instead of arrays). Two sub-objects (`analysis_1`, `analysis_2`) are structurally correct, leading to a deduction of 5 points.
- **Content Completeness (~11/40):** Only two sub-objects (`analysis_1`, `analysis_2`) match the groundtruth. Five sub-objects are missing (5 × ~5.7 points deducted ≈ 28.57 points lost), leaving approximately **11 points**.
- **Content Accuracy (50/50):** The two valid sub-objects (`analysis_1`, `analysis_2`) are fully accurate.
- **Total:** 5 + 11 + 50 = **66/100**

---

### **Results Component**
- **Structure (0/10):** All three sub-objects have invalid `features` formats (strings instead of arrays).
- **Content Completeness (0/40):** None of the sub-objects semantically match the groundtruth (missing analysis IDs, mismatched metrics/features).
- **Content Accuracy (0/50):** No valid sub-objects to evaluate.
- **Total:** 0 + 0 + 0 = **0/100**

---

### Final Scores:
```json
"Final Scores": {
    "Data": 70,
    "Analyses": 66,
    "Results": 0
}
```