Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component: Data, Analyses, and Results. Each has Structure (10 points), Content Completeness (40 points), and Content Accuracy (50 points). 

Starting with the **Data** section.

**Groundtruth Data** has four entries:
1. data_1 (proteomic)
2. data_2 (phosphoproteomic)
3. data_3 (WES)
4. data_4 (RNA-seq)

**Annotation Data** has three entries:
1. data_2 (same as GT)
2. data_3 (same as GT)
3. data_4 (changed omics term to "Gene expression profiles" and source is TCGA instead of GSA, public_id missing)

First, check **Structure (10 points)**:
All sub-objects have the required keys: id, omics, link, format, source, public_id. However, in the annotation's data_4, "public_id" is empty. But the structure is present, so full 10 points here? Wait, maybe the keys exist even if value is empty. Yes, structure is about presence, not content. So Structure gets full 10.

**Content Completeness (40 points)**:
GT has 4, Annotation has 3. Missing data_1 (proteomic) which is a sub-object. So -10 (since 40/4=10 per item). Also, the third entry in annotation (data_4) is present but with different content. Since the user says similar but not identical might count. But since the omics type differs (RNA-seq vs Gene expression), maybe it's considered extra? Hmm, the problem states that missing sub-objects lead to deduction, and extra might too. Wait, the instruction says "extra sub-objects may also incur penalties depending on contextual relevance". 

Wait, the user says "sub-objects in annotation similar but not identical to groundtruth may still qualify as matches. ... determine equivalency based on semantic correspondence." 

The original data_4 in GT is RNA-seq data from GSA with public_id HRA002195. The annotation's data_4 is "Gene expression profiles" from TCGA with no public_id. Are these semantically equivalent? Maybe not exactly. RNA-seq is a type of gene expression profiling, but the source and ID differ. So perhaps this is an extra sub-object, not replacing the original. Thus, the annotation is missing the original data_4 (RNA-seq) and added a new one (Gene expression). 

Therefore, missing data_1 and data_4 (original RNA-seq) would mean two missing sub-objects? Wait, the annotation's data_4 replaces the GT's data_4 but isn't equivalent. So they are missing data_1 and data_4 (the RNA-seq). The third entry in annotation is an extra but not a replacement. So total missing are two: proteomic (data_1) and RNA-seq (data_4). 

Each missing sub-object deducts 10 (since 40/4=10 per item). So 20 points off. Then, the extra data_4 (gene expr) might add a penalty. The user says "extra sub-objects may also incur penalties depending on contextual relevance". Since this is an incorrect entry (not present in GT), maybe another 10? But I'm not sure. Alternatively, the extra is counted as non-matching, so not penalizing beyond missing. Maybe only the two missing items, so 40 -20 =20? 

Wait, the instructions say "deduct points for missing any sub-object". Extra sub-objects might not be penalized unless they are not semantically aligned. Since the third entry in the annotation is an extra, not present in GT, so maybe the completeness score is based on how many GT sub-objects are covered. 

So the GT has 4 data entries. The annotation covers two (data_2 and 3), and has an extra. So the number of correctly present sub-objects is 2 out of 4. Therefore, completeness is 2/4 *40=20? Or per missing each, minus 10 each. So missing two (data_1 and data_4), so -20. So completeness score would be 40-20=20?

Wait, but maybe the third entry (data_4 in annotation) is considered an attempt to replace the GT's data_4, but since it's not semantically equivalent, then it doesn't count. So missing data_1 and data_4 (original), so two missing. Hence 40 - (2*10)=20.

**Content Accuracy (50 points)**: Only for the sub-objects that are present and semantically equivalent. 

Looking at the existing matches:

- data_2: same as GT, so all key-values correct except maybe public_id? In GT, public_id is IPX0002796001, and in annotation it's same. So full marks here. 

- data_3: same as GT in all fields except maybe public_id? GT has OEP001784, annotation same. All correct here. 

So those two are accurate. The other entries (missing data_1 and incorrect data_4) aren't counted here because they are either missing or not semantically equivalent. 

Each of the two correct sub-objects contribute to the accuracy. There are two correct, so (2/4)*50? Wait, no. Accuracy is per matched sub-object. The accuracy part is 50 points total for all correct key-value pairs in the matched sub-objects. 

Wait, the accuracy section says: For sub-objects deemed semantically matched in completeness, check their key-values. 

For the two matched sub-objects (data_2 and data_3):

data_2: all keys match exactly except maybe formatting. For example, "public_id" is correct. So 100% accuracy here (all keys correct).

data_3: same, all keys correct except maybe link? Both have biosino.org/node link. So all correct. 

So each contributes fully. Since there are two sub-objects contributing to accuracy, but how much does each contribute? The total possible is 50 points. 

Alternatively, for each key in the matched sub-objects, check if they are correct. Let's see:

Each sub-object has 6 keys (id, omics, link, format, source, public_id). 

For data_2:

- id: correct (though in GT it's data_2, but the content is same so ID difference doesn't matter).
- omics: phosphoproteomic matches.
- link: correct.
- format: "raw data" vs "raw data" (matches).
- source: iProx Consortium matches.
- public_id: matches.

All correct. So full 50 for data_2?

Similarly for data_3:

All keys correct. 

But wait, the accuracy is summed across all key-value pairs in all matched sub-objects. Each sub-object has 6 keys. 

Total keys in both sub-objects: 6 +6 =12. All correct, so 50/50.

Wait, but maybe the total accuracy points are allocated per sub-object's correctness. Since there are two sub-objects, each contributes (6/6)* (portion of total 50). Since there were 4 GT sub-objects, each originally counts for (50/4)*something. Hmm, perhaps the approach is:

Total accuracy score is based on the percentage of correct keys among all keys in the matched sub-objects. 

Alternatively, for each matched sub-object, check if all keys are correct. If yes, then full points for that sub-object's contribution. Since there are two such sub-objects, each worth (50/4)*number? Not sure. 

Alternatively, the total possible 50 points are distributed based on how many key-value pairs are correct in the matched sub-objects. 

Let me think again: the accuracy section evaluates the accuracy of the matched sub-objects (those considered present in completeness). The total 50 points are for all key-value pairs in those matched sub-objects. 

In the case of data_2 and data_3, each has 6 key-value pairs. Total 12 pairs. All are correct, so 12/12 = 100%, so 50 points. 

But maybe the 50 points are split per sub-object. For example, each sub-object contributes (50 / total GT sub-objects) * some factor. Wait, the instructions say "for sub-objects deemed semantically matched... deductions based on discrepancies". So perhaps each sub-object's keys are evaluated, and the total points depend on the proportion of correct keys across all keys in all matched sub-objects. 

Since both data_2 and data_3 are fully correct, their keys are all correct. So 50 points here. 

Thus, Data's accuracy is 50. 

So Data total: Structure 10 + Completeness 20 + Accuracy 50 → 80? Wait, no. Wait, the completeness was 20 (40-20). So total Data score is 10+20+50=80?

Wait, let me recalculate:

Structure: 10 (full)

Completeness: 40 - (2 missed sub-objects *10 each) → 20

Accuracy: 50 (since all keys in the two matched sub-objects are correct)

Total: 10+20+50=80. 

Now moving to **Analyses**.

Groundtruth has 6 analyses:

1. analysis_1 (WES analysis, data_3)
2. analysis_2 (proteomic analysis, data_1)
3. analysis_3 (Phosphoproteomic, data_2)
4. analysis_4 (diff expr, depends on analysis_2 and 3)
5. analysis_5 (Pathway enrich, depends on analysis_4)
6. analysis_6 (Survival, depends on 2 and 3)

Annotation's analyses:

1. analysis_3 (Phospho, data_2)
2. analysis_4 (diff expr, data_2 and analysis_3 [wait, analysis_2 is not present])
Wait, looking at the annotation's analyses:

analysis_4's analysis_data is ["analysis_2", "analysis_3"], but in the annotation, analysis_2 is not listed. Wait, the user's input shows that in the annotation analyses, there are three entries:

analysis_3, analysis_4, analysis_6. 

Wait, looking at the annotation's analyses array:

[
    {id: analysis_3, ...},
    {id: analysis_4, analysis_data: ["analysis_2", "analysis_3"]},
    {id: analysis_6, ...}
]

So analysis_2 is referenced in analysis_4's analysis_data but isn't present in the annotation's analyses array. That could be a problem.

First, checking **Structure (10 points)**:

Each analysis sub-object must have id, analysis_name, analysis_data. Some may have label. 

In the annotation's analyses:

analysis_3: has all required keys (id, name, analysis_data).

analysis_4: same, plus label.

analysis_6: same, analysis_data is array of analyses.

No missing keys, so structure is okay. So full 10 points.

**Content Completeness (40 points)**:

GT has 6 analyses; annotation has 3. Missing analyses_1, 2,5. 

So missing 3 sub-objects → 3*10 (since 40/6≈6.66 per item?), but actually since 40 points for 6, each is 40/6 ≈6.666 per sub-object. 

Wait, the instructions say "deduct points for missing any sub-object". Each missing sub-object reduces the completeness score. 

Each sub-object's absence deducts (40/total_GT_sub_objects) per missing. 

Total GT analyses:6. Each sub-object is worth 40/6 ≈6.666 points. 

Missing 3 sub-objects (1,2,5): 3*(40/6) = 20 points lost. 

Thus completeness score: 40 - 20 = 20.

However, we need to check if any of the annotation's analyses are semantically equivalent but named differently. 

Looking at the annotation's analyses:

analysis_3 is same as GT's analysis_3 (Phosphoproteomic analysis linked to data_2).

analysis_4 is same as GT's analysis_4 (diff expr analysis using analysis_2 and 3). But in the annotation, analysis_2 is missing, so maybe they didn't include analysis_2 but still have analysis_4? 

Wait, the analysis_4 in annotation references analysis_2 which isn't present. But in the groundtruth, analysis_4 uses analysis_2 and 3, which are present in GT. 

So the analysis_4 in the annotation is incomplete because analysis_2 isn't present. Does that affect completeness? The completeness is about whether the sub-object exists, not its internal links. So analysis_4 itself is present, so it's counted. 

The missing sub-objects are analyses 1,2,5. 

Additionally, the annotation includes analysis_6 (same as GT's analysis_6?), yes: GT's analysis_6 is Survival analysis using analyses 2 and 3, which in the annotation is also referencing analysis_2 and 3. But analysis_2 isn't present in the annotation. However, the existence of analysis_6 in the annotation is counted, so it's present. 

Wait, the GT analysis_6 exists, and the annotation has analysis_6, so it's present. 

Wait, the GT analyses are 1-6. The annotation has 3,4,6. So missing 1,2,5. So indeed 3 missing. 

So completeness is 40 - (3)*(40/6) = 20.

Now, **Content Accuracy (50 points)**:

Only the three present analyses (3,4,6) are considered. 

Check each for their key-value accuracy.

Starting with analysis_3 (Phosphoproteomic analysis):

In GT: analysis_3's analysis_data is data_2 (which is present in the annotation as data_2). So in annotation's analysis_3, analysis_data is "data_2", which matches. So all keys here are correct. 

Analysis_4 (diff expr analysis):

GT's analysis_4's analysis_data is [analysis_2, analysis_3]. The annotation's analysis_4's analysis_data is ["analysis_2", "analysis_3"]. However, analysis_2 is not present in the annotation's analyses array. But the key here is the correctness of the analysis_data pointers. Even though analysis_2 isn't present, the pointer is correct (assuming analysis_2 is supposed to exist). Wait, but in the annotation's analyses list, analysis_2 isn't there. So in the annotation's analysis_4, the analysis_data refers to analysis_2 which isn't present. That's an error in the analysis_data pointing to a non-existent sub-object. 

Is this a content accuracy issue? Because the analysis_data is pointing to a missing analysis_2. 

This would make the analysis_data incorrect. So in analysis_4's analysis_data, there's an invalid reference (analysis_2 is missing). 

So this key (analysis_data) is incorrect here. 

Other keys: analysis_name matches "differential gene expression analysis". 

Label's group is correct ("tumor", "NAT"). 

So analysis_4 has one incorrect key (analysis_data includes analysis_2 which isn't present). 

How many keys in analysis_4? The keys are id, analysis_name, analysis_data, label. 

So the analysis_data is wrong (because it references analysis_2 which isn't there). So that's one incorrect key out of 4. 

Wait, the keys required are id, analysis_name, analysis_data. Label is optional? Not sure. 

Assuming all keys except maybe label are mandatory. 

Alternatively, the analysis_data's validity (pointing to existing analyses) is part of the accuracy. 

This is tricky. The content accuracy is about the key-value pairs' semantic correctness. Since analysis_data references a non-existent analysis_2, this is an error. 

Thus, analysis_4 has one incorrect key (analysis_data). 

Next, analysis_6:

GT's analysis_6 has analysis_data as ["analysis_2", "analysis_3"], and in the annotation's analysis_6, it's the same. However, analysis_2 is missing. So similarly, analysis_6's analysis_data is pointing to analysis_2 which is not present in the annotations. So this is also an error. 

Thus, analysis_6's analysis_data is incorrect. 

Now, let's break down each analysis's accuracy:

Analysis_3:

All correct (name, data points to existing data_2, etc.). So all keys correct. 

Analysis_4:

analysis_data is incorrect (references analysis_2 which is missing). The rest are correct (name, label). Assuming label is part of the keys. 

So 1 incorrect key out of total keys (let's assume 4 keys: id, name, data, label). So 3/4 correct. 

Analysis_6:

analysis_data references analysis_2 (invalid), but also analysis_3 (valid). So the array includes an invalid reference. Thus, analysis_data is incorrect. The name is correct. 

Assuming keys are id, name, data. So data is wrong. 

Thus, analysis_6 has 2/3 keys correct. 

Calculating accuracy points:

Total possible keys across all matched analyses:

Analysis_3: 3 keys (id, name, analysis_data, maybe label?) – let's clarify the keys. 

Looking at GT's analysis_4, it has id, analysis_name, analysis_data, label. So each analysis has up to 4 keys. 

Assuming all analyses must have at least id, analysis_name, analysis_data, and possibly label. 

For each analysis:

Analysis_3:

Keys: id, analysis_name, analysis_data (correct), and no label in GT (since in GT's analysis_3 has no label, but annotation's analysis_3 also has none? Let me check:

In the groundtruth, analysis_3 doesn't have a label. The annotation's analysis_3 also doesn't have a label. So all keys correct. 

Analysis_3 has 3 keys (id, name, data) → all correct. 

Analysis_4:

Has id, name, analysis_data (incorrect due to analysis_2), and label (correct). So 3 correct keys (id, name, label) out of 4. 

Analysis_6:

id, name, analysis_data (incorrect). So 2/3 keys correct (if label not present). Looking at GT's analysis_6, it doesn't have a label. The annotation's analysis_6 also doesn't have a label. So analysis_6 has 2 correct (id, name) and data wrong. So 2/3 keys correct. 

Total keys across all three analyses:

Analysis_3: 3 keys (all correct → +3)
Analysis_4: 4 keys (3 correct → +3)
Analysis_6: 3 keys (2 correct → +2)
Total correct keys: 3+3+2=8
Total possible keys: 3+4+3=10

Wait, perhaps better to calculate per analysis's contribution. 

Each analysis's accuracy is based on its own keys. 

Each analysis's maximum points would be (number of keys in that analysis) * (50/total_GT_analyses). 

Alternatively, since the total accuracy is 50 points for all matched analyses, we need to compute the percentage of correct keys among all keys in matched analyses. 

Total correct keys:

Analysis_3: 3 keys (all correct) → 3/3 =100%
Analysis_4: 3 correct out of 4 → 75%
Analysis_6: 2 out of 3 → ~66.67%

Total correct keys: 

Analysis_3: 3
Analysis_4: 3 (out of 4)
Analysis_6: 2 (out of 3)

Total correct keys: 3+3+2 =8
Total possible keys: 3+4+3=10 → 8/10 = 80% → 0.8 *50 =40 points.

Alternatively, maybe each analysis is worth a portion of the 50, based on the number of keys. 

Alternatively, each key's correctness contributes to the total. 

Either way, the accuracy score would be around 40. 

So analyses accuracy is 40.

Thus, Analyses total:

Structure 10 + Completeness 20 + Accuracy 40 = 70.

Moving to **Results**.

Groundtruth Results:

There are four entries, each with analysis_id linking to an analysis, and features.

GT Results:

1. analysis_1 → features [KRA, TP53,...]
2. analysis_4 → long list
3. analysis_5 → HIPK2 etc.
4. analysis_6 → ENO1 etc.

Annotation's Results:

[
    {analysis_id: analysis_1 (but in GT analysis_1's data is data_3 which is present in the annotation's data), features match GT's first result. 
    },
    {analysis_4 → same features as GT},
    {analysis_5 → metrics has "accuracy" (GT has empty), features are different (g481wt etc. vs HIPK2 etc.)},
    ?

Wait, the annotation's results array has three entries. Let me look again:

Annotation's results:
[
    analysis_1 (features match GT's first),
    analysis_4 (features match GT's second),
    analysis_5 (has metrics="accuracy", features are different),
    — missing the analysis_6 result (fourth in GT)
]

Wait, the fourth result in GT is analysis_6, which is present in the annotation's analyses. But in the annotation's results, there is no entry for analysis_6. 

So Groundtruth has four results; annotation has three (missing the fourth, which is analysis_6's features). 

Also, the third entry in annotation is analysis_5, but in GT analysis_5's features are HIPK2, ROCK1 etc., but the annotation's analysis_5 has features like g481wt which don't match. 

**Structure (10 points)**:

Each result sub-object needs analysis_id, metrics, value, features. 

Checking the annotation's results:

All have these keys. Even though metrics and value are empty in some cases. The structure requires the keys to exist, regardless of content. So structure is okay. 10 points.

**Content Completeness (40 points)**:

GT has 4 results. Annotation has 3 (missing analysis_6's result). 

Additionally, the third entry in the annotation is analysis_5, which exists in GT. But the features are different. However, the question is whether the sub-object (result entry) is present. Since analysis_5's result is present in the annotation (even with incorrect features), it's counted as present. 

Wait, the completeness is about presence of the sub-object, not the content. So:

GT results:

1. analysis_1 (present in annotation)
2. analysis_4 (present)
3. analysis_5 (present)
4. analysis_6 (missing in annotation)

Thus, one missing (analysis_6's result). So deduction: 40/4 =10 per missing. So 40 -10=30.

But wait, the annotation has three entries, missing one (analysis_6). So deduction of 10. 

However, check if the analysis_5's result is considered semantically equivalent. The sub-object (result for analysis_5) exists, so it's present. The features being wrong affects accuracy, not completeness. 

Thus completeness is 30 (40-10). 

**Content Accuracy (50 points)**:

Evaluate the three present results (analysis_1,4,5). 

Analysis_1:

Features match exactly (same list). Metrics and value are empty in both → correct. So all keys correct. 

Analysis_4:

Same features as GT. Metrics and value empty → correct. 

Analysis_5:

Features are completely different (g481wt vs HIPK2 etc.), so features are wrong. Metrics has "accuracy" instead of empty → incorrect. Value is empty, which matches GT (empty). 

Thus:

For analysis_5:

- analysis_id: correct (points to analysis_5)
- metrics: wrong (should be empty)
- value: correct (empty)
- features: wrong. 

So 2 correct keys out of 4 (analysis_id and value). 

The other two keys (metrics, features) are incorrect. 

Analysis_1 and 4 are fully correct. 

Calculating accuracy:

Total keys across all three results:

Each result has 4 keys (analysis_id, metrics, value, features).

Total keys: 3*4=12. 

Correct keys:

Analysis_1: 4/4
Analysis_4:4/4
Analysis_5:2/4 → total correct:4+4+2=10 → 10/12 ≈83.33%. 

Thus, accuracy score: (10/12)*50 ≈41.66 → approx 42 points. 

But since points are whole numbers, maybe 41 or 42. Let's say 41. 

Alternatively, per sub-object contributions:

Each result's accuracy contributes to the total 50. 

Analysis_1 and 4 are fully correct (each contributes 4 keys), so 8/12 keys correct, plus analysis_5's 2 → total 10/12. 

So 50*(10/12)=41.66 ≈42. 

Alternatively, maybe each result is worth (50/4)* something. 

Alternatively, per result:

Each result's maximum contribution is (number of keys) * (portion of total 50). 

Total keys across all GT results: 4 results ×4 keys =16 keys. 

Each key is worth (50/16) points. 

Correct keys:

Analysis_1:4 → 4×(50/16)=12.5
Analysis_4:4 → same →12.5
Analysis_5:2 →2×3.125=6.25
Total: 12.5+12.5+6.25=31.25 → which seems low. 

Hmm conflicting methods. The instruction says "for matched sub-objects… deductions based on discrepancies in key-value semantics".

Perhaps better to consider each sub-object's accuracy as the proportion of correct keys in that sub-object, then sum them weighted by their importance. 

Alternatively, each sub-object's keys are evaluated, and the total is the sum of correct keys divided by total keys multiplied by 50. 

Using that method: 

Total correct keys:10 out of 12 → ~83.33% → 41.66, round to 42.

So accuracy is 42.

Thus Results total:10+30+42=82? 

Wait:

Structure:10

Completeness:30 (40-10)

Accuracy:42

Total: 10+30=40 +42=82.

Wait, 10+30+42=82.

Wait, but let me check again:

The missing result for analysis_6 deducts 10 from completeness (40→30). 

Accuracy is 42. 

Total:82. 

But let me recheck the features in analysis_5:

In GT, analysis_5's features are ["HIPK 2","ROCK 1","PRKCD","MAPKAPK 2"]

Annotation's analysis_5 has ["g481wt","VnXE94","ztI5s"] → none match. So features are entirely wrong. 

Metrics: GT has "", annotation has "accuracy" → mismatch. 

Value: both empty. 

So in analysis_5, two keys (metrics and features) wrong. 

Thus, two incorrect keys out of four. So 2 correct keys (analysis_id and value). 

Total keys in all three results: 3×4=12 → correct keys:4+4+2=10 → 10/12 → ~41.66. 

Thus, rounding to 42. 

Putting it all together:

Data: 80

Analyses: 70 (10+20+40?)

Wait earlier calculation for analyses accuracy was 40, so total analyses: 10(structure)+20(completeness)+40(accuracy)=70.

Results: 82. 

Wait, but let me confirm again:

For Analyses:

Structure:10

Completeness: 20 (40-20)

Accuracy:40 

Total 10+20+40=70. 

Yes.

So final scores:

Data: 80

Analyses:70

Results:82?

Wait, but in Results' accuracy calculation, I had 42, leading to 82 total. 

Wait, structure 10, completeness 30, accuracy 42 → total 10+30+42=82. 

Thus:

Final Scores: {"Data":80, "Analyses":70, "Results":82}

Wait, but let me double-check each section once more.

**Data Section Recheck:**

Structure:10 (OK)

Completeness: GT had 4, annotation had 2 correct (data_2 and 3). Missing data_1 and data_4 (original RNA-seq). 

Thus, 2 missing → deduction 20 → 20. 

Accuracy: both correct entries (data_2 and 3) have all keys right → 50. 

Total 80. Correct.

**Analyses:**

Structure:10. 

Completeness: GT had 6, annotation had 3 (missing 3 → deduction 3*(40/6)=20 → 20 left). 

Accuracy: The three analyses (3,4,6) have total keys 3+4+3=10 keys. Of which 8 correct → 8/10=80% → 40. 

Thus 40 accuracy. 

Total 70. 

**Results:**

Structure:10. 

Completeness: missing 1 → deduction 10 → 30. 

Accuracy: 10/12 keys → ~41.66 → 42. 

Total 10+30+42=82. 

Thus the final scores should be as above. 

But let me see if there are any other deductions I missed. 

In Analyses' analysis_4 and analysis_6, the analysis_data references analysis_2 which is missing. Is that affecting the accuracy? 

Yes, in analysis_4's analysis_data, since it references a missing analysis_2, the key analysis_data is incorrect. Similarly for analysis_6. 

In analysis_4, the analysis_data is ["analysis_2", "analysis_3"]. Since analysis_2 isn't present in the analyses array, this is an error. 

Thus, analysis_data key is incorrect. 

Similarly, analysis_6's analysis_data is ["analysis_2", "analysis_3"], also incorrect. 

Thus, in analysis_4 and 6, the analysis_data key is wrong. 

Earlier calculation included that. 

Yes, analysis_4's analysis_data is one key incorrect. analysis_6's analysis_data is one key incorrect. 

So those were accounted for in the accuracy calculation. 

I think that's thorough. 

Final Scores:

Data:80, Analyses:70, Results:82. 

Wait, but the Results' total comes to 82? Let me confirm again:

Structure (10) + Completeness (30) + Accuracy (42) =82. 

Yes. 

So the final scores are as such.
</think>

"Final Scores": {
    "Data": 80,
    "Analyses": 70,
    "Results": 82
}