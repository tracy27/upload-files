Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component.

First, the task is to score three objects: Data, Analyses, and Results. Each has a max of 100 points, divided into Structure (10), Content Completeness (40), and Content Accuracy (50). 

Starting with the **Data** section:

**Groundtruth Data** has three entries: data_1 (Proteomics), data_2 (Transcriptomics), data_3 (Metabolomics). 

**Annotation Result Data** only includes data_1. So, the user missed data_2 and data_3. 

**Structure Check (10 points):**
The annotation result's data array has one entry with the correct keys (id, omics, source, link, format, public_id). Since all required keys are present for data_1, structure is okay. So full 10 points here.

**Content Completeness (40 points):**
Each sub-object (data entry) contributes equally. Groundtruth has 3, Annotation has 1. Each missing sub-object would deduct (40/3 ≈13.33 points). Missing two would be 26.66 points lost. But since extra sub-objects might also penalize? Wait, the instruction says "extra sub-objects may also incur penalties depending on contextual relevance". However, in this case, the Annotation doesn't have extra, just missing. So deduction for missing two: 40 - 2*(40/3). Let me compute that exactly. 40 minus (2 * (40/3)) = 40 - 26.67 ≈13.33. But maybe it's better to deduct per missing. Alternatively, if each sub-object's presence is worth some points. Since there are 3 in groundtruth, each missing is 40/3 ≈13.33. So missing two would deduct 26.66, so 13.34 left. Hmm, but maybe the completeness is about having all required sub-objects. Since they have 1/3, maybe 40*(1/3)=13.33. That would be low. Wait the instructions say "deduct points for missing any sub-object". So for each missing, you lose points. Let me think again. Suppose each sub-object is worth 40/3≈13.33. So missing two, so 40 - 2*(13.33) = 13.34. So content completeness score is ~13.34, which rounds to 13 or 13.33. Maybe exact calculation is needed. Alternatively, perhaps each missing sub-object takes away 40/(number of groundtruth sub-objects). So here, 3 sub-objects, so each missing is 40/3. So 2 missing would be 2*(40/3) deduction, so total content completeness is 40 - 80/3 = 40 - 26.66=13.34. So 13.34/40. 

Wait, but maybe another way: total possible is 40 for all present. If they have N out of M, then (N/M)*40. Here N=1, M=3 → (1/3)*40≈13.33. Either way, same result. So Content Completeness for Data is around 13.33. 

**Content Accuracy (50 points):**
Only data_1 is present. Check its key-value pairs against groundtruth. 

Comparing data_1:

- omics: Both "Proteomics" – match.
- source: Groundtruth has "", annotation has "iProX database". Wait, no! Wait groundtruth data_1's source is "iProX database", which matches. Wait wait, looking back:

Wait groundtruth data_1's source is "iProX database", yes. The annotation's data_1's source is same. So that's correct. 

Link: both "https://iprox.org/" – match.

Format: "Raw proteomics data" vs same – correct.

Public_id: "PXD025311" matches. 

So all key-value pairs for data_1 are correct. So accuracy is perfect here. So 50 points.

Total Data score: 10 (structure) + 13.33 (content completeness) +50 (accuracy) = 73.33. Rounded maybe to 73 or 73.33. Need to note deductions.

Now moving to **Analyses**:

Groundtruth has 12 analyses (analysis_1 to analysis_12). The Annotation has 6 analyses: analysis_2,3,5,6,8,9. 

Wait let me check the annotation's analyses list:

The annotation's analyses array includes:
analysis_2 (Transcriptomics),
analysis_3 (Metabolomics),
analysis_5,
analysis_6,
analysis_8,
analysis_9.

Wait, but Groundtruth has analysis_1 through 12. So the Annotation is missing analysis_1,4,7,10,11,12. 

But first, check structure.

**Structure Check (10 points):**

Each sub-object (analysis) must have id, analysis_name, analysis_data. Some have additional fields like label.

Looking at Annotation's analyses:

- analysis_2: has analysis_name, analysis_data. Correct structure.
- analysis_3: same.
- analysis_5: has label. Which is allowed because in groundtruth analysis_5 also has label. So structure seems okay. All required keys (id, analysis_name, analysis_data) are present. So structure is good. So 10 points.

**Content Completeness (40 points):**

Groundtruth has 12 analyses, Annotation has 6. Each missing analysis would deduct points. So how many are missing?

Missing analyses: analysis_1,4,7,10,11,12 (total 6). So 6 missing out of 12. So each missing is worth 40/12≈3.33 points. 6 missing → 6*3.33≈20 points lost. So 40-20=20 points. 

But wait, maybe some of the analyses in the Annotation are semantically equivalent even if ID differs? The instructions say to focus on content not IDs. Wait, but the analyses in the Annotation include analysis_2, which corresponds to groundtruth analysis_2. So those are same. Similarly, analysis_3 is present. So the Annotation's analyses do have some of the groundtruth ones. Wait actually, the Annotation's analyses are exactly the same as groundtruth's analysis_2,3,5,6,8,9. So the missing ones are analysis_1,4,7,10,11,12. So indeed 6 missing. So the content completeness is 6/12 missing, so 50% present, hence 20 points. 

However, need to check if any extra sub-objects in the Annotation. It doesn't seem like; all are part of the groundtruth. So no penalty for extras here. 

Thus, content completeness is 20/40. 

**Content Accuracy (50 points):**

For each present analysis in the Annotation, check their key-value pairs against the groundtruth's equivalent. 

Let me go through each analysis in the Annotation:

1. **analysis_2 (Transcriptomics):**
   Groundtruth analysis_2 has analysis_data: "data2".
   In the Annotation's analysis_2, analysis_data is "data2" – which matches. So this is correct. So no deduction here.

2. **analysis_3 (Metabolomics):**
   Groundtruth analysis_3's analysis_data is "data3". The annotation's analysis_3 has "data3" – matches. Correct.

3. **analysis_5 (Differential analysis):**
   Groundtruth analysis_5 has analysis_data pointing to analysis_1 (since analysis_data: "analysis_1"). The Annotation's analysis_5 also has analysis_data: "analysis_1". The label is the same as groundtruth. So everything here matches. Accurate.

4. **analysis_6 (MCODE):**
   Groundtruth analysis_6's analysis_data is "analysis_5". The Annotation's analysis_6 has analysis_data: "analysis_5" – correct. So accurate.

5. **analysis_8 (Differential analysis):**
   Groundtruth analysis_8 has analysis_data: "analysis_2" and label with sepsis groups. The Annotation's analysis_8 matches exactly. So accurate.

6. **analysis_9 (Functional Enrichment):**
   Groundtruth analysis_9's analysis_data is "analysis_8". The Annotation's analysis_9 also has "analysis_8" – correct. 

All these analyses are accurately represented. So content accuracy is full 50. 

Wait, but what about the analysis_data references? For example, analysis_5 in the Annotation refers to "analysis_1", but in the Annotation's data array, data_1 is present but data_2 isn't. Wait does that affect anything? The analysis_data links are between analyses and data. The analysis_2's analysis_data is "data2", which in the groundtruth is linked to data_2 (transcriptomics). However, in the Annotation's data array, data_2 is missing. Does that mean the analysis_2's analysis_data is invalid? Because the data_2 doesn't exist in the Annotation's data. 

Hmm, this is a critical point. The analysis references data entries. If the data entry isn't present in the Annotation's data section, then the analysis_data pointer is incorrect. 

In the Annotation's data, only data_1 exists (proteomics). The analysis_2 refers to "data2", which in groundtruth is Transcriptomics data, but in the Annotation's data, data_2 doesn't exist. 

Therefore, the analysis_2's analysis_data ("data2") is pointing to a non-existent data entry in the Annotation. This is an error. Similarly, analysis_3 refers to "data3", which also doesn't exist in the Annotation's data. 

This means that those analyses (analysis_2 and 3) have incorrect analysis_data pointers. 

Oh! This is a problem I overlooked earlier. The analysis_data field refers to the data's IDs. If the data entries aren't present, then the analysis references are invalid. 

So this affects the content accuracy of the analyses. 

Wait, this complicates things. Let's reassess:

For **analysis_2 (Transcriptomics)** in the Annotation:

Groundtruth analysis_2's analysis_data is "data2", which corresponds to data_2 (Transcriptomics). However, in the Annotation's data, data_2 doesn't exist. Therefore, the analysis_data "data2" is invalid because that data isn't present. Thus, this key-value pair is inaccurate. 

Similarly, analysis_3's analysis_data is "data3", but data_3 is missing in the Annotation's data. So those analyses have incorrect references. 

Additionally, analysis_5's analysis_data is "analysis_1", but in the Annotation, analysis_1 is missing (since the Annotation starts at analysis_2). Wait, the Annotation's analyses don't have analysis_1. Wait, the groundtruth analysis_1 is present in groundtruth, but the Annotation does not have analysis_1. 

Wait, the Annotation's analyses array starts at analysis_2. So analysis_5's analysis_data is "analysis_1", which is a reference to analysis_1, which isn't present in the Annotation. Therefore, that's another error. 

This means several inaccuracies in the analyses:

- analysis_2's analysis_data: "data2" → invalid (data missing)
- analysis_3's analysis_data: "data3" → invalid (data missing)
- analysis_5's analysis_data: "analysis_1" → invalid (analysis missing)

Additionally, analysis_5's label is correct, so maybe other parts are okay except the analysis_data pointers.

This significantly impacts the accuracy.

Let's recalculate Content Accuracy for Analyses considering these errors.

Each analysis in the Annotation's analyses needs to be checked for accuracy, with deductions for any discrepancies.

There are 6 analyses in the Annotation. Let's go through each:

1. **analysis_2 (Transcriptomics):**
   - analysis_data: "data2" (invalid, since data_2 is missing). This is a critical error. Deduct points.
   - analysis_name is correct. 
   So, this analysis's accuracy is partially wrong. Since the analysis_data is incorrect, this could lead to significant deductions. How much? Maybe each key's inaccuracy deducts proportionally. The analysis_data is a key part of the analysis, so maybe a large chunk. Let's say this analysis's accuracy is 0 because the critical data reference is wrong. Or maybe partial?

Alternatively, perhaps each key in the analysis's key-value pairs contributes to the accuracy. The main keys are analysis_name, analysis_data, and any labels. 

Assuming each key's correctness contributes equally:

- analysis_name: correct (Transcriptomics)
- analysis_data: incorrect (points to missing data)
- label: Not present in this analysis, so no issue.

If analysis_data is crucial, maybe half the points for this analysis. Since this is a key part, maybe it's a major error leading to full deduction for this analysis's accuracy. 

Alternatively, since accuracy is per key-value pair, but the question says "discrepancies in key-value pair semantics". The analysis_data's value is pointing to a non-existent data element. So that's an incorrect value. 

Since this is a critical error, perhaps this analysis's contribution to accuracy is 0. 

Similarly, analysis_3:

2. **analysis_3 (Metabolomics):**
   - analysis_data: "data3" → invalid (data_3 is missing)
   Same issue as above. analysis_data is incorrect. So this analysis's accuracy is 0.

3. **analysis_5 (Differential analysis):**
   - analysis_data: "analysis_1" → analysis_1 is missing in the Annotation. So this is invalid. The analysis_1 is part of groundtruth but not included in the Annotation's analyses. Hence, the reference is broken. 
   Also, the label is correct. But the analysis_data is invalid. 
   So this analysis's accuracy is compromised. 

4. **analysis_6 (MCODE):**
   - analysis_data: "analysis_5" → analysis_5 exists in the Annotation. So this is okay. 
   - Other fields are correct. So this is accurate. 

5. **analysis_8 (Differential analysis):**
   - analysis_data: "analysis_2" → analysis_2 exists in the Annotation. So valid. 
   - Label is correct. So accurate. 

6. **analysis_9 (Functional Enrichment):**
   - analysis_data: "analysis_8" → exists. So okay. 

So, out of the 6 analyses in the Annotation:

- analysis_2 and 3 have invalid analysis_data references (data2/data3 missing), so their accuracy is 0 each.
- analysis_5 has invalid analysis_data (analysis_1 missing), so accuracy 0.
- analysis_6,8,9 are fully accurate. 

Total of 3 analyses with 0 accuracy, and 3 with full. 

Calculating the accuracy: 

Each analysis contributes (50 points / total number of analyses in Annotation?) or per groundtruth? 

Wait the content accuracy is for the matched sub-objects (those that are present in both). Wait no, for content accuracy, we look at the sub-objects that are deemed equivalent in content completeness (i.e., the ones that exist in the Annotation and correspond to groundtruth's). 

Wait the instruction says: "For sub-objects deemed semantically matched in the 'Content Completeness' section, deductions are applied based on discrepancies in key-value pair semantics."

So first, in content completeness, we determine which sub-objects are present and semantically matched. Then, for those, check their key-value pairs.

Wait, but in the Analyses section, the Annotation has analysis_2,3,5,6,8,9. These correspond to groundtruth's analysis_2,3,5,6,8,9. So they are semantically matched (same analysis names and structures). Therefore, these are considered matched sub-objects for content accuracy evaluation. 

However, the analysis_data fields may have issues (like pointing to missing data/analyses). So for each of these 6 analyses, we check their key-value pairs. 

Each analysis has a few key-value pairs. Let's see how many keys per analysis:

Take analysis_2 (Transcriptomics):

Keys: id, analysis_name, analysis_data. 

Each key's correctness:

- id: the ID can differ, but since we're focusing on content, the ID itself isn't evaluated. The content is the name and data references. 

- analysis_name: correct (matches groundtruth).

- analysis_data: incorrect (pointing to non-existent data).

So for analysis_2, two keys (name and data). Both are correct except analysis_data is wrong. 

Wait the key is analysis_data's value. Since the value is incorrect (the referenced data is missing), this is a discrepancy. 

How much does this impact the accuracy? Since analysis_data is crucial, perhaps this is a major error. Let's assume each key's correctness contributes equally. So for each analysis, the key-value pairs are:

analysis_2 has three keys (id, analysis_name, analysis_data). But id is ignored for content. So two keys: analysis_name and analysis_data. 

analysis_name is correct (1/2), analysis_data is wrong (0/2). So 50% accuracy for this analysis. 

Similarly for analysis_3:

Same scenario. analysis_data wrong. 50% accuracy.

analysis_5:

analysis_data is "analysis_1", which is missing. The groundtruth analysis_5's analysis_data is "analysis_1", but in the Annotation, analysis_1 doesn't exist. So the analysis_data is pointing to a non-existent analysis. Thus, incorrect. 

analysis_5's keys:

analysis_name is correct (Differential analysis), analysis_data is wrong. So 50% accuracy here too.

analysis_6,8,9 are all correct in their keys, so 100% for each.

Total accuracy contributions:

Each analysis's accuracy is calculated, and summed up, then scaled to 50 points. 

Let me compute per analysis:

Total key-value pairs to consider across all matched analyses:

Each analysis has:

- analysis_2: analysis_name (correct) and analysis_data (incorrect) → 1 correct, 1 wrong. So 50% → 0.5

- analysis_3: same → 0.5

- analysis_5: analysis_name correct (Differential analysis?), yes. analysis_data wrong → 0.5

- analysis_6: both keys correct →1

- analysis_8: both keys correct →1

- analysis_9: both keys correct →1

Total correct fractions: (0.5+0.5+0.5 +1+1+1) = 4.5

Total possible for all analyses: 6 analyses, each contributing 2 keys (assuming analysis_data and analysis_name). Total keys: 12. Correct keys: 4.5*2 (since each 0.5 is per analysis's 2 keys). Wait maybe better to calculate per analysis:

Each analysis contributes to the accuracy score. Since content accuracy is 50 points for the entire analyses object, and there are 6 sub-objects (matched ones), each sub-object's accuracy is (number of correct keys)/(total keys per sub-object). 

Wait maybe it's better to treat each analysis as a unit. Let's consider each analysis's key-value pairs:

For each analysis in the Annotation's matched set (6 analyses):

Each analysis has the following keys that matter (excluding id):

- analysis_name: must match groundtruth's analysis_name.

- analysis_data: must correctly reference existing data/analysis.

- label (if present): must match groundtruth's label.

For analysis_2:

analysis_name: correct (Transcriptomics). 

analysis_data: "data2" → data_2 is missing in data array. So invalid reference. 

No label here. 

Thus, analysis_data is incorrect. 

Two keys (name and data). One correct (name), one wrong (data). So 50% for this analysis.

Analysis_3 similarly: 50%.

Analysis_5:

analysis_name: correct (Differential analysis).

analysis_data: "analysis_1" → analysis_1 is missing in the Annotation's analyses. So invalid. 

Label is correct (matches groundtruth's label). 

Wait analysis_5 in groundtruth has a label. The Annotation's analysis_5 also has the same label. So label is correct. 

So keys here are analysis_name, analysis_data, and label. 

Three keys. 

analysis_data is wrong (1 mistake), others correct. So 2/3 ≈66.67% accuracy for this analysis.

Analysis_6:

analysis_name correct (MCODE).

analysis_data correct (analysis_5 exists in Annotation).

No label needed. 

Two keys, both correct →100%.

Analysis_8:

analysis_name correct (Differential analysis).

analysis_data correct (analysis_2 exists).

Label correct. 

Three keys (name, data, label). All correct →100%.

Analysis_9:

analysis_name correct (Functional Enrichment).

analysis_data correct (analysis_8 exists).

Two keys →100%.

Now calculating each analysis's contribution:

analysis_2: 50%

analysis_3: 50%

analysis_5: ~66.67%

analysis_6: 100%

analysis_8: 100%

analysis_9: 100%

Total percentage for all 6 analyses:

(0.5 +0.5 +0.6667 +1 +1 +1) = 4.6667

Divide by 6 (number of analyses) to get average: 4.6667 /6 ≈0.7778 →77.78% of the possible 50 points.

So 50 *0.7778 ≈38.89 points. 

Wait, but maybe each analysis's weight is equal. Since there are 6 analyses, each contributes 50/6 ≈8.333 points. 

For each analysis:

analysis_2: 0.5 *8.333≈4.166

analysis_3: same →4.166

analysis_5: 0.6667*8.333≈5.555

analysis_6:8.333

analysis_8:8.333

analysis_9:8.333

Summing these:

4.166 +4.166 +5.555 +8.333 +8.333 +8.333 ≈ 38.88 ≈38.89 points.

So content accuracy score for Analyses would be approximately 38.89.

Adding up:

Structure:10 

Content completeness:20 

Accuracy: ~38.89 

Total Analyses score: 10 +20 +38.89≈68.89. Approximately 69.

Wait but maybe I made a miscalculation here. Alternatively, perhaps the content accuracy is calculated per key, not per analysis. Let me try that approach:

Each key in all sub-objects contributes to the 50 points. 

Total number of key-value pairs to evaluate across all analyses:

For each of the 6 analyses in the Annotation's matched set:

analysis_2:

keys: analysis_name (correct), analysis_data (wrong) → 1 correct, 1 wrong (assuming two keys)

analysis_3: same →1C,1W

analysis_5:

analysis_name (C), analysis_data (W), label (C). So 2C,1W (three keys)

analysis_6:

analysis_name(C), analysis_data(C) →2C

analysis_8:

analysis_name(C), analysis_data(C), label(C) →3C

analysis_9:

analysis_name(C), analysis_data(C) →2C

Total correct keys:

analysis_2:1

analysis_3:1

analysis_5:2

analysis_6:2

analysis_8:3

analysis_9:2

Total correct: 1+1+2+2+3+2 = 11

Total keys:

analysis_2:2

analysis_3:2

analysis_5:3

analysis_6:2

analysis_8:3

analysis_9:2 → total keys: 2+2+3+2+3+2 = 14

Accuracy is (11/14)*50 ≈ (0.7857)*50≈39.28 points.

That aligns with prior calculation. So ~39.28.

Thus, total Analyses score:10+20+39.28≈69.28, so ~69.

Now for **Results**:

Wait the groundtruth and annotation results provided in the input don't have a "results" section. Looking back:

The groundtruth JSON provided by the user includes "data" and "analyses", but no "results". The annotation result also lacks "results". 

The task requires evaluating data, analyses, and results. Since neither has a results section, perhaps both are missing it. 

Wait checking the input given:

User's input shows the groundtruth and annotation result each have "article_id", "data", and "analyses". There is no "results" section in either. 

Therefore, for the Results object:

Both the groundtruth and annotation have nothing. So what's the scoring here?

Structure: Since there's no results object in either, does that count as structure being correct? Or is the absence of the object a structural error?

The instructions say: "the content to be scored is composed of three components: data, analyses, and results." So if the groundtruth includes results, but in the provided data, it's missing, but according to the user's input, both groundtruth and annotation lack results. 

Wait the user's input shows:

The groundtruth given is: the first JSON block has "data" and "analyses", but no "results".

The annotation result (second JSON) also has only data and analyses. 

Therefore, according to the task, since the groundtruth does NOT include a results section, the annotation's absence is correct. 

Wait but the task says to evaluate all three objects. If the groundtruth doesn't have results, then the annotation not having it is correct. 

Wait the instructions state: "using the groundtruth as reference answer". So if the groundtruth's results is empty or non-existent, then the annotation's results being absent is correct. 

Assuming the groundtruth has no results, then the annotation's lack of results is correct. 

**Scoring Results:**

Structure: The results object is missing in both. Since the groundtruth doesn't require it, the annotation's omission is correct. So structure is okay. 10 points.

Content Completeness: Since groundtruth has no results, the annotation also has none. So completeness is 100%. 40 points.

Content Accuracy: No content to compare, so full marks. 50 points.

Total Results score: 10+40+50=100.

Wait but this depends on whether the groundtruth has a results section. Since in the provided data, the groundtruth does not have a "results" key, the annotation's omission is correct. Hence, Results gets full marks.

So compiling all scores:

Data: 73.33 (approx 73)

Analyses: ~69.28 (approx 69)

Results: 100

Final Scores: 

"Data": 73,

"Analyses": 69,

"Results": 100.

But let me verify again for Results. Since the groundtruth has no results, the annotation's absence is correct. Structure is correct (since the absence is correct), completeness is perfect (no missing sub-objects because there are none to begin with), and accuracy is perfect (nothing to be inaccurate). So yes, 100.

Another point to check: In the Analyses, the analysis_data references to data entries that are missing in the Data section. Does that affect the Analyses' content accuracy beyond the analysis_data key? 

Yes, as discussed earlier, which was accounted for in the Analyses accuracy score. 

For Data's Content Completeness, since there are 3 sub-objects in groundtruth and 1 in annotation, the calculation was 1/3 *40≈13.33. 

So rounding the final scores appropriately. 

Final Scores rounded to integers:

Data: 73 (or 73.33),

Analyses: 69,

Results:100.

Thus, the final output should present these.
</think>

{
    "Final Scores": {
        "Data": 73,
        "Analyses": 69,
        "Results": 100
    }
}

### Scoring Process Explanation:

---

#### **Data Section (73/100)**
- **Structure (10/10):**  
  The annotation correctly uses the required keys (`id`, `omics`, `source`, `link`, `format`, `public_id`) for the single `data_1` sub-object.  

- **Content Completeness (13/40):**  
  - Groundtruth has **3** sub-objects (Proteomics, Transcriptomics, Metabolomics).  
  - Annotation only includes **1** sub-object (`data_1`).  
  - Penalty: Missing **2 sub-objects** deducts `(2/3) × 40 ≈ 26.67` → **13.33 remaining**.  

- **Content Accuracy (50/50):**  
  - `data_1` matches groundtruth perfectly in all key-value pairs.  

---

#### **Analyses Section (69/100)**
- **Structure (10/10):**  
  All sub-objects use the required keys (`id`, `analysis_name`, `analysis_data`).  

- **Content Completeness (20/40):**  
  - Groundtruth has **12** sub-objects.  
  - Annotation includes **6** sub-objects (`analysis_2`, `analysis_3`, `analysis_5`, `analysis_6`, `analysis_8`, `analysis_9`).  
  - Penalty: Missing **6 sub-objects** deducts `(6/12) × 40 = 20` → **20 remaining**.  

- **Content Accuracy (~39/50):**  
  - **Errors:**  
    - `analysis_2`/`analysis_3`: `analysis_data` points to nonexistent `data_2`/`data_3`.  
    - `analysis_5`: `analysis_data` points to nonexistent `analysis_1`.  
  - **Correct Analyses:** `analysis_6`, `analysis_8`, and `analysis_9` are fully accurate.  
  - Score reduction due to invalid references in `analysis_2`, `analysis_3`, and `analysis_5` → **~39/50**.  

---

#### **Results Section (100/100)**  
- Groundtruth does not include a `results` section, so the annotation’s omission is correct.  
- **Structure (10/10):** No errors.  
- **Content Completeness (40/40):** No missing sub-objects (groundtruth has none).  
- **Content Accuracy (50/50):** No discrepancies to address.  

--- 

**Final Scores**:  
- **Data**: 73  
- **Analyses**: 69  
- **Results**: 100