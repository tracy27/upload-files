Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component: data, analyses, and results. But wait, looking at the input, both the groundtruth and the annotation result have data and analyses sections. The user mentioned three components—data, analyses, and results—but the example input doesn't include a results section in either. Maybe results aren't present here? Or perhaps it's a mistake. Since the task says to evaluate all three, maybe the results part is missing in the input provided. Hmm, but the user might have intended just data and analyses since they're the only ones present. Let me check again.

The problem statement mentions three components: data, analyses, and results. However, in the given input, both the groundtruth and the annotation result do not have a "results" key. So maybe there was an error in the problem setup, or perhaps results are not part of this particular task. Since the user provided data and analyses in both, I'll proceed with those two components and ignore results unless instructed otherwise. Alternatively, maybe the results are part of the analyses? Wait no, looking at the groundtruth's analyses, some have labels like survival analysis with OS/PFI etc., which might be part of results, but the structure isn't clear. Since the user hasn't provided a results section in either input, perhaps we should just focus on data and analyses for this scoring. The problem might have a typo, but given the inputs, I'll proceed with the available data and analyses.

Starting with the data component first. Let me compare the groundtruth's data array with the annotation's data array. 

Groundtruth has 68 data entries (from data_1 to data_68). The annotation result has 32 data entries (data_3, data_7, ..., up to data_68). Wait let me count exactly:

Looking at the annotation's data array:
- There are items listed up to data_68 (the last one is data_68 with omics "copy number alteration"). Let's count them:

1. data_3
2. data_7
3. data_10
4. data_12
5. data_15
6. data_16
7. data_19
8. data_23
9. data_27
10. data_28
11. data_29
12. data_30
13. data_33
14. data_34
15. data_36
16. data_39
17. data_43
18. data_49
19. data_50
20. data_54
21. data_55
22. data_57
23. data_59
24. data_60
25. data_61
26. data_63
27. data_64
28. data_65
29. data_67
30. data_68

Wait that's 30 items? Let me recount:

1. data_3
2. data_7
3. data_10
4. data_12
5. data_15
6. data_16
7. data_19
8. data_23
9. data_27
10. data_28
11. data_29
12. data_30
13. data_33
14. data_34
15. data_36
16. data_39
17. data_43
18. data_49
19. data_50
20. data_54
21. data_55
22. data_57
23. data_59
24. data_60
25. data_61
26. data_63
27. data_64
28. data_65
29. data_67
30. data_68

Yes, 30 entries. But in the groundtruth, there are 68 data entries. The annotation is missing many. 

First, for the data component's structure: the structure here requires checking if each data entry has the correct keys. The required keys for data objects are "format", "id", "link", "omics", "public_id", "source". Looking at the groundtruth examples, each data entry includes these keys, even if some values are empty strings. The annotation's data entries also follow this structure. For instance, data_3 in annotation has all the keys. So structure score is 10/10 because all sub-objects in data have the right keys and structure. No issues here.

Next, content completeness (40 points). Here, we need to check if all groundtruth sub-objects are present in the annotation, considering possible synonyms or variations. 

The groundtruth has 68 data entries. The annotation has 30. That's a big discrepancy. So first, how many of the groundtruth's data entries are present in the annotation?

Let me list the data IDs present in the annotation's data array:

Present data IDs in annotation:
data_3, 7, 10,12,15,16,19,23,27,28,29,30,33,34,36,39,43,49,50,54,55,57,59,60,61,63,64,65,67,68.

Total of 30 entries. Now, the groundtruth has 68. So missing entries would be 68 - 30 = 38 entries. But wait, perhaps some of the missing entries are duplicates or have different IDs but same content? Wait, the user said to ignore IDs when checking semantic content. So even if the ID is different, if the content matches, it's okay. But the annotation entries have the same IDs as groundtruth? Let me see:

Take data_3 in the annotation. In groundtruth, there is data_3 with public_id GSE176031. The annotation's data_3 matches exactly. So that's a match. Similarly, data_7 is present in groundtruth (GSE35988), so that's good. 

Now, looking for entries in groundtruth that are missing in the annotation:

Looking through the groundtruth data array, let's see:

Starting from data_1: 

data_1: Single-cell RNA-seq, GSE193337 (GEO). Not present in the annotation's data list. So missing.

data_2: GSE185344. Not in the annotation.

data_4: prostate_portal_300921 (Prostate Cell Atlas). Not present.

data_5: GSE141445. Not in the list.

data_6: TCGA-PRAD (GEO? Wait source is TCGA). In the annotation, there's data_6? No, the annotation's data starts with data_3. Wait the annotation's data array does not have data_6. So missing.

Continuing:

data_8: GEO GSE94767. Not present.

data_9: GSE134051. Not present.

data_11: GSE21034. Not present.

data_13: prostate_dkfz_2018 (cBioPortal) – Wait, in the annotation, data_15 has public_id prostate_dkfz_2018, which is data_15 in groundtruth. Wait data_15 in groundtruth is indeed that. So data_15 is present in the annotation. 

Wait, groundtruth's data_13 is "prostate_dkfz_2018" source cBioPortal? Wait no, data_15 in groundtruth is "prostate_dkfz_2018" from cBioPortal. Let me confirm:

Looking back at groundtruth data array:

data_13: public_id prad_su2c_2019, source cBioPortal.

data_15: public_id prostate_dkfz_2018, source cBioPortal. Yes, so data_15 is present in the annotation.

So data_13 (prad_su2c_2019) is not in the annotation.

Continuing:

data_14: WCDT/WCDT. Not present in the annotation's data.

data_17: GSE46602. Not present.

data_18: GSE54460. Not present.

data_20: GSE116918. Not present.

data_21: E-MTAB-6128 (ArrayExpress). Not in the annotation's data.

data_22: Alumkal_2020 (Supplements). Not present.

data_24: GSE28680. Not present.

data_25: GSE46691. Not present.

data_26: TcgaTargetGtex (UCSC Xena). Present as data_26 in the groundtruth, but the annotation's data_26? Wait the annotation's data includes data_27 (PCAWG from UCSC Xena). Wait the groundtruth's data_26 is TcgaTargetGtex, which is not in the annotation's data entries. So missing.

data_27: PCAWG (UCSC Xena) – yes, the annotation has data_27, which is in groundtruth's data_27. So that's present.

data_28: ICGC (UCSC Xena) – present in the annotation.

data_31: Kallisto (zenodo). Not in the annotation.

data_32: GSE111636. Not present.

data_35: phs002419 (dbGaP). Not in the annotation's data.

data_37: Checkmate010 (Supplements). Not present.

data_38: Checkmate025 (EGA). Not present.

data_40: Miao_2018 (source empty). Not in the annotation.

data_41: GSE67501 (GEO). Not present.

data_42: IMmotion151 (EGA). Not in the data.

data_44: GSE179730. Not present.

data_45: GSE162137. Not present.

data_46: GSE165252. Not present.

data_47: PRJNA482620 (NCBI). Not present.

data_48: PRJEB25780 (NCBI). Not present.

data_51: GSE126044. Not present.

data_52: GSE135222. Not present.

data_53: OAK (EGA). Not present.

data_56: GSE115821. Not present.

data_58: GSE78220. Not present.

data_62: SRP067586. Not present.

data_66: expression matrix (TCGA-PRAD). Not present in the annotation's data; the annotation has data_67 and 68, but not data_66.

data_67 and 68 are present, though. Wait data_67 (DNA methylation) and data_68 (copy number) are included in the annotation. So data_66 (expression matrix) is missing.

So the total missing entries are numerous. Each missing sub-object in data would deduct points. The content completeness is out of 40, so each missing entry could deduct a portion. Let's see how many are missing.

Total groundtruth data entries: 68.

Present in annotation: 30.

Missing: 68-30=38.

But some of the missing might have been replaced with extra entries? Wait the problem says "extra sub-objects may also incur penalties depending on contextual relevance". But in the annotation's data, are there any extra entries beyond what's in the groundtruth? Looking at the data IDs in the annotation's data, they are all present in the groundtruth except maybe none. Because the IDs used (like data_3, data_7 etc.) are existing in groundtruth. So there are no extra entries beyond groundtruth. So the penalty is only for missing entries.

However, the problem states that for content completeness, points are deducted for missing sub-objects. The maximum is 40, so per missing item, the deduction would depend on the total number. Let me think.

Alternatively, perhaps each missing sub-object deducts (40 / total_groundtruth_sub_objects) * number_of_missing. But the problem says to deduct points for missing any sub-object. Since the maximum is 40, perhaps each missing sub-object reduces the score by (40 / 68)*1 per missing. But that might be complicated. Alternatively, since the content completeness is about presence of all groundtruth sub-objects, if the annotation is missing many, then the score would be very low.

Alternatively, since the user says "deduct points for missing any sub-object", perhaps each missing sub-object deducts a certain amount, like 40 / total_groundtruth_sub_objects per missing. Since there are 68, each missing would be worth (40/68) ≈0.588 points. But 38 missing would be 38*(40/68)= 38*(~0.588)= ~22.3 points deduction. But that might be too granular. Alternatively, maybe the content completeness is evaluated per sub-object, and each missing is a fixed penalty, but I'm not sure. Alternatively, maybe if the annotation has all required sub-objects, it gets full marks, but missing each one reduces the score proportionally. Let me recheck the instructions.

The problem states: "Content completeness accounts for 40 points: This section should score at the sub-object level. Deduct points for missing any sub-object."

So, perhaps for each missing sub-object, you lose some fraction of the 40. To compute, total possible completeness points are 40. Each sub-object in the groundtruth is a component that needs to be present. So total sub-objects in groundtruth are 68. So each is worth (40/68) points. Missing 38 would deduct 38*(40/68)= 38*(0.588)= ~22.3, so content completeness score would be 40 -22.3≈17.7. But since we can't have fractions, maybe rounded to 18. However, this might be too technical. Alternatively, perhaps the scorer is supposed to deduct 40*(number of missing)/total. So 40*(38/68)= approx 22.3 deducted, resulting in 17.7. But maybe the scorer can adjust. Alternatively, maybe it's simpler: if half are missing, deduct half the points. But in this case, over half are missing (38 out of 68), so maybe around 20 points left?

Alternatively, maybe the system expects a more straightforward approach. Since the annotation is missing most of the data entries, the content completeness score would be very low. Let me see: if the annotation had all 68, it would get 40. Missing 38, so the percentage is (30/68)*40≈ 17.65. So approximately 18 points.

Additionally, check for any extra entries. The problem mentions that extra sub-objects may incur penalties. But in this case, the annotation's data entries are all from the groundtruth, so no extras. Hence, no penalty there.

Moving on to content accuracy (50 points). This evaluates the accuracy of the key-value pairs in the sub-objects that are present. For each sub-object present in both, check if the keys match semantically. Even if wording differs but meaning is same, it's okay.

Looking at the present data entries in the annotation:

For each data entry in the annotation, compare with groundtruth:

Take data_3 (GSE176031, GEO): matches exactly, so accurate.

data_7: GSE35988, GEO: matches.

data_10: GSE183019, GEO: matches.

data_12: GSE6099, GEO: correct.

data_15: prostate_dkfz_2018, cBioPortal: matches.

data_16: GSE70770, GEO: correct.

data_19: GSE84042, GEO: yes.

data_23: GSE6811, GEO: matches.

data_27: PCAWG, UCSC Xena: correct.

data_28: ICGC, UCSC Xena: yes.

data_29: GSE2109, GEO: matches.

data_30: IMvigor210, R package: matches.

data_33: GSE173839, GEO: correct.

data_34: GSE194040, GEO: yes.

data_36: Checkmate009, ArrayExpress: matches.

data_39: E_MTAB_3218, ArrayExpress: note in groundtruth, data_39's public_id is E_MTAB_3218, but in groundtruth's data_39, the public_id is "E_MTAB_3218" (with underscore?), whereas in the annotation it's written as "E_MTAB_3218" – assuming that's correct. The source is ArrayExpress, which matches.

data_43: Javelin101, Supplements: matches.

data_49: GSE195832, Mendeley Data: yes.

data_50: TJ_Val, Mendeley Data: matches.

data_54: POPLAR, EGA: in groundtruth, data_54 is POPLAR from EGA. The annotation's data_54 has same, so correct.

data_55: Checkmate038, ArrayExpress: matches.

data_57: GSE131521, GEO: correct.

data_59: GSE91061, GEO: yes.

data_60: phs000452, dbGaP: matches.

data_61: PRJEB23709, NCBI: yes.

data_63: GSE100797, GEO: correct.

data_64: GSE96619, GEO: yes.

data_65: GSE202687, GEO: matches.

data_67: DNA methylation, TCGA-PRAD: in groundtruth, data_67 has omics "DNA methylation", public_id "", source TCGA-PRAD. Annotation's data_67 has same, so correct.

data_68: copy number alteration, TCGA-PRAD: yes.

All these seem to match exactly. Are there any discrepancies?

Wait, data_68 in groundtruth has "copy number alteration" as omics type, which matches the annotation's entry. 

Therefore, all the present sub-objects in the annotation's data have accurate key-value pairs. Thus, content accuracy is full 50 points.

But wait, what about the public_id for data_39? In groundtruth's data_39, the public_id is "E_MTAB_3218", and in the annotation, it's written as "E_MTAB_3218". Wait, looking at the groundtruth:

In groundtruth data_39: "public_id": "E_MTAB_3218", "source": "ArrayExpress"

In the annotation's data_39: "public_id": "E_MTAB_3218" (same as groundtruth). So correct.

Another point: data_67 and 68 in the groundtruth have format fields. For data_67 (DNA methylation), the format in groundtruth is "DNA methylation"? Wait no, let me check:

Wait in groundtruth, data_67 has format "DNA methylation" and omics "DNA methylation". Similarly, data_68 has format "copy number alteration", omics "copy number alteration".

In the annotation's data_67 and 68, the format and omics fields are correctly set. So those are accurate.

Therefore, all the present data entries are accurate. Thus, content accuracy is 50/50.

Thus, the total data score would be:

Structure: 10

Content completeness: approx 18 (as calculated earlier)

Content accuracy: 50

Total: 10 +18+50 =78? Wait but let's recalculate precisely.

Wait content completeness was 30 out of 68 entries present. So the completeness score is (30/68)*40 ≈ 17.65, which rounds to 18. So total data score is 10+18+50 =78.

But maybe the scorer can adjust. Alternatively, if the deduction is per missing item, but the problem says "deduct points for missing any sub-object". If each missing sub-object deducts (40/68) points, then 38 missing would deduct 38*(40/68)= ~22.3, so content completeness is 40 -22.3= 17.7, so 18.

Thus data total 10+18+50=78.

Now moving to analyses scoring.

First, structure (10 points). Each analysis object must have the correct keys. The required keys are "id", "analysis_name", "analysis_data", and possibly others like "label".

Looking at the groundtruth analyses:

Each analysis has id, analysis_name, analysis_data. Some have a label (e.g., analysis_6 has "label").

The annotation's analyses:

Analysis_1 has analysis_data as [data_1,...], which matches structure.

Analysis_2, 3,4 have analysis_data arrays. Analysis_8 has analysis_data pointing to "analysis_7".

The groundtruth's analysis_5 references analysis_1 via "analysis_data": "analysis_1", which is a string. Similarly, analysis_8 in the annotation uses "analysis_7" as analysis_data. The structure is consistent. All keys are present. So structure score is 10/10.

Content completeness (40 points):

Groundtruth has 7 analyses (analysis_1 to analysis_8? Wait counting:

Groundtruth's analyses array has 7 entries: analysis_1 through analysis_8 (since analysis_8 is the 7th entry? Wait let's count:

Groundtruth analyses list:

1. analysis_1 (single-cell RNA-seq)
2. analysis_2 (Transcriptomics)
3. analysis_3 (Transcriptomics)
4. analysis_4 (Transcriptomics)
5. analysis_5 (single cell cluster)
6. analysis_6 (survival)
7. analysis_7 (PCA)
8. analysis_8 (survival again)

Yes, 8 analyses in groundtruth. The annotation's analyses array has 5 entries:

analysis_1, analysis_2, analysis_3, analysis_4, analysis_8.

So missing analyses: analysis_5, analysis_6, analysis_7.

That's 3 missing analyses. Also, the analysis_8 in the annotation is present but was originally in the groundtruth, so it's counted as present.

So total groundtruth analyses: 8.

Present in annotation: 5.

Missing: 3.

Additionally, check for extra analyses. The annotation does not have any extra beyond the groundtruth's 8? The analyses in the annotation are analysis_1 to 4, and 8. So no extras.

Thus, content completeness is penalized for missing 3 analyses. Each analysis is a sub-object here, so each missing analysis deducts (40/8)=5 points. 3 missing would deduct 15 points, so content completeness score: 40-15=25.

Wait but need to check if the missing analyses are indeed required. Let me confirm each:

Missing analyses from groundtruth:

analysis_5: "single cell cluster" referencing analysis_1. Is this a required sub-object? Yes, as per groundtruth.

analysis_6: Survival analysis linked to analysis_1 with labels.

analysis_7: PCA linked to analysis_2.

These are all part of the groundtruth's analyses, so their absence deducts points.

Thus, 3 missing, so 40 - (3*(40/8)) = 40 - 15 =25.

Content accuracy (50 points):

Evaluate the accuracy of the present analyses' key-value pairs. For each present analysis in the annotation, check if their analysis_data and other keys match the groundtruth.

Starting with analysis_1:

Groundtruth analysis_1 has analysis_data: ["data_1", "data_2", "data_3", "data_4", "data_5"]

Annotation's analysis_1 has analysis_data: ["data_1", "data_2", "data_3", "data_4", "data_5"] — same as groundtruth. So accurate.

analysis_2:

Groundtruth analysis_2's analysis_data includes data_6 to 25. Let's see the groundtruth's analysis_2 has:

["data_6","data_7",..."data_25"] (20 entries). The annotation's analysis_2 has the same list. Wait let's check:

Groundtruth analysis_2's analysis_data array:

["data_6", "data_7", "data_8", "data_9", "data_10", "data_11", "data_12", "data_13", "data_14", "data_15", "data_16", "data_17", "data_18", "data_19", "data_20", "data_21", "data_22", "data_23", "data_24", "data_25"]

Total 20 entries.

Annotation's analysis_2's analysis_data has the same list. So correct.

analysis_3:

Groundtruth analysis_3 has analysis_data: ["data_26", "data_27", "data_28", "data_29"]. The annotation's analysis_3 has the same. So accurate.

analysis_4:

Groundtruth analysis_4 has a long list of data entries. The annotation's analysis_4's analysis_data matches exactly the groundtruth's list. Let me confirm:

Groundtruth analysis_4's analysis_data includes up to data_65. The annotation's analysis_4 has the same entries. So accurate.

analysis_8:

In groundtruth, analysis_8 is:

{
  "id": "analysis_8",
  "analysis_name": "Survival analysis",
  "analysis_data": "analysis_7",
  "label": {"SRS": ["High", "Low"]}
}

In the annotation's analysis_8, the structure is similar but check the analysis_data and label. The analysis_data is "analysis_7", which exists in groundtruth. The label has SRS: ["High", "Low"], which matches groundtruth. So accurate.

Now check for any discrepancies:

Is there any difference in the analysis_data lists?

For analysis_1: correct.

Analysis_2's data entries are all present in the annotation's analysis_2, so correct.

Same for others.

Thus, all present analyses in the annotation have accurate key-value pairs. So content accuracy is 50/50.

However, wait, the analysis_8 in the annotation has the correct label, but in the groundtruth's analysis_8, the label is only for SRS. Since the annotation's analysis_8 is exactly the same as groundtruth's analysis_8, it's correct.

Thus, content accuracy is full 50 points.

Therefore, the analyses score is:

Structure:10,

Content completeness:25,

Content accuracy:50,

Total: 10+25+50=85.

Wait wait, but content completeness was 25? Let me recheck.

Total analyses in groundtruth:8.

Present in annotation:5 (missing 3).

Each analysis is a sub-object, so total sub-objects are 8. Each is worth 40/8=5 points. So missing 3 analyses deduct 3*5=15. So content completeness is 40-15=25. Yes.

So analyses total is 10+25+50=85.

Now, what about the Results component? The user mentioned three components but the inputs don't have results. Since the input provided doesn't include results in either groundtruth or annotation, perhaps the results score is N/A, but the task says to evaluate all three. Maybe the user made an error, but since there's no data, perhaps we assume it's zero? Or perhaps the problem assumes results are part of analyses. Alternatively, maybe the results are considered under the analyses' label fields? For example, the survival analysis's labels like OS, PFI etc. But in the groundtruth, analysis_6 has those labels, which are part of analyses, not results. Since the results section is missing, perhaps it's impossible to score. Given that the user's input lacks results, maybe we should treat it as not applicable, but the task requires scoring all three. Alternatively, maybe the results are considered as part of the analyses. Alternatively, perhaps the problem intended to exclude results here, but the instructions say to consider three components. Since I can't proceed without results data, perhaps the results score is 0. But since it's unclear, maybe the user made a mistake and the results aren't part of this, so we can omit it and only score data and analyses. However, the task says "each object consists of multiple sub-objects"—maybe the three objects are data, analyses, and results. Since results aren't present, perhaps the results score is 0, but that seems harsh. Alternatively, the user may have forgotten to include it, but given the input, I must work with what's provided.

Alternatively, maybe the "results" are within the analyses. For example, the survival analysis labels could be part of results. But without explicit results sections in the inputs, I can't score that. Given the ambiguity, and since the problem's input doesn't have results, I'll assume the results are not part of this evaluation and only score data and analyses. The problem's instruction says "three components: data, analyses, and results", but the input lacks results. Perhaps the user intended to omit it, so I'll proceed with only data and analyses, but the final output requires all three. Maybe the results score is 0, but that's unfair. Alternatively, maybe the results are considered part of analyses, so the analyses score covers it. However, strictly following the problem's instruction, since the results are missing in both inputs, perhaps the results score is 0. But the user may not have included it. Since I can't proceed, perhaps I'll assign 0 for results. But the problem says to score each object (data, analyses, results), so I have to provide all three scores. Since there's no results in the input, I'll set results to 0, but note the assumption.

Alternatively, maybe the results are part of the analyses. For instance, the analysis_6 has labels for OS, PFI, etc., which could be results. If the results component is part of the analyses' labels, then perhaps the results are covered there. But without a separate results section, it's hard to tell. Given the problem's ambiguity, but needing to comply with the instructions, I'll proceed with data and analyses scores as above, and set results to 0 with a note. However, since the user might have missed including results, but the task requires three scores, I'll proceed with:

Final Scores: Data:78, Analyses:85, Results:0. But that might be incorrect. Alternatively, maybe the results are not required here. Alternatively, perhaps the problem's example input is incomplete, but given the information, I have to proceed as best as possible.

Alternatively, maybe the results are considered as the analyses' outputs, so they are included in the analyses score. Since the problem says the three components are data, analyses, and results, but the results aren't present, it's ambiguous. Since I have to provide all three scores, I'll assume that results are not part of this task's input and thus score them 0, but indicate that in the explanation.

However, the user might have intended to have results as part of the analyses. But given the time constraints, I'll proceed with the scores for data and analyses as computed, and set results to 0 with a note.

But the user's instructions say to use the format with all three, so I'll proceed accordingly.

Final Scores:

Data: 78 (structure 10 + content completeness ~18 + accuracy 50)

Analyses: 85 (10+25+50)

Results: 0 (no data provided)

But the problem might expect all three to be scored, even if results are missing. Alternatively, maybe I miscounted.

Wait, let me verify once more:

For analyses' content completeness, the missing analyses are analysis_5,6,7. So 3 out of 8, each worth 5 points, so 15 deduction, leaving 25.

Content accuracy for analyses: since all present analyses are accurate, 50.

Analyses total: 10+25+50=85.

Data total: 10 + ~18 (content completeness) +50 =78.

Thus, the final scores would be:

"Final Scores": {
    "Data": 78,
    "Analyses": 85,
    "Results": 0
}

But I need to confirm if the Results component is necessary. Since it's part of the task's requirement but absent in the input, perhaps the scorer should assume zero. Alternatively, maybe there's a misunderstanding. Alternatively, maybe the analyses' labels are part of the results. For instance, in the groundtruth, analysis_6 has a label with OS, PFI, DFI, DSS categories, which might be results of the analysis. Similarly, analysis_8 has SRS. If results refer to these labels, then perhaps the results are part of the analyses. In that case, the results score could be derived from the presence and accuracy of these labels.

Looking back at the task description: "results" as a component. Maybe each analysis's output (like the labels) are part of results. Thus, perhaps the results component is separate and the analyses' labels are part of results. But in the given inputs, there's no "results" section. Hence, without the results section, the score would be zero. Given the problem's instructions require evaluating results, but it's missing, I have to assign 0.

Thus, final scores as above.
</think>

"Final Scores": {
    "Data": 78,
    "Analyses": 85,
    "Results": 0
}