Okay, I need to evaluate the predicted annotation against the ground truth for each component: Data, Analyses, and Results. Let's start with the Data component first.

**Data Component Evaluation**

First, check the structure. The predicted data is a list of objects with keys like id, omics, source, link, format, public_id. It looks like valid JSON. The ground truth also has similar structures. So structure seems okay. But wait, looking at the ground truth's data_20, the ID is written as "data20" without an underscore? Wait no, in ground truth it's "data20", but in the predicted data, it's "data20". Hmm, maybe that's a typo but since the user said not to penalize IDs, so that's fine. All other IDs seem properly formatted. So structure score: 100?

Wait, looking at the predicted data entries:

Looking at data_8 in predicted: omics is "DNA methylation profiles", which isn't present in the ground truth. The ground truth's data_8 was "bulk RNA-seq" from EGA, but in predicted it's DNA methylation from Mendeley Data Portal. That's an extra entry not in ground truth. Similarly, data_13 in predicted is WES (Whole Exome Sequencing?), which isn't in the ground truth. Also, data_17 is "Bulk transcriptome" from National Omics Data Encyclopedia, which wasn't there before. And data20 in predicted is "Genomics" from Mergeomics, which is new. These are extra data entries not present in ground truth. 

Now checking completeness. The ground truth has 22 data entries (up to data22). The predicted has 22 entries too. But some are different. For example, data_8 in ground truth is bulk RNA-seq from GEO, but in predicted it's DNA methylation. Wait, actually in the ground truth data_8 was "source": "EGA", "public_id": "phs000915.v2.p2". But in predicted, data_8 is from Mendeley Data Portal. So that's incorrect. 

Also, data_13 in ground truth is "bulk RNA-seq" from EGA with public_id EGAD00001001244, but in predicted, it's WES from National Omics Data Encyclopedia with different info. 

So the predicted has some missing entries from ground truth. For instance, data_20 in ground truth had "omics": "bulk RNA-seq", "source": "GEO", "public_id": "GSE240058", but in predicted, data20 is Genomics from Mergeomics, which doesn't match. However, in predicted data22 has public_id GSE240058, which might correspond to data20 and data22 in ground truth? Let me check.

Ground truth data20: omics "bulk RNA-seq", source GEO, public_id GSE240058. In predicted data22 has omics "single cell RNA-seq", source GEO, public_id GSE240058. So that's a discrepancy. Ground truth data22 is "single cell RNA-seq", but predicted data22's omics is same but the format is txt vs FASTQ in ground truth? Not sure. Need to check each entry.

Completeness: The ground truth has data entries that are not present in predicted, like data_20 (the bulk RNA-seq with GSE240058), and data_21 (SCLC subtype annotations) is present in both. Data_22 in ground truth is single cell RNA-seq from GEO with public_id GSE240058 (same as data20 in ground truth?), but in predicted data22 has public_id GSE240058, but omics type is same. Maybe that's considered covered? 

Wait, maybe some entries are swapped. Let me count the number of data entries in ground truth and predicted. Ground truth has 22 entries (from data_1 to data22). Predicted also has 22. But some are replacements. So the completeness might have some missing but added others. 

Accuracy: For each existing entry in predicted, does it match the ground truth? Let's go through each:

- data_1 to data_7: these look the same except data_8 onwards. 

data_8: ground truth had bulk RNA-seq from EGA, but predicted has DNA methylation from another source. So that's wrong. 

data_9 to data_12: these are same as in ground truth (GSE numbers match). 

data_13: ground truth has bulk RNA-seq from EGA (public_id EGAD...), but predicted is WES. Not accurate. 

data_14 to data_16: same as ground truth except data_17 is new in predicted. 

data_17 is a new entry in predicted (Bulk transcriptome from National Omics Data Encyclopedia), not present in ground truth. 

data_18 and 19: same as ground truth. 

data20 in predicted is Genomics (not bulk RNA-seq) and data22's omics is single cell RNA-seq but the public ID matches GSE240058 which is part of data20 and data22 in ground truth. 

Hmm, this is getting complicated. Maybe the predicted missed some entries but added others. 

Completeness loss would be the number of missing ground truth entries divided by total. Let's see:

Missing in predicted compared to ground truth:

- data_8 (bulk RNA-seq from EGA, public_id phs000915.v2.p2)
- data_13 (bulk RNA-seq from EGA, public_id EGAD00001001244)
- data_20 (bulk RNA-seq from GEO, public_id GSE240058)
- data_18 in ground truth's data_18 was single-cell RNA-seq from GEO (GSE210358), which is present in predicted as data_18. So that's okay. 

Wait, data_13 in ground truth is present as data_13 in predicted but with different omics and source. So that's an error. 

Total missing entries from ground truth: data_8, data_13, data20 (since data20's correct entry isn't fully represented, perhaps). Additionally, data22 in ground truth has a different omics type than the predicted data22? Ground truth data22 is "single cell RNA-seq" which matches predicted data22's omics, but the source and public_id might align? Wait ground truth data22 has source GEO and public_id GSE240058, same as data20 in ground truth? Wait no, data20 in ground truth is bulk RNA-seq with GSE240058, but data22 is single cell with same public ID? That might be a mistake in ground truth. But assuming that's correct, the predicted data22 has same public ID but omics type matches data22's. So that's okay. 

Therefore, missing entries: data_8, data_13, data20. Three entries missing. Plus data_13 was replaced but incorrectly. 

Extra entries in predicted: data_8 (as DNA methylation), data_13 (WES), data_17 (Bulk transcriptome), data20 (Genomics). That's four extra. 

Total ground truth data entries: 22. Missing 3 (data8,13,20), so completeness loss is 3/22 ≈ 13.6%, so completeness around 86.4. But also, the extra entries penalize. Since completeness is about covering the ground truth, maybe the penalty is more for missing entries. 

Accuracy: For existing entries, how many are accurate. Let's say:

Out of the 22 in predicted, subtract the incorrect ones. 

data_8 (wrong), data_13 (wrong), data_17 (extra and not in GT), data20 (wrong), plus others:

Wait, data_17 is an extra so not counted in accuracy unless overlapping. 

For entries that exist in both:

data_1 to 7: accurate (except maybe formatting like "bulk RNA-seq" vs "Bulk RNA-sequencing", but that's semantic equivalence? Probably acceptable.)

data_9-12: accurate. 

data_14-16: accurate. 

data_18-19: accurate. 

data21: accurate. 

data22: accurate (since public ID and omics type match data22 in GT).

But data_8 (GT's data8 is missing in predicted's data8 which is wrong), so those entries are inaccurate. 

The accuracy would lose points for the incorrect entries. Let's say 5 incorrect entries (data8, data13, data20, data17 (if counts), and maybe others). 

This is getting complex. Maybe overall, the data component has some inaccuracies and missing entries leading to maybe 70-80% accuracy and completeness. 

Structure-wise, all entries have correct keys? Let me check:

In ground truth, each data entry has id, omics, source, link, format, public_id. Predicted seems to have all those keys. So structure is good. 

So Data Score: Maybe 80 (structure 100, accuracy maybe 85, completeness 75 average? Final composite? Or per aspect. Wait the user wants separate scores for each component (Data, etc.), each scored out of 100 based on structure, accuracy, completeness. Wait the criteria says each component gets a single score considering all three aspects. 

Wait the instructions say "assign a separate score (0-100) for each of the three components. The score for each component is based on three evaluation aspects: 1.Structure, 2.Accuracy, 3.Completeness."

Hmm, so each component's score is a combination of the three aspects. But how to combine them? The user says "Gap-Based Scoring: Score based on the gap between predicted annotation and ground truth, not rigidly on the above rules". So perhaps consider each aspect's contribution and then compute an overall score per component. 

Alternatively, maybe each aspect contributes equally (each 0-100, then average?) or weighted. Since the user didn't specify, perhaps just judge overall.

For Data component:

Structure: Perfect, so 100.

Accuracy: Some entries are wrong (like data8, data13, data20), others are extra but not part of GT. Let's estimate accuracy as maybe 70% (since about a third of entries have errors). 

Completeness: Missing 3 out of 22 (~13%) and adding extras, so completeness could be around 85%. 

So combining, maybe around 80?

Wait perhaps more precisely:

Accuracy: For the entries present in both, how many are accurate. Let's say total entries in predicted that overlap with GT (excluding extras):

If predicted has 22 entries, but 4 are extras (data8, data13, data17, data20), so 18 overlapping. Of those 18, some have errors:

- data8 in GT is missing in predicted's data8 (which is wrong), so that's a miss.

- data13 is incorrect.

- data20 is incorrect.

Additionally, data_17 is an extra so not part of comparison. 

Wait perhaps it's better to compare each entry in GT and see if they're present in predicted with correct attributes. 

For example:

Ground truth's data_8 (bulk RNA-seq from EGA) is not in predicted. So that's a missing entry. 

Ground truth's data_13 (bulk RNA-seq from EGA, public_id EGAD...) is replaced in predicted as WES, so that's an error. 

Ground truth's data20 (bulk RNA-seq from GEO, GSE240058) is in predicted's data20 as Genomics, which is wrong. 

Other entries like data22 in GT is present as data22 in predicted with correct public_id and omics type (single cell RNA-seq), so that's okay. 

So for accuracy, the entries that are present in both but incorrect: data8 (missing), data13 (wrong), data20 (wrong). So 3 entries incorrect. 

Plus, other entries like data_8 in predicted is an extra but incorrect. 

Overall, maybe accuracy is (number of correct entries / total in GT) *100? 

Total correct entries: (22 GT entries minus 3 missing/incorrect ones) = 19? Or more precisely:

For each GT data entry:

Check if there's a corresponding entry in predicted with same omics, source, public_id, etc. 

Let me go through each GT data entry:

1. data_1: present and correct. 

2. data_2: correct. 

3. data_3: correct. 

4. data_4: correct. 

5. data_5: correct. 

6. data_6: correct. 

7. data_7: correct. 

8. data_8: Not present correctly in predicted (predicted's data8 is DNA methylation instead). So incorrect. 

9. data_9: correct. 

10. data_10: correct. 

11. data_11: correct. 

12. data_12: correct. 

13. data_13: Not present correctly (predicted's data13 is WES). Incorrect. 

14. data_14: correct. 

15. data_15: correct. 

16. data_16: correct. 

17. data_17: Not in GT (GT's data17 is single-cell RNA-seq from GEO GSE151426, but predicted's data17 is Bulk transcriptome from National... which is different). Wait, GT's data17 is:

Ground truth data17: omics "single-cell RNA-seq", source GEO, link to GSE151426, public_id GSE151426. 

Predicted data17 has "Bulk transcriptome", so different omics. So that's an error. But in GT data17 exists and predicted has data17 as different. So that's another incorrect entry. 

Wait did I miss that earlier?

Yes, data17 in GT is present in predicted but with wrong omics. So that's another error. 

So data_17 is also incorrect. 

Continuing:

18. data_18: correct (matches predicted's data18). 

19. data_19: correct. 

20. data20: incorrect in predicted (Genomics vs bulk RNA-seq). 

21. data21: correct. 

22. data22: correct (same public ID and omics type). 

So now counting incorrect entries in GT:

data_8 (missing), data13 (wrong), data17 (wrong), data20 (wrong). So 4 incorrect entries. 

Therefore, correct entries: 22 -4=18. So accuracy would be 18/22≈81.8%. 

Completeness: The predicted has 22 entries but only 18 of them correspond to GT entries (since 4 are extras). So completeness is (18/22)*100 ≈81.8%. 

But also, the missing entries in GT (like data8, data13, data20) mean that the predicted didn't cover them, so completeness is about coverage of GT, so the number of GT entries found in predicted correctly divided by total GT entries. 

Thus, 18/22 ≈81.8% for completeness as well. 

Adding structure (100%), the total score might be average of the three? Or considering structure is perfect, maybe (81.8 +81.8)/2 ≈81.8, rounded to 80 or 85. 

Perhaps giving a Data score of 80. 

Now moving to Analyses component.

**Analyses Component Evaluation**

First, check structure. The analyses in predicted seem to be valid JSON. Each has id, analysis_name, analysis_data. Some have labels. Structure looks okay except maybe one entry: analysis_7 in predicted uses "data" instead of "analysis_data". Looking at the ground truth's analysis_7:

Ground truth analysis_7: {"id": "analysis_7", "analysis_name": "ATAC-seq", "data": ["data_2"]}. Wait, actually in ground truth, analysis_7 uses "data" instead of "analysis_data"? Let me check ground truth's analyses:

Yes, in ground truth, analysis_7 has "data": ["data_2"], whereas others use "analysis_data". The user's instruction says objects should follow proper structure. So in ground truth, this is an inconsistency. But the predicted's analysis_7 also uses "data": [...] which matches the ground truth's mistake. So structurally, both have the same error, so maybe not penalizing? Or does the schema require "analysis_data"?

Assuming the correct key is "analysis_data", then in ground truth analysis_7 and predicted analysis_7 both have "data" instead of "analysis_data", making their structure invalid. But since both have the same issue, maybe not deducting points here. Or if structure requires correct keys, then this is an error. 

But the user says "Verify that each object follows a proper key–value structure." If the standard key is "analysis_data", then using "data" is wrong. So both have structure issues here. But since the user says to compare predicted vs ground truth, and both have the same error, maybe structure is still okay? Or not. 

This is a bit ambiguous. Let's assume that the correct key is "analysis_data", so in both cases, it's an error, but since the predicted matches the ground truth's mistake, maybe it's acceptable for structure? Or structure must be valid regardless. 

Assuming structure is supposed to be correct, then this is a problem. But since both have it, maybe the structure score is reduced because of this. 

Looking further, the rest of the analyses entries in predicted seem to have "analysis_data" except analysis_7 and analysis_9. Let me check:

Analysis_7 in predicted has "data": [...] 

Analysis_9: same as ground truth's analysis_9, which also has "data": [...]. 

Similarly, in ground truth, analysis_9 also uses "data" instead of "analysis_data". 

So in the analyses section, two entries (analysis_7 and analysis_9) have incorrect keys. This would deduct structure points. 

How many analyses are there? Ground truth has 22 analyses. Let's see how many have the error:

analysis_7: yes

analysis_9: yes

any others?

Looking at predicted analyses:

analysis_7, analysis_9 use "data".

Others like analysis_1 has "analysis_data".

So 2 out of 22 entries have incorrect keys. So structure is mostly correct except those two. So structure score would be (20/22)*100 ~90.9. Maybe 90? 

Next, accuracy and completeness.

Looking at the analyses:

First, check if the analysis entries in predicted match the ground truth in terms of analysis name, linked data, and labels.

Starting with analysis_1 to analysis_6:

analysis_1: same as GT.

analysis_2: same as GT (label matches).

analysis_3: in GT analysis_3 has analysis_data: ["data_6","data_7","data_8","data_9","data_10"], but in predicted analysis_3's analysis_data includes data_8 which in GT was data_8 (bulk RNA-seq from EGA). However in predicted, data_8 is DNA methylation. Does that matter? The analysis links to data_8, which in predicted is incorrect data, but the analysis itself is about Transcriptomics. Since the analysis's purpose may still be correct even if the data is wrong, but the linkage to incorrect data might affect accuracy. 

Wait, the analysis's accuracy depends on whether it's correctly associated with the right data. Since the data linked (data_8) in predicted's analysis_3 is incorrect (because in GT data_8 was a different dataset), the analysis is inaccurately connected. So this affects accuracy. 

This complicates things. Let's proceed step by step.

Analysis_3 in GT: data_6,7,8,9,10. In predicted, analysis_3's analysis_data includes data_8 (which is DNA methylation instead of the original bulk RNA-seq). So the linkage is incorrect. Thus analysis_3 is inaccurate.

Similarly, analysis_4 in GT has analysis_data: ["analysis_1", "data_5", "analysis_3"]. In predicted analysis_4 is called "Functional Enrichment Analysis" and analysis_data is ["analysis_2", "data_10", "analysis_2"], which is different from GT. So analysis_4 is different in both name and data sources, so inaccurate.

Analysis_5 in predicted is similar to GT (name Differential Analysis, but data is same as GT's analysis_5 (analysis_1). The label also matches. So analysis_5 is accurate.

Analysis_6 in GT is "Functional Enrichment Analysis" linked to analysis_5. In predicted analysis_6 also has that. So accurate.

Analysis_7 and 9 have the key error but their content (analysis name and data linkage) may be accurate otherwise. 

Analysis_7 in predicted has analysis_name "ATAC-seq" and data ["data_2"], which matches GT's analysis_7 (same name and data). So content is accurate despite the key error. 

Analysis_8: same as GT.

Analysis_9: same as GT (name ChIP-seq, data ["data_4"]). 

Analysis_10: same as GT (Transcriptomics linking data6 and7).

Analysis_11 in GT is "Differential Analysis" with data ["analysis_10", "data_14", "analysis_1"], but predicted analysis_11 is "Single cell Clustering" with analysis_data ["analysis_14", "data_2", "analysis_11"], which is different. The label in predicted is a string "q8mr5A_NUI" which doesn't match GT's HC labels. So analysis_11 is incorrect.

Analysis_12: in GT it's "Single cell Transcriptomics" linked to data3. In predicted analysis_12 same. Accurate.

Analysis_13 in GT is "Single cell Clustering" linked to analysis9. In predicted analysis_13 is "Least Square (sPLS) regression" linked to analysis8. So different name and data, so inaccurate.

Analysis_14 in GT is "Transcriptomics" linked to data11. In predicted analysis_14 is "Bray‒Curtis NMDS" linked to data7. Different name and data. Inaccurate.

Analysis_15: same as GT (PCA on analysis_11).

Analysis_16: same as GT.

Analysis_17 in predicted is "Weighted key driver analysis..." linked to analysis15. In GT, analysis17 is PCA on analysis16. So different analysis name and data linkage. 

Analysis_18: same as GT (Transcriptomics on data13, but data13 in predicted is wrong but the analysis's own data linkage is correct (to data13, even if data is wrong, but analysis's purpose may still be valid? Or does the analysis's accuracy depend on the correctness of the data it references?)

Hmm, analysis_18's data is data13 which is incorrect (WES instead of bulk RNA-seq), so the analysis's association is incorrect. Thus the analysis itself is inaccurate because it's using wrong data.

Analysis_19: In GT analysis19 is "Principal component analysis" on analysis18 and data15. In predicted analysis19 is "Proteomics" linked to analysis18. So different name and data. Inaccurate.

Analysis_20: In GT, analysis20 is Single cell Transcriptomics on data17,18,19. In predicted analysis20 has those data entries (data17 is different but the IDs match?), so data17 in predicted is "Bulk transcriptome", but the analysis is still named correctly. However, the data linked (data17) is incorrect. So the analysis's purpose may be off, but the name and linkage are as per GT's structure. Hmm, maybe partially accurate?

Wait data17 in GT is single-cell RNA-seq, but in predicted it's Bulk transcriptome. So analysis20's data linkage is incorrect (data17 is wrong), so the analysis is inaccurate.

Analysis_21 and 22: 

analysis21: same as GT except data16 and analysis20, which might still be correct.

analysis22: same as GT.

So going through all analyses:

Accurate analyses:

analysis_1,2,5,6,7,8,9,10,12,15,16,21,22 – that's 13 out of 22.

Inaccurate: analysis3,4,11,13,14,17,18,19,20. Total 9.

Plus analysis_7 and 9 have structural errors but content accurate. 

Accuracy: (13/22)*100≈59%. But some may have partial accuracy. 

Completeness: The GT has 22 analyses. The predicted also has 22. But some are misplaced. For example, analysis_14 in GT is present but with wrong details. So completeness considers if all GT analyses are present in predicted with correct links. 

Some analyses are completely missing in predicted? Let me check:

GT's analysis_19: in predicted it's present but with different name/data. So not missing. 

All GT analyses are present in predicted but some are incorrect. So completeness is about having the same count and covering the necessary steps. Since all are present but some are wrong, completeness might be lower. 

Alternatively, completeness is how much of the GT analyses are present in predicted with correct data. So of the 22, 13 are accurate, so completeness 13/22≈59%.

Combined with structure (90 for structure), perhaps the Analyses score is around 70? Because structure is 90, accuracy ~60, completeness ~60. Average might be 70. 

But need to think carefully. 

Alternatively, structure penalty: 2 errors out of 22 entries. Each analysis entry has structure issues in 2 cases. So structure score 20/22*100≈90. 

Accuracy: let's say 13/22≈59. 

Completeness: similar to accuracy since it's about presence and correctness. 

Overall, maybe 70. 

Finally, Results component.

**Results Component Evaluation**

Ground truth has one result: analysis_id "analysis_11", features ["IL1RL1", "KRT36", "PIK3CG", "NPY"].

Predicted results have analysis_id "analysis_10", metrics "average prediction accuracy", value "nn^4f&8", features ["WfsKYfo", "3Ez3dzK", "EQIY7fH", "5e8HkpPKK"].

So the analysis linked is different (analysis_10 vs analysis_11). The features are all different strings, not matching. Metrics and value fields are present but have random values. 

Structure: The result object has the required fields (analysis_id, features). The other fields (metrics, value) are optional? Ground truth left them empty, while predicted filled them with nonsense. Structure-wise, it's valid JSON, so structure score 100. 

Accuracy: The analysis linked is wrong (analysis_10 vs analysis_11). Features don't match at all. So accuracy is 0%? 

Completeness: Only one result expected. Since it's entirely incorrect, completeness is 0%. 

Thus, Results score would be 0. 

But wait, maybe the user allows some leniency. If the analysis_id is wrong but the features are somewhat related, but here they are random. So probably 0. 

Putting it all together:

Data: 80

Analyses: 70 

Results: 0 

Wait but let me recheck Results:

If the structure is okay (yes), but the content is completely off. So Accuracy 0, Completeness 0, Structure 100. 

Average would be (0+0+100)/3 ≈33.3, but according to the criteria, it's based on the gap between predicted and GT. Since it's entirely wrong except structure, maybe 10? 

Alternatively, the user might say structure is 100, but accuracy and completeness 0, so overall 0. 

Probably the results get 0. 

Final scores would be:

Data: 80 

Analyses: 70 

Results: 0 

But maybe I missed something. 

Wait in Analyses, the analysis_7 and 9 have key errors (using "data" instead of "analysis_data"), but since both the ground truth and predicted have the same error, maybe structure isn't penalized? Or does the structure require the correct key regardless of the ground truth's mistake?

The instructions say "Confirm that the component is valid JSON. Verify that each object follows a proper key–value structure." So if the proper key is "analysis_data", then using "data" is an error. Even if GT made the same mistake, the predicted is still incorrect. So structure is penalized. 

Thus, structure score for Analyses is 20/22 correct keys → ~90. 

But perhaps the keys were supposed to be "data" in some cases? Like in analysis_7, the analysis is ATAC-seq, which is a data type, so maybe "data" is correct? Wait no, analysis_data refers to the data inputs to the analysis. The key "analysis_data" is likely intended, so "data" is a mistake. 

So structure is indeed penalized. 

Another possible error: analysis_13 in predicted has "Least Square (sPLS) regression" which might not be present in GT, but the structure is okay. 

Thus, my initial scores hold. 

Final JSON:

{
    "Final Scores": {
        "Data": 80,
        "Analyses": 70,
        "Results": 0
    }
}
</think>

{
    "Final Scores": {
        "Data": 80,
        "Analyses": 70,
        "Results": 0
    }
}

### Explanation:
#### **Data Component (80/100)**
- **Structure**: Valid JSON with proper key-value structure for most entries. Minor inconsistencies (e.g., `data20` missing an underscore) were ignored as per instructions.  
- **Accuracy**:  
  - Missed/Incorrect entries:  
    - `data_8` (DNA methylation instead of bulk RNA-seq),  
    - `data_13` (WES instead of bulk RNA-seq),  
    - `data_20` (Genomics instead of bulk RNA-seq),  
    - `data_17` (Bulk transcriptome not in GT).  
  - Some entries (e.g., `data_22`) matched GT. Overall, ~70% accuracy.  
- **Completeness**:  
  - Missed 3 GT entries (`data_8`, `data_13`, `data_20`).  
  - Added 4 extra entries.  
  - ~85% completeness due to partial overlap.  

#### **Analyses Component (70/100)**  
- **Structure**:  
  - `analysis_7` and `analysis_9` used "data" instead of "analysis_data" (incorrect key), reducing score.  
  - Other entries were structurally valid.  
- **Accuracy**:  
  - ~59% accuracy due to incorrect analysis names/data links (e.g., `analysis_4`, `analysis_11`, `analysis_14`).  
- **Completeness**:  
  - All GT analyses were present but 40% had incorrect configurations.  

#### **Results Component (0/100)**  
- **Structure**: Valid JSON but irrelevant.  
- **Accuracy/Completeness**:  
  - Incorrect `analysis_id` (`analysis_10` vs `analysis_11`).  
  - Features and metrics were nonsensical, providing no meaningful results.  
  - Zero alignment with GT.