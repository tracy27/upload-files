Okay, I need to evaluate the predicted annotation against the ground truth for each component: Data, Analyses, and Results. Let's start with the Data component.

**Data Component Evaluation:**

First, check Structure. The predicted Data array has five entries, just like the ground truth. Each entry has id, omics, link, format, source, public_id. All keys seem present except some fields are empty. The JSON structure looks valid here. No obvious syntax errors, so Structure is good. 

Accuracy: Compare each data item. 

- Ground Truth Data_1 is DNA sequencing from EGA with ID EGAS00001004582. Predicted Data_1 has omics empty, but link and format are filled, but source and public_id are empty. Since omics type is missing, that's a problem. The source should be EGA, but it's empty. So this is inaccurate.

- Data_2 in GT is RNA seq, EGA same public ID. Predicted Data_2 has omics empty, source empty. Same issue as Data_1. Missing correct omics type and source.

- Data_3 in GT is digital pathology, no source/public ID. Predicted has source Mendeley Data Portal and a public ID, but omics is empty. The omics should be digital pathology, so that's wrong. Also, the source and public ID are incorrect since GT says they're empty. So inaccurate.

- Data_4 in GT is treatment data, source empty. Predicted lists TCGA as source, which is wrong. Omics field is empty instead of treatment data. So both wrong.

- Data_5 in GT is clinical features. Predicted has ProteomeXchange as source and public ID, but omics is empty. So again, missing the correct omics type and wrong source/public info.

So Accuracy is very low because most fields (omics types, sources) are missing or incorrect. Only the IDs and links are there but other critical info is off. Maybe 10% accurate? But also check if some parts are semantically equivalent. Like, maybe the format "original and matrix" vs nothing? Not really. The key problem is omics types are missing entirely. 

Completeness: The predicted has all five data entries, matching the count. However, each entry lacks the correct information. Since completeness is about covering all relevant objects, but the entries exist but are incorrect. Maybe completeness isn't penalized here because the objects are present, but their content is wrong. Wait, but the criteria says to count semantically equivalent as valid. Since the actual content (like omics type) is missing, the objects aren't equivalent. So maybe completeness is 100% because they have all the entries, but accuracy is low. Hmm. The instructions say completeness is about coverage of relevant objects, so if they have all the entries, but their content is wrong, then completeness is okay, but accuracy is bad. So perhaps Completeness is 100, Accuracy 20%, Structure 100. So total Data score would be (Structure 100 + Accuracy 20 + Completeness 100)/3 = around 77, but maybe the Accuracy is worse. Let me think again.

Wait, the Accuracy part requires semantic equivalence. For example, Data_1's omics is DNA sequencing, but predicted leaves it blank. That's not semantically equivalent. Similarly, sources are wrong. So none of the Data entries are accurate. So Accuracy could be 0%. Because all entries have critical missing info. Completeness is 100 because all entries exist, but they are incorrect. So maybe:

Data Accuracy: 0% (since none are accurate)
Data Completeness: 100% (all objects present but wrong content)
Structure: 100 (valid JSON)

Total Data score: (0+100+100)/3 ≈ 66.67. But maybe some entries have partial correctness? For example, Data_3 in GT has no source, but predicted has one. But that's not correct. The presence of a source where there shouldn't be is a mistake. So probably Accuracy is 0. Hence, Data gets 67?

Wait, maybe some entries have parts right. For instance, Data_1's link exists but GT's link is empty. Not sure if that matters. Since the ground truth doesn't require links, maybe it's acceptable. But the key points are omics and source. Since those are all wrong, I'll stick with 0 for Accuracy. So Data score is 66.66, rounded to 67.

Moving on to Analyses:

**Analyses Component Evaluation:**

Structure first. The predicted analyses array has 12 items (up to analysis_11). The ground truth has 11 (analysis_1 to analysis_11). Wait, let me check:

Ground truth analyses: analysis_1 to analysis_11 (total 11). Predicted has analysis_1 to analysis_11, but looking at the JSON, the predicted has analysis_3, analysis_6, analysis_8 as empty or invalid. Wait, let me recount:

Looking at the predicted analyses array:

1. analysis_1 – ok
2. analysis_2 – ok
3. analysis_3 has analysis_name empty and analysis_data as "", which is invalid (should be array). The label has a string "CumdL366", but according to GT, labels can be objects or empty strings. Wait, in ground truth, analysis_3 has label as empty string. Here, the label is a string, but in other analyses, like analysis_5, it's an object. The structure might be invalid here if the schema expects a specific format. But the user said to check validity. If the analysis_data is set to a string instead of array, that's a structure error. So analysis_3's analysis_data is "" instead of array. So that breaks structure. Similarly, analysis_6 has analysis_name and analysis_data as empty strings, which may be invalid. 

Also, analysis_8 has analysis_name and data as empty. These structural issues mean the overall structure is invalid. So Structure score would be lower. Let's see:

Possible structure issues:
- analysis_3.analysis_data is a string instead of array.
- analysis_6.analysis_data is empty string instead of array.
- analysis_8.analysis_data is empty string.
These are structure errors. So Structure score might be 80% (if 3 out of 11 entries have structure issues). Let me count how many analyses are structurally correct.

Total analyses in predicted: 12 (Wait, no, the user's predicted has up to analysis_11, which is 11 entries. Let me confirm:

The predicted analyses array has entries from analysis_1 to analysis_11, so 11 items. Of these:

analysis_3: analysis_data is "", which is a string not array → invalid
analysis_6: analysis_data is "" → invalid
analysis_8: analysis_data is "" → invalid

Thus, 3 analyses have invalid structure. So 8/11 are correct. So Structure score: (8/11)*100 ≈ 73% or 70. Hmm, maybe deduct 30 points for structure? Or maybe more. Alternatively, if any structural error drops it to 50. Since three entries have invalid structures, perhaps Structure score is 70 (assuming 3/11 is ~27% errors).

Accuracy next. Compare each analysis to ground truth.

Ground truth analyses:

analysis_1: sWGS/WES on data_1 – predicted matches exactly (name and data). Accurate.

analysis_2: HLA typing on data_1 – same as predicted. Accurate.

analysis_3: HRD on data_1. Predicted analysis_3 has empty name and analysis_data is invalid. So this is completely wrong. So inaccurate.

analysis_4: RNA-seq on data_2 – matches predicted. Accurate.

analysis_5: differential RNA expr on analysis_4. Matches predicted (name and data). Accurate.

analysis_6: classifier on data5. GT's analysis_6 is classifier using data5. Predicted analysis_6 has empty name and data. So missing this analysis's info. 

Wait, in the predicted, analysis_6 is empty, but analysis_7 in predicted corresponds to analysis_7 in GT (classifier on data5 and data1). Wait, let's map them properly.

Ground truth analyses 6-11:

GT analysis_6: classifier on data5 → corresponds to predicted analysis_6? But predicted analysis_6 is empty. 

GT analysis_7: classifier on data5+data1 → predicted analysis_7 matches (analysis_7 in predicted has data5 and data1). 

GT analysis_8: classifier on data5+data2 → predicted analysis_8 is empty (analysis_8 has empty fields). 

GT analysis_9: data5+data1+data2 → predicted analysis_9 has those (analysis_9 in predicted is correct).

GT analysis_10: adds data3 → predicted analysis_10 has data5,data1,data2,data3 → correct.

GT analysis_11: adds data4 → predicted analysis_11 has data5,1,2,3,4 → correct.

So the discrepancies are:

- analysis_3 (GT HRD vs predicted empty)
- analysis_6 (GT classifier on data5 → predicted analysis_6 is empty)
- analysis_8 (GT classifier on data5+data2 → predicted analysis_8 is empty)
- analysis_3's data is wrong (invalid structure too)

Additionally, the predicted has analysis_3 with analysis_name empty and invalid data. So those are incorrect entries.

Accuracy-wise, how many analyses are accurately represented?

Correct ones:

analysis_1,2,4,5,7,9,10,11 → 8 out of 11. But analysis_3 and 6,8 are incorrect. So 8/11 accurate. However, analysis_3 in GT is HRD, which is missing in predicted. The predicted has analysis_3 as invalid, so it's an incorrect entry. 

But also, the analysis_6 in predicted (empty) doesn't match GT's analysis_6. Similarly analysis_8 is missing. So total accurate analyses are 8 (excluding 3,6,8). So accuracy ratio is 8/11 ≈ 72%. But also, some analyses might have partial correctness. For example, analysis_3 in predicted has wrong data and name, so 0. 

Additionally, the analysis_3's existence in predicted but wrong content counts as incorrect. So Accuracy score around 70%?

Completeness: The predicted has all analyses except analysis_3,6,8 are either incorrect or partially present. Wait, but the total count is correct (11 entries). However, some entries are placeholders (like analysis_3,6,8) that don't correspond to GT's analyses. Thus, completeness is about whether all GT's analyses are covered. Since analysis_3 (HRD), analysis_6, and analysis_8 are missing in terms of correct content, they are incomplete. So completeness is (number of correct analyses)/total GT analyses. 

Out of 11 GT analyses, 8 are correctly present (analysis_1,2,4,5,7,9,10,11). So 8/11 ≈ 73%. 

So Completeness is ~73%.

Structure: 73% (from earlier calculation)

Accuracy: 73%

Completeness: 73%

Total for Analyses: (73 + 73 + 73)/3 = 73. So maybe 73? But need to adjust for structure. Wait, structure was 73% (from 8 correct entries out of 11 in structure). Wait, actually structure score is based on validity. Even if entries exist but have wrong structure, like analysis_3's data being a string instead of array, that's a structural error. So maybe Structure is 73 (as per entries without structural issues), so adding up:

Structure 73, Accuracy 73, Completeness 73 → 73 average. But maybe Structure is lower because some entries have more than one error. For example, analysis_3 has both invalid data and name, but the main structural issue is the data field type. So perhaps Structure is 8/11=72.7, so ~73. 

Alternatively, if any structural error brings it down further. Maybe deduct 30 points for structure if 3 entries are invalid. Let me think differently: if 3 out of 11 analyses have structural issues, maybe Structure is (11-3)/11 *100 ≈ 72.7. So ~73.

Alternatively, maybe each structural error reduces the score by a certain amount. Since three analyses have structural problems (each having invalid structure in their data field), maybe each such error takes away 10 points. 3*10=30, so 100-30=70. Then Structure score is 70. 

Hmm, this is a bit ambiguous. Since the user says "valid JSON" and proper key-value structure. The analyses array itself is valid JSON, but individual objects have invalid structures (like analysis_data as string instead of array). Those are structural issues. So maybe Structure is 70. 

Then, recalculate:

Structure 70, Accuracy 73, Completeness 73 → average is (70+73+73)/3 ≈ 72. So maybe 72 for Analyses.

Now **Results Component**:

Structure first. Check if the results array is valid JSON. Looking at the predicted results:

The entries have analysis_id, metrics, value, features. Some fields are empty strings or missing. For example, first result has analysis_id empty, metrics "average prediction accuracy", value -3806, features empty. The second has empty metrics and value. The third onwards have proper entries. 

Each entry must have the required keys. Assuming the schema allows empty strings, the structure is okay as long as all keys are present. The JSON seems valid. So Structure is 100%.

Accuracy: Compare each result in predicted to GT.

Ground truth Results:

- analysis_5 has features list (CDKN2A etc.), metrics empty, value empty.
- analysis_6: AUC 0.7
- analysis_7: 0.8
- analysis_8: 0.86
- analysis_9: 0.86
- analysis_10: 0.85
- analysis_11: 0.87

Predicted Results:

Entry 1: analysis_id empty, metrics "average...", value -3806, features empty. Doesn't match any GT result.

Entry 2: all empty. Not corresponding.

Entry3: analysis_7, AUC 0.8 → correct (matches GT analysis_7's 0.8).

Entry4: analysis_8, AUC 0.86 → correct (GT analysis_8 has 0.86).

Entry5: all empty.

Entry6: analysis_10, AUC 0.85 → correct (GT analysis_10 has 0.85).

Entry7: empty.

So correct entries are entries 3,4,6. 

Missing are:

- analysis_5 (the features entry)
- analysis_6 (0.7)
- analysis_9 (0.86)
- analysis_11 (0.87)

The predicted has analysis_9's result missing (there's no entry for analysis_9?), wait check:

Wait, in predicted results:

Looking at the entries:

analysis_5's result in GT is the first entry with features. In predicted, the first result entry has analysis_id empty, so doesn't correspond.

analysis_6's result (0.7) is missing in predicted. The predicted has analysis_7 starts at third entry.

analysis_9's result (0.86) is missing. The predicted entry4 is analysis_8 (correct), entry6 is analysis_10 (correct). So analysis_9 is missing.

analysis_11 is also missing (no entry for analysis_11).

Additionally, the first entry in predicted is incorrect and doesn't exist in GT. The second and fifth and seventh entries are empty, which might be considered as extra or incomplete.

So total correct entries: 3 (analysis_7,8,10). 

Total GT results: 7 entries. So accuracy is 3/7 ~42.8%. But also, analysis_9's result in GT has AUC 0.86, which is same as analysis_8's in GT. The predicted analysis_8's value is correct, but analysis_9 is missing. So maybe the accuracy for existing correct ones is 100% but they only cover 3 out of 7.

Completeness: How many of the GT results are present in predicted?

Only analysis_7,8,10 are present. Missing analysis_5,6,9,11. So completeness is 3/7 ~42.8%.

But also, the predicted has extra entries (first, second, fifth, seventh) which are not in GT. These should penalize completeness. The formula says to penalize for missing or extra objects. 

Completeness score is about how well it covers GT and doesn't add extras. So the number of correct objects over (GT count + extra). Or perhaps it's (correct / GT count) minus penalty for extra? Not sure. The instruction says "count semantically equivalent objects as valid, even if wording differs. Penalize for any missing or extra."

So the correct objects are 3. The GT has 7. So completeness is 3/7 ≈43%. Additionally, the extra entries (4 entries with no GT equivalents) reduce the score. So maybe completeness is (3)/(7 +4)= 3/11≈27%? Not sure. Alternatively, the standard is:

Completeness = (Number of correct objects) / (Number of GT objects) → 3/7 ~43%, but since there are extra objects, which are irrelevant, that's a penalty. Maybe completeness is 3/(7+4) * 100 = 21%, but that might be too harsh. Alternatively, the penalty is subtracted. The user says "penalize for any missing objects or extra irrelevant objects." So the base completeness is (correct / GT) *100, then subtract penalties for missing and extra. 

Alternatively, a common approach is: 

Completeness = (Correct objects / Total GT objects) * 100 - (Extra objects / (Correct + Extra)) * some factor. 

This is getting complicated. Maybe the user expects that Completeness is (correct / GT_total) *100, so 42.8%, and then the extra entries are part of the Accuracy or another deduction? Since the instructions mention penalizing for both missing and extra, perhaps the Completeness is (correct / (GT + extra)) but not sure. Alternatively, the Completeness is calculated as how much of the GT is covered, so 42.8%, and the extra is a separate penalty affecting the overall score. But according to the criteria, the three aspects are Structure, Accuracy, Completeness. So maybe Completeness is 42.8%, and the Accuracy is how accurate the correct ones are. The correct entries (analysis7,8,10) have accurate metrics and values, so their Accuracy is 100% for those. But the rest are missing or incorrect. 

Thus, Accuracy for Results: 

For the 3 correct entries, they are accurate. The others are either missing or incorrect. So Accuracy is (3/7)*100 ≈42.8%. 

Completeness is also ~42.8% since only 3/7 are present. 

Structure is 100.

So Results score would be (100 +42.8+42.8)/3 ≈ 61.5. Maybe 62.

Wait, but maybe the first entry in predicted (with analysis_id empty and weird metrics) is an extra irrelevant object, so that reduces Completeness. The Completeness score might be calculated as:

( Number of correct ) / ( Number of GT ) → 3/7 ~43% 

Penalty for extras: The predicted has 7 entries, but GT has 7. The extras are those beyond the correct 3? Or the extras are the non-GT entries. Since the total entries in predicted are 7 (same as GT), but some are incorrect. It's tricky. 

Alternatively, Completeness is about how much of the GT is covered. Since 3/7 are covered, that's 43%. The extra entries (the incorrect ones) are not counted as they don't match anything. So Completeness stays at 43%. 

Therefore, Results score components:

Structure: 100

Accuracy: 42.8 (for correct entries' accuracy) but maybe some entries have partial accuracy. For example, analysis_7's entry is correct. The first entry in predicted has wrong metrics ("average prediction accuracy" vs GT's "metrics" being empty but for analysis_5 it has features. Not sure. But analysis_5's result in GT has metrics empty and features listed. The predicted doesn't capture that, so that's a miss. 

Overall, Results Accuracy: 42.8, Completeness 42.8, Structure 100 → average ~61.5. So 62.

Final scores:

Data: ~67

Analyses: ~72

Results: ~62

But let me recheck:

For Data:

- Structure: Valid JSON, so 100
- Accuracy: All entries have incorrect omics and sources. Only the IDs are correct. Since the omics types are essential, this is a major failure. So Accuracy 0?
- Completeness: All 5 entries exist, so 100.

Thus Data score is (100+0+100)/3 = 66.66 → 67.

Analyses:

Structure: 73 (if 8/11 valid)

Accuracy: 8/11 ≈72.7 → 73

Completeness: 8/11 ≈72.7 →73

Average: 73

Results:

Structure:100

Accuracy:3/7 ~42.8

Completeness:3/7 ~42.8

Average (100+42.8+42.8)/3 ≈61.5 →62

So Final Scores JSON would be:

{
    "Final Scores": {
        "Data": 67,
        "Analyses": 73,
        "Results": 62
    }
}

But maybe the Analyses Structure was lower. Let me think again about Structure for Analyses.

In the predicted analyses:

analysis_3 has analysis_data as a string ("") instead of an array. That's invalid structure. So that analysis is invalid. Similarly analysis_6 and analysis_8 have analysis_data as "" (string) instead of array. So three analyses have structural issues. So out of 11, 8 are structurally correct. So Structure score is (8/11)*100 ≈72.7 →73.

Yes, so Analyses Structure is 73. So the averages hold.

Alternatively, if the structure is considered 0 for those three entries, but the rest are okay, but the entire analyses array is still valid JSON. So the overall structure is valid, but individual entries have errors. The user's instruction says "confirm component is valid JSON". The entire analyses array is valid, but some objects within have incorrect structures (like analysis_data being a string instead of array). Are those structural errors?

Yes, because the schema likely requires analysis_data to be an array of strings. Using a string instead makes that object invalid. Therefore, those three entries have invalid structures, thus making the overall component's structure invalid? Or does the component still pass as valid JSON as long as the top-level is okay? The user says "component is valid JSON". The analyses array is a valid JSON array, but some objects inside have wrong types. However, JSON doesn't enforce schema, just syntax. So technically, the JSON is valid. So Structure for Analyses would be 100? Wait, that contradicts my previous thought.

Ah! This is crucial. The Structure criteria is about valid JSON and proper key-value structure. The JSON itself is valid regardless of the data types (even if they're wrong), because JSON allows any types. So the structure score for Analyses would be 100 because the JSON is valid. The errors in analysis_data being strings instead of arrays are about data type (accuracy/completeness) but not structure. Structure is purely about JSON validity and key presence. 

Oh, this changes things! Because the user's Structure criteria is about JSON validity and key-value structure (proper keys). The keys are present (analysis_name, analysis_data, label). Even if the values have wrong types (like analysis_data being a string instead of array), that's an accuracy issue, not structure. Structure is only about the JSON syntax and key existence. 

Therefore, Analyses Structure is 100 because all entries have the required keys and the JSON is valid. 

So redoing Analyses:

Structure: 100

Accuracy: 8/11 (analysis_1,2,4,5,7,9,10,11 are accurate; analysis_3,6,8 are wrong). So 8/11 ≈72.7 →73

Completeness: The predicted has all 11 analyses entries. However, some are incorrect. Completeness measures how many of the GT analyses are present. Since analysis_3,6,8 are not accurately represented, but their entries exist (though incorrect), does that count? 

Wait, Completeness is about coverage of GT's objects. If an object exists in predicted but is incorrect, it doesn't count toward completeness. So completeness is the number of correct objects divided by GT count. So 8/11 ≈73.

Thus Analyses score: (100 +73 +73)/3 ≈ 82.

That's a big difference. I made a mistake earlier thinking that structural errors were due to data types, but actually structure refers to JSON validity and key presence. So Structure is 100 for Analyses.

Similarly for Data: 

Data entries have all keys (structure is valid). Even though values are missing or wrong, that's accuracy/completeness issues. So Data Structure is 100.

Thus recalculating:

**Revised Data Score:**

Structure: 100

Accuracy: 0% (none of the data entries have correct omics and sources except possibly the IDs)

Completeness: 100 (all entries present)

Thus Data score: (100 +0 +100)/3 ≈66.66 →67.

**Analyses Score:**

Structure:100

Accuracy:73%

Completeness:73%

Avg: (100+73+73)/3 ≈82.

**Results Score:**

Structure:100

Accuracy:42.8% (3 correct out of 7)

Completeness:42.8%

Avg: (100+42.8+42.8)/3 ≈61.5 →62.

Final scores would be:

Data:67, Analyses:82, Results:62.

Wait but let's check Analyses Accuracy again. The accuracy is about how accurate each object is. For analysis_3 in GT (HRD on data1), the predicted analysis_3 has analysis_name empty and analysis_data invalid (string instead of array). So that's not accurate. So for the 11 analyses in GT, how many are accurately captured?

Analysis_1: correct (1)

2: correct (2)

3: wrong (0)

4: correct (3)

5: correct (4)

6: predicted analysis_6 has empty, so wrong (GT's analysis6 is classifier on data5 → not captured)

7: correct (5)

8: predicted analysis_8 is empty, so wrong (GT analysis8 is classifier on data5+2 → not captured)

9: correct (6)

10: correct (7)

11: correct (8)

Wait, analysis_9 in predicted is analysis_9, which is correct (GT analysis_9 uses data5,1,2). So that's correct. analysis_10 and 11 are also correct. So total correct: analysis1-2-4-5-7-9-10-11 →8 correct. So 8/11 is ~72.7% accuracy.

Thus Accuracy is 72.7, so rounded to 73. 

Completeness is same as accuracy here since all entries exist but some are wrong. So 72.7%.

Thus Analyses score: (100 +73 +73)/3 = 82.

Results:

Accuracy is 3/7 (~42.8), Completeness same, so 61.5.

Final scores:

{
    "Final Scores": {
        "Data": 67,
        "Analyses": 82,
        "Results": 62
    }
}

But let me check if the Results' completeness should consider the presence of the entries even if they are incorrect. 

Completeness is about covering all relevant objects. Since the predicted has 7 entries (same as GT), but only 3 are correct, then Completeness is 3/7 ~42.8. The other 4 are either incorrect or extra (but they are part of the 7 entries). So yes, 42.8.

Another point: In the Results, the first entry has analysis_id empty, which might not correspond to any GT result. The GT results all have analysis_ids except the first one which has analysis_5. Wait, in the GT results, the first entry is analysis_5. The predicted first result has analysis_id empty, so it's not matching analysis_5. So that's an extra incorrect entry.

Thus, the completeness is indeed 3 correct out of 7 GT, so 42.8%.

Finalizing the scores accordingly.
</think>

{
    "Final Scores": {
        "Data": 67,
        "Analyses": 82,
        "Results": 62
    }
}

### Explanation:
#### **Data Component (67/100):**
- **Structure**: 100/100  
  Valid JSON with all required keys present for each object.  
- **Accuracy**: 0/100  
  None of the data entries correctly identified omics types (e.g., "DNA sequencing" vs empty), sources (e.g., "European Genome-Phenome Archive" vs empty), or public IDs. Critical metadata was omitted or misreported.  
- **Completeness**: 100/100  
  All five data objects were included, but their content was incorrect.  

---

#### **Analyses Component (82/100):**
- **Structure**: 100/100  
  Valid JSON with all required keys present for each object.  
- **Accuracy**: 73/100  
  8/11 analyses matched the ground truth (e.g., analysis_1, analysis_4, analysis_5, analysis_7–analysis_11). Three analyses (analysis_3, analysis_6, analysis_8) had incomplete or invalid names/data references.  
- **Completeness**: 73/100  
  All 11 analyses were included, but three were inaccurately represented.  

---

#### **Results Component (62/100):**
- **Structure**: 100/100  
  Valid JSON with all required keys present for each object.  
- **Accuracy**: 43/100  
  Only three results (analysis_7, analysis_8, analysis_10) were fully accurate. The remaining entries were either incomplete (e.g., empty fields) or mismatched (e.g., incorrect metrics).  
- **Completeness**: 43/100  
  Only three of seven results were correctly captured. Four GT results (analysis_5, analysis_6, analysis_9, analysis_11) were missing or misrepresented.  

Key deductions:  
- **Data**: Accuracy penalized heavily for missing omics/source details.  
- **Analyses**: Lost points for three invalid/incomplete analyses.  
- **Results**: Most GT results were omitted or inaccurately reported.