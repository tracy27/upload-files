Okay, I need to evaluate the predicted annotation against the ground truth for each component: Data, Analyses, and Results. Let's start with the Data component first.

**Data Component:**

First, check the structure. The predicted data is a list of objects with the required keys like id, omics, link, etc. All entries seem to have valid JSON structure. No syntax errors here. So Structure score should be 100.

Next, accuracy. Compare each entry in the predicted data with the ground truth. 

Looking at the ground truth data has 12 entries, while the predicted has 5. Let's see which ones are present:

- data_1 matches exactly. 
- data_3, data_4, data_5, data_11 also match their ground truth counterparts. 

Missing entries are data_2 (WGS), data_6,7,8,9 (expression profiles from different GEOs), data_10, data_12 (other DNA methylation). 

Accuracy penalty: For each missing entry, but since they are present except some, maybe the accuracy is lower because some important data types are missing. But the existing ones are accurate. However, completeness is a separate aspect.

Completeness: The predicted data misses several entries (data_2, data_6-9, data_10, data_12). There are 7 missing out of 12 total. That's about 58% missing. But maybe some are less critical? Not sure. Since completeness requires covering all relevant objects, this is a big hit. Maybe 50% on completeness?

Wait, the ground truth has data_2 (WGS), data_6-9 (expression profiles), data_10-12 (methylation). The predicted includes data_1,3,4,5,11. So missing data_2,6,7,8,9,10,12. That's 7 missing entries. So completeness would be (5/12)*100 ≈ 41.67%. But maybe some of those are considered less important? The problem states to penalize missing objects. So maybe completeness is around 40%.

But wait, maybe some entries in the ground truth are duplicates? Like data_6 to data_9 are all expression profiles with different public IDs. The predicted has none of these except data_3 (which is RNA-seq, different). So those are indeed missing. Similarly, data_10 and 12 are methylation profiles from TCGA and another GEO, so important.

So the completeness is low. The predicted data has only 5 out of 12, so 41.67%. But since the user wants completeness measured by coverage of relevant objects, maybe deduct points. Let's say completeness score is around 40.

Accuracy-wise, the existing entries are correct. So accuracy score could be 100, but perhaps some fields are empty in ground truth but filled in prediction? Looking at the ground truth data_6 to data_9: their source and format are empty. In prediction, for example, data_11 has source as empty, which matches. So no inaccuracies there. So accuracy is 100.

Total Data score: Structure 100, Accuracy 100, Completeness ~40. Total would be average of these three? Wait, the scoring criteria says each component has its own score based on the three aspects. Wait, actually, the user says each component (Data, etc.) gets a single score out of 100 considering structure, accuracy, and completeness. Hmm, need to clarify how to combine them.

Wait, the scoring criteria says: "Each component [...] you will assign a separate score (0-100) [...] based on the three evaluation aspects: 1. Structure, 2. Accuracy, 3. Completeness". So each of the three aspects contributes to the overall component score. How exactly? Are they weighted equally? The instructions don't specify, so perhaps each aspect contributes equally (each 0-100, then averaged). Alternatively, maybe each aspect is a factor in the total. Since the user didn't specify, perhaps treat each aspect as a separate component. Wait, no, the user says per component (like Data) the score is based on the three aspects. So probably each aspect contributes to the component's score. Maybe each aspect is scored out of 100 and then combined into a single score. Alternatively, the three aspects are considered together. Since the instructions aren't clear, but the output requires a single score per component, perhaps each aspect is evaluated and the final score is a combination where each aspect's importance is considered. Since the user says "based on three evaluation aspects", perhaps each aspect is equally weighted. So each aspect contributes 1/3 to the total.

For Data:

Structure: Perfect, so 100.

Accuracy: All existing entries are accurate (no mismatches in content except missing ones), so 100.

Completeness: 5 out of 12, so roughly 41.67%, but maybe rounded. Let's say 40.

Total: (100 + 100 + 40)/3 = 83.33 → approx 83.

Wait, but maybe completeness is more critical. Alternatively, perhaps completeness is part of accuracy? The user separated them. The instructions state accuracy is about correctness of existing entries, completeness is coverage. So maybe the 3 aspects are separate and each is scored 0-100, then the component's score is the average of the three. So yes, that's likely. So for Data: 100 (structure) + 100 (accuracy) + 40 (completeness) divided by 3 → 80.

Wait, let me recalculate: 100+100+40=240; 240/3=80. So Data score would be 80.

Wait, but maybe the completeness is more nuanced. The ground truth has 12 entries. The predicted has 5, but some are missing. But maybe some are not essential? For example, the data_6-9 are four expression profiles from different GEOs, but perhaps they are all similar. If the prediction missed some but included others, maybe the completeness isn't as bad? Or maybe each missing entry counts as a point deduction. Since the user says "count semantically equivalent objects as valid even if wording differs", but here, the missing ones are distinct entries. Each missing entry reduces completeness. So 7 missing, out of 12, so 5/12 present. 5/12 is ~41.67%. So completeness score would be (5/12)*100 ≈41.67, rounded to 40 or 42. Let's take 42 for better precision.

Thus, Data component:

Structure: 100

Accuracy: 100 (all existing entries are correct)

Completeness: 42 (5/12)

Average: (100 + 100 +42)/3 = 80.66 → ~81. So maybe 80 or 81. Let's say 80 for simplicity.

**Analyses Component:**

Check structure first. The predicted analyses have JSON structure. Each analysis object has the required keys. Let me look for any structural issues. 

Looking at the predicted analyses:

- All have id, analysis_name, analysis_data. Some have label.

The ground truth has analysis_13 appearing twice (once as PCA and again as distinct methylation profile). Wait, in the ground truth analyses, analysis_13 is listed twice. Let me check:

Ground truth analyses array includes two entries with id "analysis_13":

One is PCA (analysis_13):

{
"id": "analysis_13",
"analysis_name": "Principal component analysis (PCA)",
"analysis_data": ["analysis_2", "analysis_3"]
},

Another later one:

{
"id": "analysis_13",
"analysis_name": "distinct methylation profile",
"analysis_data": ["data_5"],
"label": {...}
}

Wait that's a problem in the ground truth itself? Because having duplicate IDs is invalid. Wait the user provided ground truth might have a mistake, but according to the user's input, the ground truth has that duplication. But the predicted analysis has only one analysis_13 (the PCA one). Hmm, but in the predicted analyses, the second analysis_13 (distinct methylation) is missing. But perhaps the duplication in GT is an error, but we'll proceed as per given data.

Assuming the ground truth is correct, but the duplication is an issue. However, in the predicted, they only have one analysis_13 (PCA). 

Now checking structure of predicted analyses:

All objects are properly formed. So structure score 100.

Accuracy: Check each analysis in predicted vs ground truth.

First, list all analyses in ground truth (GT):

analysis_1 (Genomics: data1,2)

analysis_2 (Transcriptomics: data3)

analysis_3 (Transcriptomics: data6,7,8,9)

analysis_4 (Diff: analysis2,3; labels)

analysis_5 (Func Enrich: analysis4,3)

analysis_6 (Diff: data5,10,11,12; labels)

analysis_7 (Func Enrich: analysis6)

analysis_8 (SNF: analysis2, data5)

analysis_9 (Func Enrich: analysis1,2; labels)

analysis_10 (Diff: analysis1,2; labels)

analysis_11 (single cell: data4)

analysis_12 (clustering: analysis11)

analysis_13 (PCA: analysis2,3)

analysis_13 again (distinct methylation: data5; labels)

Wait the last analysis_13 in GT has ID analysis_13 again. So in total, 14 analyses in GT, but with duplicate id.

In predicted analyses:

analysis_2, analysis_13 (PCA), analysis_4, analysis_5, analysis_7, analysis_8, analysis_12.

Wait looking at the predicted analyses array:

[
{...analysis_2},
{...analysis_13 (PCA)},
{...analysis_4},
{...analysis_5},
{...analysis_7 (Func Enrich analysis6?)},
{...analysis_8},
{...analysis_12}
]

Wait the predicted analysis_7 refers to analysis6, but analysis6 isn't present in the predicted analyses. Wait in the predicted, analysis_7 has analysis_data: ["analysis_6"], but analysis_6 is not in the predicted's analyses list. That's an inconsistency. Also, analysis_6 in GT is about differential analysis of DNA methylation profiles. 

Additionally, in GT analysis_5's analysis_data includes analysis_3, which in predicted's analysis_5 analysis_data is ["analysis_4", "analysis_3"] but analysis_3 is not present in predicted's analyses (since predicted doesn't have analysis_3). Wait analysis_3 in GT is Transcriptomics with data6-9, which are missing in data section, so maybe analysis_3 is also missing in predicted?

Wait the predicted analyses include analysis_2 (Transcriptomics on data3), but analysis_3 (Transcriptomics on data6-9) is missing. Thus, analysis_4 in predicted references analysis_3 which is not present in predicted's data (since data6-9 are missing). Wait but in analyses, the analysis_3 exists in GT but not in predicted. 

This causes a chain of missing dependencies. 

So for accuracy, each analysis must correctly reference existing data or other analyses. For instance, analysis_4 in predicted references analysis_3, which isn't present in predicted's analyses (since analysis_3 is missing from predicted's analyses array). Therefore, this is an inaccuracy because analysis_3 doesn't exist in the predicted's analyses. Thus, this is an error in accuracy.

Similarly, analysis_7 in predicted references analysis_6, which isn't present in predicted's analyses. 

Also, the analysis_13 (PCA) in predicted has analysis_data ["analysis_2", "analysis_3"], but analysis_3 is not present in predicted's analyses (since analysis_3 isn't listed there). Therefore, this is incorrect.

Moreover, the predicted analyses miss several analyses from GT, like analysis_6, analysis_9, analysis_10, analysis_11, and the second analysis_13 (distinct methylation).

Additionally, analysis_8 in predicted references data_5 (which exists in data), but in GT analysis_8 references analysis_2 and data_5, which is okay as long as analysis_2 is present. 

Analysis_12 in predicted references analysis_11, but analysis_11 (single cell Transcriptomics on data4) is present in GT but not in predicted. Wait in predicted's analyses array, analysis_11 is not listed. Wait the predicted's analyses array includes analysis_12 which references analysis_11, but analysis_11 is not in the predicted's analyses. Hence, analysis_12 is incorrectly referencing a non-existent analysis.

So, multiple inaccuracies due to missing analyses and incorrect references.

Accuracy deductions:

- analysis_2 is present and correct (matches GT's analysis_2).

- analysis_13 (PCA) is present but its analysis_data includes analysis_3 which is missing in predicted's analyses. So this is inaccurate.

- analysis_4: The analysis_data includes analysis_3 which is not present (in predicted's analyses array). Also, in GT analysis_4's analysis_data is ["analysis_2", "analysis_3"], which the predicted has, but since analysis_3 is missing, this is wrong. Additionally, the label in GT for analysis_4 has patient labels including "MPAL" (spelled as "MPAL" in GT, while predicted has "MPAL" which matches?), so labels are okay.

- analysis_5 references analysis_4 and analysis_3. But analysis_3 is missing, so this is inaccurate.

- analysis_7 references analysis_6, which is missing. So inaccurate.

- analysis_8 is okay as it references existing analysis_2 and data_5 (data exists in data section).

- analysis_12 references analysis_11 (missing), so inaccurate.

Additionally, missing analyses like analysis_6, analysis_9, analysis_10, analysis_11, and the second analysis_13 (distinct methylation) are all inaccuracies because they are present in GT but not in predicted, leading to incomplete coverage and thus affecting both accuracy and completeness.

Wait, for accuracy, it's about the existing entries being correct. The missing analyses are part of completeness. However, if an existing analysis in predicted references non-existent analyses/data, that's an accuracy issue.

So for accuracy score, considering that several analyses have incorrect references (like analysis_4, analysis_5, analysis_7, analysis_12, analysis_13) and some analyses are missing entirely (analysis_6, analysis_9, analysis_10, analysis_11, and the second analysis_13), but focusing on the existing entries' correctness:

The analyses present in predicted:

- analysis_2: correct.

- analysis_13 (PCA): data references analysis_3 which is not present → inaccurate.

- analysis_4: references analysis_3 (missing) → inaccurate.

- analysis_5: references analysis_3 (missing) → inaccurate.

- analysis_7: references analysis_6 (missing) → inaccurate.

- analysis_8: correct (references existing analysis_2 and data_5).

- analysis_12: references analysis_11 (missing) → inaccurate.

Out of 7 analyses in predicted, only analysis_2 and analysis_8 are fully accurate. The rest have incorrect references. So accuracy might be low.

Perhaps accuracy score: Let's see how many are accurate.

Total analyses in predicted:7

Accurate ones:

analysis_2: yes.

analysis_8: yes.

analysis_13 (PCA): no (due to analysis_3 missing).

analysis_4: no.

analysis_5: no.

analysis_7: no.

analysis_12: no.

So 2/7 accurate entries → ~28.5% accuracy. But maybe partial credits? For example, analysis_13's name is correct but references wrong, so partially accurate? Not sure. The user says accuracy is based on semantic equivalence. So if the analysis is supposed to do PCA on analysis_2 and analysis_3 (which exists in GT), but in predicted, analysis_3 isn't present, so the references are wrong. Hence, it's inaccurate.

Alternatively, if analysis_3 is missing in the data, then analysis_3 can't exist in analyses. Since the data_6-9 are missing in the data section, analysis_3 (which uses them) shouldn't be present. But the predicted analysis_3 isn't there anyway. Wait, the analysis_3 in GT is based on data_6-9, which are missing from the data in predicted. So the analysis_3 shouldn't exist in the predicted because the data isn't there. But the predicted analyses include analysis_4 which references analysis_3, which is impossible if data_6-9 are missing. Hence, the references are invalid, making those analyses incorrect.

This is getting complex. Maybe the accuracy is around 30% (2/7). But perhaps other factors. Alternatively, the analysis_13 in predicted has the right name and data links (except analysis_3 missing). But since analysis_3 isn't in predicted's analyses, this is wrong. So it's inaccurate. So overall accuracy for analyses: 2/7 *100≈28.57 → 30% accuracy. But maybe other aspects. Maybe the presence of some correct analyses gives some points. Maybe a score of 30 for accuracy.

Completeness: The predicted has 7 analyses out of 14 in GT (excluding the duplicate analysis_13). Wait, but GT has 14 analyses (including the duplicate). Assuming the duplicate is an error and should be counted once, then GT has 13 unique analyses. Let's count:

GT analyses (unique IDs):

analysis_1, 2,3,4,5,6,7,8,9,10,11,12,13 (first occurrence). The second analysis_13 is a duplicate, so total 13.

Predicted has 7 analyses: analysis_2,13,4,5,7,8,12 → 7 out of 13 → ~53.85%. But some of the missing analyses are critical. For example, analysis_1 (Genomics) is missing, which is part of the data_1 and data_2 (WES/WGS). Since data_2 is missing from the data, analysis_1 (using data_1 and data_2) might not be needed, but the data_1 is present. However, the analysis_1 is still part of GT and should be included in completeness. Since it's missing, that's a deduction. 

Completeness score would be (7/13)*100 ≈53.85 → ~54.

Structure score is 100.

Total Analyses score: (100 + 30 +54)/3 ≈61.3 → ~61.

But maybe the accuracy is lower because even the existing analyses have errors beyond just missing references. For example, analysis_13 in predicted is named PCA and references analysis_2 and analysis_3. Since analysis_3 is missing, but in GT the PCA analysis does reference analysis_2 and analysis_3 (which exists in GT), but in predicted analysis_3 isn't there. So the analysis_13 in predicted is incorrect in its references. So it's not accurate. So accuracy is very low.

Alternatively, maybe the accuracy is calculated by how many analyses are both present and correct. So 2 out of 7 (analysis_2 and 8) are correct. The rest have errors. So 2/7 → ~28.57, so 30.

Thus, Analyses component score: (100 + 30 +54)/3=61.3 → 61.

**Results Component:**

First, structure check. The results in predicted are valid JSON. Each object has analysis_id, metrics, value, features. Looks good. Structure score 100.

Accuracy: Check each result entry in predicted against GT.

Ground truth has 14 results entries.

Predicted has 7 entries.

Check each:

1. analysis_4 (p, NOTCH1): matches GT's analysis_4 first entry. Correct.

2. analysis_5 (FDR HSC): matches one of GT's analysis_5 entries (first FDR entry). Correct.

3. analysis_5 (NES HSC): matches. Correct.

4. analysis_5 (NES Myeloid): in GT there's an entry with analysis_5, metrics NES, features "Myeloid cell differentiation", value [1.39]. Yes, so this is correct.

5. analysis_5 (FDR Lymphocyte): matches GT's analysis_5 entry with FDR and "Lymphocyte differentiation".

6. analysis_5 (NES Lymphocyte): correct.

7. analysis_13 (p for RUNX3 P1 promotor): in GT, the second analysis_13 (the distinct methylation one) has a p-value entry for RUNX3 P1 promotor with values all "p<0.01". The predicted has this, so correct.

However, in GT there are more results entries related to analysis_13, like the first entry (RUNX3 P2 and BCL11B). These are missing in predicted. But the existing ones are accurate. Also, the analysis_13 in results refers to the second analysis_13 (distinct methylation profile) in GT, which is not present in the predicted analyses. Wait, but the predicted analyses have analysis_13 (PCA) but the results refer to analysis_13 (distinct methylation) which is missing in predicted's analyses. 

Wait this is a problem. The predicted analyses do not have the second analysis_13 (distinct methylation), yet the results include an entry for analysis_13 (which in GT is that second one). So the analysis_id in the result refers to an analysis not present in the predicted analyses. Therefore, this is an accuracy error because the analysis doesn't exist in the predicted's analyses.

Wait, but in the predicted analyses, there is an analysis_13 (PCA), but the results entry for analysis_13 refers to the second analysis_13 (distinct methylation) which isn't in predicted analyses. Hence, this is an error. So the analysis_id in results must correspond to an analysis in the analyses section. 

Therefore, the last result entry (analysis_13) is referencing an analysis not present in predicted's analyses. So that's an accuracy issue. 

Similarly, other results entries:

- The first six entries (analysis_4 and 5) are okay because analysis_4 and 5 are present in predicted's analyses.

- The seventh entry (analysis_13) is problematic because in predicted analyses, analysis_13 is the PCA one, which doesn't involve RUNX3 P1 promotor. The analysis_13 in GT (distinct methylation) isn't in the predicted analyses. Hence, this result entry is referencing a non-existent analysis, making it inaccurate.

Thus, out of 7 entries in predicted results:

- 6 are accurate (analysis_4 and 5 entries except the last one).

- 1 (analysis_13) is incorrect due to non-existent analysis.

So accuracy: 6/7 ≈85.71 → ~86.

Completeness: Ground truth has 14 entries. Predicted has 7, but missing several. For example, all results from analysis_1, analysis_6, analysis_7 (other entries), analysis_9, analysis_10, analysis_12, and the other analysis_13 results. 

Completeness is 7/14 = 50%. But some missing results are critical. The completeness score would be (7/14)*100 =50.

However, the analysis_13 result in predicted is incorrect (referencing wrong analysis), so it doesn't count towards completeness. So actual correct entries are 6, so 6/14≈42.86 → ~43. But the instructions say to count semantically equivalent objects as valid even if wording differs. The analysis_13 result is not valid because the analysis doesn't exist, so it's not counted. Thus, completeness is 6/14 ≈42.86 → ~43.

But maybe the user considers the incorrect analysis_13 as an extra, hence penalizing completeness. So total completeness is (correct entries)/total GT entries → 6/14≈42.86.

Thus, Accuracy: ~85.7 (rounded to 85 or 86)

Completeness: ~43

Structure: 100

Total Results score: (100 +85.7 +42.86)/3 ≈ 76.

Wait, let me recheck:

If the analysis_13 result is considered an extra (since it references an analysis not in predicted's analyses), then it's an incorrect entry and shouldn't be counted. So the correct entries are 6 (the first six entries). So accuracy is 6/7 ≈85.7 (since one is wrong). Completeness is 6/14 ≈42.86. So:

Accuracy: 86 (approx)

Completeness:43.

Total: (100 +86 +43)/3 = 229/3≈76.3 → ~76.

Alternatively, maybe the analysis_13 result's analysis_id is technically present (as analysis_13 exists in analyses, but it's the PCA one, not the methylation one), so the analysis_id exists but the result is about a different analysis. Hence, it's an accuracy error because the analysis in question isn't the one described in the result. 

So yes, that entry is inaccurate. 

Therefore, final Results score is approximately 76.

**Final Scores Summary:**

Data: 80

Analyses:61

Results:76

But need to ensure calculations are precise.

Wait let me recalculate each component precisely:

**Data Component:**

Structure: 100

Accuracy: 100 (existing entries are correct)

Completeness: 5 out of 12 → (5/12)*100=41.666… → 41.67

Total: (100 + 100 +41.67)/3 = 241.67 /3 ≈80.56 → 81.

Rounded to nearest integer: 81 or 80? Maybe 81.

**Analyses Component:**

Structure:100

Accuracy: 2/7 entries are fully correct (analysis_2 and analysis_8). The rest have errors. 

Wait, analysis_2: yes.

analysis_8: yes.

Others (analysis_13,4,5,7,12) have errors. So 2/7 → ~28.57 → 29.

Completeness: 7/13 → ~53.85 → 54.

Total: (100 +28.57 +53.85)/3 → 182.42 /3 ≈60.8 → 61.

**Results Component:**

Structure:100

Accuracy:6 correct out of 7 entries (excluding the last one which is wrong). 6/7≈85.71 →86.

Completeness:6 correct entries (since the last one is wrong) over 14 total → 6/14≈42.86 →43.

Total: (100 +85.71 +42.86)/3 ≈228.57/3≈76.19 →76.

Thus, final scores:

Data: 81,

Analyses:61,

Results:76.

But the user might expect rounding differently. Alternatively, maybe the Data completeness was 41.67, leading to 80.56 → 81. Let's confirm:

Data:

(100 +100 +41.67)=241.67/3≈80.56 → 81.

Analyses:

Accuracy: Let's think again. Maybe some analyses have partial accuracy. For example, analysis_13 in predicted has the correct analysis name (PCA) and references analysis_2 (correct) but analysis_3 is missing. Since analysis_3 is not present, the analysis_data is partially correct (includes analysis_2). Does that count as partially accurate? The user says accuracy is based on semantic equivalence. If the analysis is supposed to do PCA on analysis_2 and 3 (from GT), but in predicted it references analysis_2 and non-existing analysis_3, then it's incorrect. So no partial credit. So the accuracy remains 2/7.

Alternatively, if analysis_3 is not present (because data_6-9 are missing), then the analysis_13 in predicted is correct as far as possible. But according to the ground truth, the analysis_13 (PCA) does require analysis_3. Since analysis_3 is missing in predicted's analyses, the analysis_13 can't be fully correct. Hence, it's inaccurate.

Hence the initial calculation holds.

So final scores as above.
</think>

{
    "Final Scores": {
        "Data": 81,
        "Analyses": 61,
        "Results": 76
    }
}

**Explanation:**

### **Data Component (81/100)**
- **Structure:** 100/100  
  The predicted data is valid JSON with proper key-value structures for all entries.
  
- **Accuracy:** 100/100  
  All included entries (data_1, data_3, data_4, data_5, data_11) are accurate and semantically match the ground truth. No factual errors were found in existing entries.

- **Completeness:** 42/100  
  The predicted data includes only 5 out of 12 total entries from the ground truth. Key omissions include WGS (data_2), expression profiles (data_6–9), and DNA methylation data (data_10, data_12). This results in significant coverage gaps (~41% coverage).

---

### **Analyses Component (61/100)**
- **Structure:** 100/100  
  The predicted analyses are valid JSON with proper key-value structures.

- **Accuracy:** 29/100  
  Only two analyses (analysis_2 and analysis_8) are fully accurate. Other analyses (e.g., analysis_4, analysis_5, analysis_12, analysis_13) contain invalid references to missing analyses (e.g., analysis_3, analysis_6, analysis_11) or incomplete dependencies (e.g., analysis_13 references analysis_3, which is absent). The second analysis_13 (distinct methylation profile) is entirely missing from the predicted analyses, further reducing accuracy.

- **Completeness:** 54/100  
  The predicted analyses include 7 out of 13 unique analyses from the ground truth. Critical omissions include Genomics (analysis_1), SNF analysis dependencies (analysis_9, analysis_10), and the distinct methylation analysis (second analysis_13). This results in ~54% coverage.

---

### **Results Component (76/100)**
- **Structure:** 100/100  
  The predicted results are valid JSON with proper key-value structures.

- **Accuracy:** 86/100  
  Six of seven results entries are accurate (e.g., analysis_4 and analysis_5 metrics). The final entry for analysis_13 (RUNX3 P1 promoter) is inaccurate because it references an analysis (distinct methylation profile) not included in the predicted analyses.

- **Completeness:** 43/100  
  The predicted results include 6 accurate entries out of 14 total in the ground truth. Missing entries include metrics from analysis_1, analysis_6, analysis_7, analysis_9, analysis_10, analysis_12, and the remaining analysis_13 results. This results in ~43% coverage.