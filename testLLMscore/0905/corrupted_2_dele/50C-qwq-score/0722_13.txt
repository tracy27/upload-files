Okay, I need to score the annotation result based on the provided groundtruth. Let me start by understanding the task requirements thoroughly.

First, the user wants me to evaluate three main components: data, analyses, and results. Each component will have a score out of 100, broken down into Structure (10 points), Content Completeness (40 points), and Content Accuracy (50 points). 

Starting with the **Data** section. The groundtruth has three data entries (data_1, data_2, data_3), but the annotation result only includes data_3. Wait, data_3 is present in both, but the first two data entries (RNA-seq and proteomics) are missing in the annotation. That means under Content Completeness, we need to deduct points because two sub-objects are missing. 

Looking at Structure: Both use the correct JSON structure with id, omics, etc., so maybe full points here? Unless there's an issue with the keys. The groundtruth has "format" and "source", which are present in the annotation's data_3 even if empty. So Structure is okay, so 10/10.

Content Completeness: Groundtruth has 3 sub-objects, Annotation has 1. Since they missed 2, each missing would be (40 points /3) *2. Wait, how exactly to calculate? The instructions say deduct for each missing sub-object. Since there are three in groundtruth, each missing one could deduct 40/3 ≈13.3 per missing. But maybe it's better to consider proportionally. Alternatively, total points for completeness are 40; each missing sub-object reduces by (40/number of groundtruth sub-objects)*number missing. Here, 2 missing: (40/3)*2 ≈26.6 deduction, so 40 - 26.6≈13.3. Hmm, but maybe simpler: since each sub-object contributes equally, perhaps each is worth 40/3 ≈13.33 points. Missing two would lose about 26.66, leaving 13.33. That seems right. Also, check if any extra sub-objects in annotation. They don't add any beyond data_3, so no penalty for extras here.

Content Accuracy: For the existing data_3, compare with groundtruth. The sub-object matches exactly (same omics, empty fields). So accuracy here is perfect. Thus, 50 points. So total Data score would be 10 + ~13.3 +50 ≈73.3? Wait, but let me confirm:

Wait, actually, for Content Accuracy, only the existing sub-objects in both are considered. Since data_3 is present in both, and its key-values match (even if some fields are empty, like format and source, which were also empty in groundtruth). So accuracy is perfect for that sub-object. Since there's only one sub-object in annotation that's present, the 50 points are fully earned. 

Thus, Data total: 10 (structure) + 13.3 (completeness) +50 (accuracy) = 73.3. Rounded to nearest whole number? Maybe 73.

Moving on to **Analyses**: Groundtruth has 11 analyses (analysis_2 to analysis_11 plus annlysis_8 and 9). The annotation has 5 analyses: analysis_2, analysis_5, analysis_6, annlysis_9, analysis_10. Wait, checking the annotation's analyses:

Looking at the annotations analyses array:
- analysis_2: Gene set enrichment analysis, analysis_data: analysis_1
- analysis_5: proteomics, data_2
- analysis_6: GO analysis, data_1
- annlysis_9: differential expression analysis, data_2
- analysis_10: metabolome analysis, data_3

Wait, in groundtruth, analysis_2 is present, but analysis_5, analysis_6, analysis_10 are also present. However, the others like analysis_3,4,7,8,11 are missing. Additionally, the annotation has an entry named "annlysis_9" (with typo?), but the groundtruth has "analysis_9" and "analysis_8". Wait in groundtruth, the analyses include analysis_8 and annlysis_8 (typo?), but in the annotation, it's annlysis_9. Need to check exact names.

Wait, in groundtruth, the analyses include "analysis_8" and another entry with id "annlysis_8" (probably a typo). The annotation has "annlysis_9" which matches the groundtruth's "analysis_9" (since in groundtruth, analysis_9 is "differential expression analysis" with data_2). Wait, in the groundtruth's analyses array, looking at the entries:

Groundtruth analyses list:

analysis_2, analysis_3, analysis_4, analysis_5, analysis_6, analysis_7, analysis_8, annlysis_8, annlysis_9, analysis_10, analysis_11.

Wait, the groundtruth has an entry with id "annlysis_8" (probably a typo for analysis_8), and then "annlysis_9". The annotation includes "annlysis_9", which is present in groundtruth. However, the annotation doesn't have analysis_3,4,7,8, analysis_11.

So the annotation's analyses include 5 sub-objects, while groundtruth has 11. So missing 6 sub-objects. 

Structure: The structure of each analysis in the annotation looks correct (id, analysis_name, analysis_data). Even if analysis_data is an array or string, as long as the key exists. So structure is okay, 10 points.

Content Completeness: Groundtruth has 11 sub-objects. The annotation has 5, but some may not be equivalent. We need to check if the ones in the annotation correspond semantically to the groundtruth's. 

Let's see:

1. analysis_2: Present in both, so counts.
2. analysis_5: Present, okay.
3. analysis_6: Present, okay.
4. annlysis_9 (annotation's) vs groundtruth's analysis_9: Yes, same name and data. So that's correct.
5. analysis_10: Present in both, okay.

Other analyses in groundtruth (analysis_3,4,7,8, analysis_11, annlysis_8) are missing. So 6 missing sub-objects. 

Each missing would deduct (40/11)*6 ≈ 21.8 points. So 40 - 21.8 ≈18.2.

But wait, some of the missing might be optional? No, the optional fields are in the key-value pairs, not the sub-objects themselves. So each missing sub-object is a deduction. 

Additionally, the annotation has an extra sub-object? Let me check: The annotation's analyses are 5, all accounted for as above, none are extra. Because they're either matching or required? Wait, no, the groundtruth has more, so the annotation is missing many. The annotation does not have any extra beyond what's in the groundtruth except maybe annlysis_9, which is in groundtruth. So no penalty for extras.

Thus Content Completeness: ~18.2.

Content Accuracy: Now, for the existing sub-objects in both. Let's check each:

analysis_2: In groundtruth, analysis_data is "analysis_1", which matches the annotation's analysis_data. So correct. 

analysis_5: analysis_data is data_2, which matches. Correct.

analysis_6: analysis_data is data_1, correct.

analysis_10: analysis_data is data_3, correct.

annlysis_9: analysis_data is data_2, which matches groundtruth's analysis_9 (assuming annlysis_9 is a typo but semantically same as analysis_9). 

Now, check analysis names and other fields. All the analysis_names match exactly? Let's see:

analysis_2: "Gene set enrichment analysis" – same in both.

analysis_5: "proteomics" – same.

analysis_6: "Gene ontology (GO) analysis" – same.

analysis_10: "metabolome analysis" – same.

annlysis_9: "differential expression analysis" – same as groundtruth's analysis_9.

So all names are accurate. The analysis_data links are correct. The only possible issue might be the typo in annlysis_8 vs analysis_8 in groundtruth, but since the annotation's annlysis_9 is correctly corresponding to groundtruth's analysis_9, the ID difference is allowed as per instructions (IDs can differ as they're just identifiers).

Therefore, all key-value pairs in these sub-objects are accurate. So Content Accuracy is 50 points. 

Total Analyses score: 10 + 18.2 +50 ≈78.2 → ~78.

Now **Results** section. Groundtruth has 9 results entries (analysis_1 to analysis_9). The annotation's results have 6 entries: analysis_2, analysis_4, analysis_5, analysis_6, analysis_7, analysis_8. 

First, check which are missing in the annotation compared to groundtruth:

Missing: analysis_1, analysis_3, analysis_9 (from groundtruth's results array). Wait, let me list groundtruth results:

Groundtruth results:

analysis_1, 2, 3, 4,5,6,7,8,9 → 9 items.

Annotation has analysis_2,4,5,6,7,8 → missing analysis_1,3,9.

So missing 3 sub-objects. 

Structure: The structure for each result entry (analysis_id, metrics, value, features) is correct in the annotation. So 10 points.

Content Completeness: Groundtruth has 9, annotation has 6. Missing 3. Deduction: (40/9)*3 ≈13.3. So 40 -13.3≈26.7.

Check if any extra entries. The annotation doesn't have any beyond the existing ones except those missing. So no penalty for extras.

Content Accuracy: Now, for each present sub-object in both:

Analysis_2: In groundtruth, features are ["1005 and 3259 differentially expressed genes"], same as annotation. So correct.

Analysis_4: Features match exactly the groundtruth's ["UPR/ER", etc.].

Analysis_5: Same features as groundtruth.

Analysis_6: Features have "Lipid synthesis seemed " (with trailing space?), but groundtruth has "Lipid synthesis seemed " (maybe a typo in both?). Assuming it's a minor error but semantically same. So acceptable.

Analysis_7: Metrics "p<0.05", features match.

Analysis_8: Metrics "Gene Enrichment Score", features match.

Wait, but in groundtruth's analysis_8, the features are listed as ["MSX1", ...] same as annotation. 

However, check if any discrepancies. For analysis_4, the features are exactly the same. 

Wait, but in the groundtruth results for analysis_3, which is missing in the annotation, so not part of this evaluation.

Thus, all the present entries in the annotation have accurate key-values. So Content Accuracy is 50 points.

Total Results score: 10 +26.7 +50 ≈86.7 → ~87.

Wait, but let's double-check for any inaccuracies:

In analysis_6's features, the last item in groundtruth is "Lipid synthesis seemed " (ends with space?), but that's probably a typo. The annotation might have the same. If both have it, then it's okay. Since the instruction says prioritize semantic over literal, maybe that's acceptable.

Another thing: The analysis_id in the results must correspond to existing analyses. For instance, analysis_7 in results refers to analysis_7 in analyses. Since in the analyses section, the annotation included analysis_7? Wait, no, looking back: the annotation's analyses only have up to analysis_6, analysis_5, etc. Wait, in the annotation's analyses, the analyses listed are analysis_2,5,6, annlysis_9 (as analysis_9), and analysis_10. There's no analysis_7 in the analyses array of the annotation. Wait, but in the results, analysis_7 is referenced. Is that an issue?

Wait, the analyses in the annotation's analyses array do not include analysis_7. The analysis_7 in results refers to an analysis that isn't present in the annotation's analyses. That's a problem. Because the analysis_7 in results must have a corresponding analysis in the analyses section. Otherwise, it's an invalid reference. 

Ah! This is a critical point. The analysis_7 in the results is linked to an analysis that isn't present in the annotation's analyses. Since the analyses in the annotation do not include analysis_7 (which was present in groundtruth's analyses array), this is an error in the annotation's results. 

This affects the Content Accuracy for the Results section. Specifically, the analysis_7 entry in results references an analysis that's missing in the analyses section. Therefore, this feature entry (analysis_7) is incorrect, leading to a deduction. 

How much should be deducted? Let's see:

The features for analysis_7 are correct (the features list matches), but the existence of analysis_7 itself in the results without being present in analyses is a problem. Wait, actually, the analysis_id in results must refer to an analysis in the analyses array. If the analysis isn't there, then the entire result entry is invalid. 

Wait, the analysis_7 in results is part of the results array, but since the analysis_7 isn't in the analyses array of the annotation, this is an inconsistency. Therefore, the presence of this result entry (analysis_7) would be considered as an extra? Or is it a missing analysis in the analyses section causing this?

Actually, the problem here is that in the annotation's analyses array, they didn't include analysis_7 (from groundtruth), so when they have a result for analysis_7, it's referencing a non-existent analysis. This makes the result entry for analysis_7 invalid. 

Therefore, the result for analysis_7 should not be counted as valid. Hence, in the Results section's Content Completeness, we might have an extra entry (analysis_7), but since the analysis wasn't present, it's an extra? Or is it considered a missing analysis?

Alternatively, perhaps the analysis_7 in results is considered an extra because there's no corresponding analysis in the analyses array. So that would be an extra sub-object in results, which could lead to a penalty. 

Wait, the Content Completeness section says: "Extra sub-objects may also incur penalties depending on contextual relevance." So if the analysis_7 in results has no corresponding analysis in the analyses array, it's an invalid sub-object, hence an extra, leading to a deduction. 

Therefore, in the Results section:

Original count was 6 entries (including analysis_7), but analysis_7 is invalid because its analysis isn't present. So effectively, the valid entries are 5 (excluding analysis_7). Therefore, the missing entries are analysis_1,3,9 plus analysis_7 is invalid. Wait, no. Let me clarify:

The groundtruth requires that for results, each analysis_id must correspond to an analysis in the analyses array. The annotation's analyses do not have analysis_7, so the result entry for analysis_7 is invalid. Hence, it shouldn't be counted towards completeness. 

Therefore, the actual valid entries in the results are analysis_2,4,5,6,8 → 5 entries. The missing from groundtruth's 9 are analysis_1,3,7,9 → four missing? Wait, groundtruth has analysis_1 through analysis_9 (9 entries). The annotation's valid results (excluding analysis_7) are analysis_2,4,5,6,8 → five. So missing are analysis_1,3,7,9 → four missing. 

Wait, analysis_7 was in groundtruth's results (yes, looking back: groundtruth results include analysis_7, but in the annotation's results, they have analysis_7 but no corresponding analysis in analyses. So the annotation's inclusion of analysis_7 in results is invalid, so they shouldn't count it. Thus, the actual valid sub-objects in results are 5, but the groundtruth had 9. So missing 4 (analysis_1,3,7,9) and analysis_7 is an extra? Or no, it's an invalid entry, so not counted. 

This complicates things. Let's recalculate:

If analysis_7 is invalid, then the valid results in the annotation are 5 (excluding analysis_7). So missing 4 entries (analysis_1,3,9, and possibly analysis_7? No, analysis_7 was present in groundtruth's results but not in the annotation's analyses, making the result entry invalid, so it's not counted. So the missing are analysis_1, analysis_3, analysis_9, and analysis_7 (since the annotation didn't include a valid analysis_7 and thus can't have its result). 

Wait, but analysis_7's presence in results without the analysis is a mistake. So the Content Completeness is penalized for missing analysis_1,3,7,9 (four missing), and the invalid analysis_7 result is treated as an extra? Or just considered not present?

Hmm, this is tricky. According to the rules, for Content Completeness, we deduct for missing sub-objects. The analysis_7's result is present but invalid, so it's not a valid sub-object. Therefore, the number of valid sub-objects is 5 (analysis_2,4,5,6,8). Groundtruth has 9, so missing 4. 

Thus, Content Completeness deduction: (4/9)*40 ≈17.8, so 40 -17.8≈22.2. 

Plus, the invalid analysis_7 entry might count as an extra, but since it's invalid, it's not an extra but an incorrect one. The instruction says "extra sub-objects may incur penalties", so if the analysis_7 result is considered an extra (since it's not in groundtruth's results?), but actually, analysis_7's result is in groundtruth's results. Wait, groundtruth does have a results entry for analysis_7. So the annotation's attempt to include it is okay, but since the analysis isn't present in their analyses array, it's an inconsistency. 

This is a structural error, perhaps affecting the Structure score? Or part of Content Accuracy?

Wait, the Structure score is about the JSON structure, not the content validity. So the Structure remains okay. The problem is in the Content Accuracy for the analysis_7 entry. 

Since analysis_7's analysis isn't present, the result entry for it is invalid. Therefore, that result entry is incorrect, so in Content Accuracy, we need to deduct points for that entry. 

Calculating Content Accuracy: Each sub-object's key-value pairs are evaluated. For the valid entries (analysis_2,4,5,6,8):

All their key-value pairs are accurate except analysis_7's entry (which is invalid). Since analysis_7 is invalid, its features are considered inaccurate. However, the analysis_7 entry in results was included despite no analysis_7 in analyses. 

The Content Accuracy is 50 points for all matched sub-objects. The analysis_7 is a matched sub-object (in terms of existence in groundtruth), but its analysis_data link is invalid because the analysis isn't present. Wait, no—the analysis_id in results is just an identifier, but the analysis must exist in analyses. Since it doesn't, the analysis_7's result is invalid, so the features there may be wrong. 

Assuming the features for analysis_7 are correct (they match groundtruth), but the analysis itself isn't there, is that a problem? The features are part of the results. The error is in the dependencies between analyses and results, but according to the scoring criteria, Content Accuracy looks at the key-value pairs' semantic accuracy. 

The analysis_id is a key-value pair. If the analysis_id refers to a non-existent analysis, that's an accuracy issue. Thus, the analysis_7's analysis_id is incorrect, so that entry's key-value pairs are inaccurate. 

Therefore, the analysis_7 entry in results has an invalid analysis_id (since there's no analysis_7 in the analyses array), so its key-value pairs are incorrect. 

Thus, among the 6 result entries in the annotation:

- 5 are valid (analysis_2,4,5,6,8) with accurate data.

- analysis_7 is invalid due to missing analysis in analyses, so its features are considered inaccurate. 

The total number of relevant sub-objects for accuracy is the groundtruth's 9. But only the ones present in both are considered. The analysis_7 in results is present in groundtruth, but its accuracy is flawed. 

So for Content Accuracy:

Out of the 9 groundtruth results, the annotation has:

- analysis_2: accurate.

- analysis_4: accurate.

- analysis_5: accurate.

- analysis_6: accurate (despite trailing space?).

- analysis_7: inaccurate (due to missing analysis).

- analysis_8: accurate.

- analysis_1,3,9: missing.

Thus, for Content Accuracy, the total possible is 50 points divided by 9 sub-objects. Each correct one gives (50/9)*points. But maybe it's better to compute:

Accuracy is based on matched sub-objects (those present in both). The matched ones are analysis_2,4,5,6,7,8 (but analysis_7 is invalid). Wait, actually, the analysis_7 is in both (groundtruth has it, and the annotation tried to include it), but the annotation's version is invalid. 

Therefore, for the matched sub-objects (analysis_2,4,5,6,7,8):

Of these six:

- analysis_2: accurate (40/60 of total?)

Wait, perhaps better approach: 

Total possible accuracy points (50) are divided by the number of matched sub-objects (those present in both groundtruth and annotation, considering semantic equivalence). 

The matched sub-objects are:

analysis_2,4,5,6,7,8 (since they exist in both). 

For each of these:

- analysis_2: accurate (full points).

- analysis_4: accurate.

- analysis_5: accurate.

- analysis_6: accurate (trailing space might be minor, but instruction says prioritize semantic).

- analysis_7: inaccurate (because analysis missing, so the analysis_id is invalid).

- analysis_8: accurate.

So out of 6 matched sub-objects, 5 are accurate, 1 is inaccurate. 

Thus, the accuracy score is (5/6)*50 ≈41.67. 

Additionally, the missing sub-objects (analysis_1,3,9) contribute nothing to accuracy, since they weren't present. 

Wait, but the instruction says for Content Accuracy, only the matched sub-objects (from completeness phase) are considered. So the 6 matched sub-objects (analysis_2,4,5,6,7,8) are the basis. 

So 5 correct, 1 incorrect. 

Therefore, Accuracy deduction: 1 incorrect out of 6 → (1/6)*50 ≈8.33 deduction, so 50-8.33≈41.67. 

Adding that to the previous calculations:

Structure: 10

Completeness: 22.2 (for missing 4 out of 9)

Accuracy: ~41.67

Total Results score: 10+22.2+41.67≈73.87 ≈74.

Hmm, that's lower than before. This shows the impact of the analysis_7 error.

Alternatively, maybe the analysis_7's result entry is considered invalid and thus not counted in the matched sub-objects. Then the matched sub-objects would be 5 (analysis_2,4,5,6,8). Of these, all are accurate. 

Then Accuracy would be (5/5)*50=50, and Completeness would be:

Groundtruth has 9, annotation has 5 valid (excluding analysis_7), so missing 4 (analysis_1,3,7,9). 

Thus:

Completeness: (4/9)*40 deduction → 40 - (4*40/9)=40-17.78≈22.22

Accuracy:50

Total Results: 10+22.22+50≈82.22≈82.

But which approach is correct? The key is whether the analysis_7 entry in results is considered a valid sub-object for the purposes of Content Accuracy. Since the analysis_7's analysis is missing in the analyses array, the result entry's analysis_id is invalid. But the features might still match the groundtruth's. 

According to the instructions, for Content Accuracy, "sub-objects deemed semantically matched in the 'Content Completeness' section" are considered. If in the completeness phase, the analysis_7's result was counted as present (since it's in both groundtruth and annotation), then it must be evaluated. However, because the analysis_7 isn't present in the analyses array, it's an inconsistency but not necessarily a lack of semantic match. 

The problem here is more about data integrity than semantic content. Since the scoring criteria don't mention dependencies between analyses and results, perhaps the analysis_7's result is considered semantically correct (same analysis_id and features), but the missing analysis in the analyses array is a separate issue. 

Wait, but the analysis_7's existence in results without being in analyses is a structural error? Or part of Content Completeness? 

This is getting too tangled. Perhaps the best way is to proceed with the initial calculation but note the discrepancy. Given time constraints, I'll go with the second approach where analysis_7's result is considered invalid and thus excluded from the matched sub-objects. Therefore, Results score would be 82.

Alternatively, the user might expect that the analysis_7's presence in results is allowed even without the analyses entry, but that's unlikely. Since the analyses must be linked properly, it's an error. 

Given this uncertainty, I think the safer deduction is to penalize the accuracy for analysis_7's invalidity. So let's stick with 74 for Results. 

Now compiling all scores:

Data: ~73

Analyses: ~78

Results: ~74 

Wait, but earlier calculations varied. Let me recheck each carefully:

**DATA:**

Structure: 10/10

Completeness: 1 out of 3 sub-objects present. So (1/3)*40≈13.33

Accuracy: 50 (only data_3 is present and accurate)

Total: 10+13.33+50≈73.33 → 73

**ANALYSES:**

Structure:10

Completeness: 5 out of 11 → (5/11)*40≈18.18

Accuracy:50 (all present sub-objects are accurate except maybe analysis_8 typo?)

Wait, in analyses, the annotation has "annlysis_9" which is a typo for "analysis_9", but the analysis_name and analysis_data match. So it's considered semantically correct. Thus, accuracy is full 50.

Total: 10+18.18+50≈78.18 →78

**RESULTS:**

Structure:10

Completeness: 5 valid out of 9 → (5/9)*40≈22.22

Accuracy: All 5 are accurate (excluding analysis_7 which is excluded). Thus 50.

Total: 10+22.22+50≈82.22 →82

But considering the analysis_7's issue, if we treat it as invalid and thus excluded, the completeness becomes 5/9, accuracy 5/5 (since the 5 are correct). So 82.

Therefore final scores:

Data:73

Analyses:78

Results:82

Wait, but let me confirm once more:

For Results:

Groundtruth has 9 results entries. Annotation's valid results (excluding analysis_7 because analyses lack analysis_7) are 5 (analysis_2,4,5,6,8). 

Thus:

Completeness: (5/9)*40 ≈22.22 (so 40- (4/9)*40)

Accuracy: 5/5 →50.

Yes, so Results gets 82.22≈82.

Thus, rounding to whole numbers:

Data:73

Analyses:78

Results:82

Final Scores: { "Data":73, "Analyses":78, "Results":82 }

But let me check again for Analyses:

In Analyses, the groundtruth has 11 analyses. The annotation has 5, but all are correct except the misspellings (annlysis_9 is okay, analysis_10 is okay). So the 5 are all correct. Thus Content Completeness is 5/11 → ~18.18. So 10+18.18+50=78.18 →78.

Yes. 

Another possible oversight: In the Analyses, the groundtruth has an entry with id "annlysis_8" (likely a typo for analysis_8). The annotation doesn't have that, so it's counted as missing. But the annotation has annlysis_9, which corresponds to analysis_9. The analysis_8 in groundtruth is present as "analysis_8" and "annlysis_8"—but the latter is likely a duplicate or typo. However, since the annotation doesn't have either, it's considered missing. 

Wait, the groundtruth's analyses array includes both "analysis_8" and "annlysis_8" (with typo). That's probably a mistake in the groundtruth, but for scoring, we take it as given. So the groundtruth has two entries for analysis_8 (one with typo). The annotation has neither, so those count as missing. 

Wait, in the groundtruth's analyses array:

Looking at the groundtruth's analyses:

After analysis_7 comes:

{
    "id": "analysis_8",
    "analysis_name": "Transcriptional regulatory network analysis",
    "analysis_data": [
        "analysis_1"
    ]
},
{
    "id": "annlysis_8",
    "analysis_name": "PCA analysis",
    "analysis_data": [
        "data_2"
    ]
},
{
    "id": "annlysis_9",
    ...
}

So "analysis_8" and "annlysis_8" are two different analyses (different names). The first is "Transcriptional..." and the second is "PCA analysis".

Thus, the groundtruth has both as distinct entries. The annotation's analyses do not include either of them (since the analyses array in the annotation doesn't have "analysis_8" or "annlysis_8"). 

Hence, those two are additional missing analyses. So in the Analyses section:

Total missing sub-objects from groundtruth's 11:

Original count was 11. Annotation has 5, so 11-5=6 missing. Those 6 include analysis_3,4,7, analysis_8, annlysis_8, analysis_11.

Thus, yes, the 6 missing. So the calculation holds. 

Thus, the final scores are as computed.
</think>

"Final Scores": {
    "Data": 73,
    "Analyses": 78,
    "Results": 82
}