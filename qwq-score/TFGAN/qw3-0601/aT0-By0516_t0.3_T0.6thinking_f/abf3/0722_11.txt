Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component: Data, Analyses, and Results.

**First, the Data Section:**
Groundtruth has two data entries with omics types "Serum proteome data" and "Tissue transcriptome data". In the annotation result, the corresponding data entries have "Proteomics" and "Transcriptomics". These seem like acceptable synonyms, so they match semantically. 

Looking at structure: Both have arrays of objects with id, omics, link, format, source, public_id. The keys are present, so structure is good (10/10).

Completeness: The annotation includes both data entries, no missing sub-objects. But wait, the groundtruth has empty fields for link, format, public_id, while the annotation fills in source with a specific institution. Since those fields are allowed to be empty or filled, but the user didn't mention penalties for filling them unless incorrect. Since the sources are correctly filled, and other fields left as null (equivalent to empty strings?), maybe they’re okay. So completeness is full (40/40). 

Accuracy: The omics terms are slightly different but equivalent. The source info in the annotation is more detailed and correct. Link/format/public_id are optional? If groundtruth left them blank, but annotation kept them null, that's acceptable. So accuracy here is perfect (50/50). Total Data score 100.

**Next, Analyses Section:**
Groundtruth has four analyses: PCA, Spearman, differential expression, ROC. The annotation has many more analyses (12), including some that aren’t in the groundtruth like PSM, RCS, etc. Need to check if the required ones are present and others are extra.

Structure: All analysis entries in the annotation have the required keys (id, analysis_name, analysis_data). Even though there's an extra "label" key, the main structure is okay. So structure gets full points (10/10).

Completeness: Groundtruth requires analyses_1 to 4. In the annotation:
- analysis_9 is PCA (matches analysis_1 in groundtruth)
- analysis_11 is Spearman (matches analysis_2)
- analysis_2 is differential expression (matches analysis_3)
- analysis_4 is ROC (matches analysis_4)
So all four core analyses are present. However, the order differs but IDs don't matter. The extra analyses (like 5-12) are additional but not part of the groundtruth. Since the instruction says "sub-objects in annotation that are similar but not identical may qualify", but the extra ones are beyond the groundtruth's scope. Wait, the groundtruth's analyses are the required ones, so the annotation must include all four. Since they do, completeness is okay. But maybe the groundtruth's differential expression analysis uses both data_1 and data_2, whereas the annotation's analysis_2 only uses data_2. That might be an issue for accuracy, but for completeness, presence matters. So completeness is 40/40. 

Wait, but the groundtruth's analysis_3 has analysis_data as [data_2, data_1], which is same as [data_1, data_2]. The annotation's analysis_2 uses only data_2, so that's a problem. Wait, analysis_3 in groundtruth is "differential expression analysis" using data_2 and data_1. In the annotation, analysis_2 is "Differential expression analysis" using only data_2. So missing data_1 here. That would affect completeness because the sub-object (analysis_2 in annotation) isn't fully equivalent. Hmm, so maybe the differential expression analysis in the annotation doesn't match the groundtruth's requirement of using both datasets. So perhaps the annotation missed this aspect, leading to a deduction in completeness. Alternatively, since the name is correct but the data used is incomplete, it might count as incomplete. 

Wait, the groundtruth's analysis_3 (diff expr) uses both data_2 and data_1, but the annotation's analysis_2 (diff expr) only uses data_2. Therefore, the analysis_data is incomplete for that sub-object. So the sub-object (analysis_2) in the annotation does not fully correspond to the groundtruth's analysis_3, so maybe it's considered a missing sub-object? Or is it a partial match? This could lead to a deduction in completeness. Alternatively, maybe the analysis_data discrepancy reduces accuracy rather than completeness. 

This needs careful consideration. For completeness, we're checking whether the sub-object exists. Since the analysis exists but its data is incomplete, maybe it's still considered present but inaccurate. Thus, completeness remains full, but accuracy is penalized. 

Moving on: The other analyses in the groundtruth (PCA, Spearman, ROC) are present in the annotation. So overall, all required analyses are there. The extra analyses add to the count but don't penalize completeness. Unless the groundtruth expects exactly those four, then having more doesn't hurt completeness. Hence, completeness stays at 40.

Accuracy: Now looking at each matched analysis:

- PCA (groundtruth analysis_1 vs annotation analysis_9): Names match (PCA vs Principal Component Analysis), data references correct (both use data1 and data2). So accurate (full points).

- Spearman (groundtruth analysis_2 vs annotation analysis_11): Name matches, data in groundtruth uses both data1 and data2, but annotation's analysis_11 only uses data1. Wait, the groundtruth analysis_2 has analysis_data as [data1,data2], but the annotation's analysis_11 has analysis_data [data1]. So discrepancy here. That's an inaccuracy in the analysis_data field. So that's a deduction.

- Differential Expression (groundtruth analysis_3 vs annotation analysis_2): The name matches (differential expression), but the analysis_data in groundtruth includes both datasets, while the annotation's uses only data2. So another inaccuracy here.

- ROC (groundtruth analysis_4 vs annotation analysis_4): Names match (ROC analysis vs Receiver...), and analysis_data in groundtruth is data1, which matches the annotation's analysis_4. So accurate here.

Additionally, the groundtruth analysis_4's analysis_data is ["data_1"], which matches annotation's analysis_4's ["data_1"]. So that's correct.

Now, the Spearman analysis in the annotation (analysis_11) has data only from data1, whereas groundtruth used both. This would mean the Spearman analysis in the annotation is less accurate. Similarly, the diff expr analysis uses only data2 instead of both. So each of these could lose points for accuracy.

Let's compute accuracy deductions:

Each analysis contributes (50 points / 4 analyses = ~12.5 per analysis). 

For PCA: full (12.5)

Spearman: 50% accuracy because data is missing one dataset → 6.25

Diff Expr: 50% accuracy (missing data1) → 6.25

ROC: full (12.5)

Total accuracy points: 12.5 +6.25+6.25+12.5= 37.5. But wait, total accuracy is 50 points. Alternatively, each key-value pair in the sub-object is checked. 

Alternatively, each analysis's key-value pairs:

For each analysis, the analysis_name and analysis_data are the main elements. Let's see:

Analysis_1 (PCA):

- analysis_name: correct (exact or synonym?) "PCA" vs "Principal Component Analysis"—yes, same concept. So accurate.

- analysis_data: matches (data1 and data2). Full points.

Analysis_2 (Spearman):

- analysis_name: exact match (Spearman correlation analysis vs Spearman correlation analysis in groundtruth's analysis_2? Wait, the groundtruth analysis_2's name is "Spearman correlation analysis", and the annotation's analysis_11 is "Spearman correlation analysis"—so name matches perfectly. But the analysis_data in groundtruth is data1 and data2, while the annotation's analysis_11 uses only data1. So analysis_data discrepancy. So for this analysis, the analysis_data is wrong. So this analysis's accuracy is partially wrong. Since analysis_data is part of the sub-object's content, that's a key-value inaccuracy. 

Similarly, the differential expression analysis (analysis_2 in annotation vs analysis_3 in groundtruth):

The analysis_data in groundtruth is data2 and data1 (order doesn't matter), but in the annotation's analysis_2 it's just data2. So missing data1 here. Thus, the analysis_data is incomplete. 

Calculating accuracy deductions:

Each analysis has two key-value pairs: analysis_name and analysis_data. 

For Spearman (analysis_11):

- analysis_name: correct (+1)

- analysis_data: incorrect (only data1 vs both) → -1 → total 0.5 (if scaled). 

But how to compute this?

Alternatively, each analysis's accuracy is based on all key-value pairs. Suppose each analysis's possible points are (total 50)/4 = 12.5. For each analysis, if any key is wrong, subtract a portion.

For example, Spearman analysis: analysis_data is wrong. So half the points for that analysis (6.25 instead of 12.5).

Same for differential expression analysis: analysis_data missing data1 → half points (6.25).

Thus total accuracy: 

PCA: 12.5

Spearman: 6.25

Diff Expr: 6.25

ROC: 12.5

Total: 37.5. But since accuracy is 50 points, 37.5/50 → 75% → 37.5 points. Wait, but maybe each key is worth certain points. Alternatively, perhaps each analysis's accuracy is 50/4 = 12.5, and each key within the analysis is a part of that. Maybe better to think per key.

Alternatively, let's consider each sub-object (each analysis) contributes equally to the accuracy. There are 4 required analyses. Each should get 12.5 points. 

For each of the 4:

PCA: full (12.5)

Spearman: lost 50% → 6.25

Diff Expr: lost 50% → 6.25

ROC: full (12.5)

Total: 12.5+6.25+6.25+12.5 = 37.5. So 37.5 out of 50 for accuracy. So 75% → 37.5.

Additionally, the other analyses in the annotation (beyond the 4) may not affect accuracy since they're extra, but completeness already accounted for that. So total accuracy: 37.5. Wait but maybe I made a mistake here. 

Wait, the instruction says for accuracy, only the matched sub-objects (from completeness) are considered. Since the extra analyses are not part of the groundtruth, they don't contribute to accuracy. Only the four that are matched. 

Thus, the accuracy score would be 37.5, rounded perhaps? But the total must be integer. Maybe 37.5 ≈ 38. But perhaps better to keep decimals until the end.

So Analyses total score: Structure 10 + Completeness 40 + Accuracy 37.5 → 87.5. Rounded to 88? Or maybe fractions allowed. The user might expect integers. Hmm. Alternatively, maybe I miscalculated. Let me recheck.

Alternatively, perhaps the analysis_data discrepancy is a major error. Let's see: For the Spearman analysis in groundtruth, it uses both data sets. In the annotation's Spearman (analysis_11), it uses only data1. That's a significant inaccuracy because the analysis is supposed to involve both data sources. So maybe that's a full penalty for that analysis's accuracy. So instead of 50%, maybe 0 for that analysis's accuracy? 

Similarly, the differential expression analysis in groundtruth used both datasets, but the annotation's version uses only data2. So again, maybe 0 for that analysis's accuracy. 

If that's the case, then:

PCA: 12.5

Spearman: 0

Diff Expr: 0

ROC: 12.5

Total: 25 → 25/50 → 50% accuracy. Then total accuracy would be 25, making total analyses score 10+40+25=75. 

Hmm, this depends on how severe the error is. The analysis_data is crucial because it specifies which datasets the analysis used. If the analysis uses the wrong data, that's a major inaccuracy. So perhaps those two analyses (Spearman and Diff Expr) are completely inaccurate in their data usage, so they get zero for accuracy. 

In that case, accuracy would be 25 (PCA and ROC) → 25/50 = 50. 

Total analyses score would be 10+40+25=75. 

Alternatively, maybe the analysis_data discrepancy is a partial deduction. Let me think again. 

The groundtruth's Spearman analysis uses both data1 and data2. The annotation's Spearman analysis (analysis_11) uses only data1. So it's missing one dataset, which might mean the analysis is partially accurate. Depending on how critical the data inclusion is. Since the analysis_name is correct, but the data is incomplete, maybe half marks. 

Same with the differential expression analysis. 

Therefore, 37.5 is plausible. 

Another point: the groundtruth's analysis_3 (differential expression) lists analysis_data as [data_2, data_1] which is same as [data_1, data_2]. The order doesn't matter. The annotation's analysis_2 for diff expr uses [data_2], so missing data1. So that's an inaccuracy. 

The Spearman analysis in groundtruth uses both datasets, so the annotation's version missing one is a problem. 

So maybe each of those two analyses (Spearman and Diff Expr) loses half their accuracy points. 

Total accuracy: 12.5 (PCA) + 6.25 (Spearman) +6.25 (Diff) +12.5 (ROC)= 37.5 → 37.5/50 → 75%. 

Thus, the accuracy score is 37.5. 

So Analyses total: 10 +40 +37.5= 87.5. 

But maybe the structure has an issue? Let me check the structure again. 

The groundtruth analyses have keys: id, analysis_name, analysis_data. The annotation's analyses include an extra "label" key. The structure requires only the presence of the specified keys, but the presence of an extra key (label) doesn't affect the structure score as long as required keys are there. So structure is okay. 

Hence, Analyses score is 87.5. Rounding to 88, but the user may want exact decimals? The example shows integers. Maybe round to nearest whole number. 

Wait, the user says "score each object with max 100 points." Probably expects integers. So 87.5 rounds to 88. 

But maybe the label key is part of the structure? Wait, the groundtruth analyses don't have a "label" key. The instruction says structure is about the correct JSON structure and keys. So if the annotation adds an extra key (label), does that violate structure? 

Ah! This is important. The structure section says: "verify the correct JSON structure of each object and proper key-value pair structure in sub-objects. Do not base scoring on the actual content of the key-value pairs." 

The groundtruth's analyses sub-objects have keys: id, analysis_name, analysis_data. The annotation's analyses have those plus "label". Adding an extra key ("label") is a structural error. So the structure is incorrect. 

Oh, this is a mistake I overlooked earlier. 

Therefore, the structure score for Analyses would be penalized. 

Each analysis sub-object in the annotation has an extra key "label" which isn't present in the groundtruth. This violates the structure. 

How much to deduct? The structure is 10 points total for the entire Analyses object. 

If any of the sub-objects have incorrect keys, the structure is flawed. Since every analysis in the annotation (except maybe some?) has "label", even if null. The groundtruth's analyses don't have "label". 

Therefore, the structure is invalid because the keys are not the same. 

Hence, structure score is 0? Or partial?

Wait, the structure requires that the sub-objects have the correct keys as per the groundtruth. Since the groundtruth's analyses don't have "label", adding it is incorrect. 

However, the problem says "structure accounts for 10 points: verify correct JSON structure and proper key-value pair structure". 

If the keys are different, that's a structure error. 

Since all analyses in the annotation include an extra key ("label"), which wasn't in the groundtruth, this is a structural flaw. Hence, the structure is incorrect. 

Therefore, structure score for Analyses is 0? 

Wait, but maybe the presence of an extra key doesn't break the structure entirely. Maybe the required keys are present, but extra keys are allowed? 

The instructions say "proper key-value pair structure". The groundtruth's keys are id, analysis_name, analysis_data. The annotation added label, so it's an extra key. 

The problem statement says "structure accounts for 10 points: This section should focus solely on verifying the correct JSON structure of each object and proper key-value pair structure in sub-objects. Do not base scoring on the actual content of the key-value pairs."

So if the keys in the sub-objects differ, that's a structure issue. Since the annotation has an extra key, the structure is incorrect. 

Thus, the structure score for Analyses would be 0. But that seems harsh. Alternatively, perhaps the structure is considered correct as long as the required keys are present. The extra keys might be allowed as long as the required ones are there. 

The task says "proper key-value pair structure". The groundtruth's structure includes only those three keys. The annotation added an extra key, so the structure is improper. 

Hence, structure score for Analyses is 0. 

Wait, but the user might have intended that extra keys are allowed as long as the required ones are present. The problem says "proper key-value pair structure", which could mean that the required keys exist and are properly formatted. Extra keys might not matter. 

Alternatively, maybe the structure score is 10 only if all sub-objects have exactly the correct keys. 

This is ambiguous, but since the instruction emphasizes structure over content, and the presence of an extra key changes the structure, I'll assume the structure is incorrect. 

Thus, structure score: 0. 

Then, for Analyses:

Structure: 0 

Completeness:40 (since all required analyses are present)

Accuracy:37.5 (as before)

Total:0 +40 +37.5 = 77.5 → 78. 

But this is a big difference. 

Wait, perhaps the "label" key is allowed as part of the structure. The groundtruth might have optional keys? Looking back at the groundtruth analyses:

Groundtruth analyses have analysis_data as an array, but no "label". The annotation added "label" which is an extra field. If the schema allows for additional fields, then structure remains okay. 

The problem says "proper key-value pair structure"—maybe the required keys are present, and extra ones are acceptable. The structure score is about having the required keys and proper nesting. 

In that case, the structure is okay. Because the required keys (id, analysis_name, analysis_data) are present. The extra "label" is an addition but doesn't break structure. 

Hence, structure remains 10. 

Therefore, going back to the prior calculation: 10 +40 +37.5 = 87.5 → 88. 

Proceeding with that. 

**Now the Results Section:**

Groundtruth has three results entries linked to analyses_2, 3, 4. 

Annotation has many more results (15 entries), but need to check if the required ones are present and accurate.

Structure: Each result must have analysis_id, metrics, features, value. 

Looking at the groundtruth's results:

Result 1 (analysis_2): metrics "correlation", features "IGHM", value [0.56, p<0.001]

Result 2 (analysis_3): metrics "log2(foldchange)", features "IGHM", value [2.64, p<0.001]

Result 3 (analysis_4): metrics "auc", features list of three items, value list with three entries. 

Now, check the annotation's results:

Looking for entries with analysis_ids matching the groundtruth's analyses (analysis_2,3,4). 

Groundtruth analysis_2 is Spearman, so looking for analysis_11 in annotation (since that's the Spearman analysis). 

Groundtruth analysis_3 is differential expression → annotation's analysis_2.

Groundtruth analysis_4 is ROC → analysis_4 in annotation.

So:

For analysis_2 (Spearman in groundtruth) → annotation's analysis_11:

The groundtruth's result for analysis_2 has metrics "correlation", features IGHM, value [0.56, p<0.001].

In the annotation, analysis_11's results are:

There is a result entry for analysis_11 with metrics "Spearman correlation" and "p-value". The first entry has value "0.56", second has "<0.001". The features are ["IgM (IgM)"] which matches IGHM. 

So this is split into two entries (one for correlation value, one for p-value). The groundtruth combined them into a single value array. 

Does this count as accurate? The metrics are split into separate entries, but the data is present. The features are correct. The values are present. 

Semantically, it's equivalent, just structured differently. Since the problem allows for semantic equivalence over literal matching, this should be acceptable. 

Thus, the required result for analysis_2 is covered, albeit split into two entries. So completeness is okay. 

Next, analysis_3 (differential expression in groundtruth) → annotation's analysis_2:

Groundtruth's result has metrics "log2(foldchange)", features IGHM, value [2.64, p<0.001].

In the annotation, analysis_2's results include:

- analysis_id: analysis_2 has a result with metrics "Log2(foldchange)", value "2.64", features include IGHM among others. Also a p-value entry. 

The features in groundtruth is just IGHM, but the annotation lists more features (but includes IGHM). The metrics is correct. The value for log2(fc) is correct (2.64). The p-value is also present. 

However, the features in the annotation include extra items (GPCR-related genes, etc.), but since the groundtruth's feature is IGHM, which is included, this might be acceptable. The presence of additional features could be an extra, but since the required IGHM is present, it counts. 

The value in groundtruth is an array [2.64, p<0.001], while the annotation separates them into two entries (metric: log2 and p-value). So similar to the Spearman case. 

Thus, completeness is satisfied. 

Third, analysis_4 (ROC in groundtruth) → annotation's analysis_4:

Groundtruth has metrics "auc", features as three items, value with three entries. 

The annotation's analysis_4 has a result with metrics "AUC", value "0.81", features IgM. Also a confidence interval and threshold. 

The groundtruth's features are three items, but the annotation's is just "IgM". This is a discrepancy. The features in groundtruth are ["preEM", ...], but the annotation's is just IgM. This might mean the features are incomplete. 

Additionally, the value in groundtruth is three entries (each AUC with CI?), but the annotation provides a single AUC value. So the value is incomplete. 

Therefore, the result for analysis_4 in the annotation is missing the other features and parts of the value. 

So this result is incomplete/ inaccurate. 

Thus, the three required results:

1. analysis_2 (Spearman): Present but split into two entries. 

2. analysis_3 (diff expr): Present with extra features but key elements there. 

3. analysis_4 (ROC): Not fully accurate in features and value. 

**Completeness for Results:** 

Are all three groundtruth results present? 

Yes, but the third (analysis_4) may be incomplete in features. 

The problem states for completeness: deduct for missing sub-objects. Since the sub-object exists (there is a result for analysis_4), but the content is incomplete, it's counted as present. 

Thus, completeness is full (40/40). 

Structure: Each result entry must have the required keys. Checking a few examples:

Take the first result entry in annotation (analysis_1):

Has analysis_id, metrics, value, features. The groundtruth's results require analysis_id, metrics, features, value. The keys are correct. The value in groundtruth is an array; the annotation sometimes has single values or arrays. As long as the keys exist, structure is okay. 

E.g., the first result for analysis_1 has value as "0.56", which is a string, but the groundtruth had an array. The structure requires the presence of the key, not the type. So structure is okay. 

Hence, structure score 10/10.

Accuracy:

Now evaluating each of the three required results:

1. analysis_2 (Spearman):

Groundtruth's metrics: "correlation", features IGHM, value [0.56, p<0.001]

Annotation splits into two entries:

- metrics "Spearman correlation" (which is equivalent to "correlation" in groundtruth? Maybe "Spearman correlation" is the metric name, so the metrics key is accurate. 

The value is "0.56" and "p<0.001" in separate entries. 

The features are ["IGHM (IgM)"] which matches IGHM. 

Thus, the metrics and features are accurate. The value is split but covers both parts. Semantically correct. 

So this is fully accurate (no deduction).

2. analysis_3 (diff expr):

Groundtruth: metrics "log2(foldchange)", features IGHM, value [2.64, p<0.001]

Annotation has:

- Log2(foldchange) with value 2.64 and features including IGHM.

- A p-value entry for the same analysis. 

Thus, the metrics and values are present. The features include IGHM, so accurate. The extra features in the features list (other genes) don't affect the presence of IGHM. 

Thus, this is accurate. 

3. analysis_4 (ROC):

Groundtruth: metrics "auc", features [three items], value three entries. 

Annotation's entry for analysis_4 has:

Metrics "AUC", value "0.81" (single value instead of three), features just IgM. 

The features in groundtruth were ["preEM", ...], which the annotation doesn't include. The value is a single AUC instead of three. 

This is a significant inaccuracy. 

Thus, this result is inaccurate. 

So of the three required results:

Two are accurate (analysis_2 and 3), one is inaccurate (analysis_4). 

Total accuracy points: 

Each of the three required results contributes (50/3 ≈ 16.67 points each). 

For analysis_4: 0 points. 

Total: 16.67*2 = 33.33 

So accuracy score is 33.33. 

Adding up:

Structure:10 

Completeness:40 

Accuracy: ~33.33 

Total Results score: 83.33. Approximately 83. 

But maybe the metrics in analysis_4's groundtruth is "auc", which matches the annotation's "AUC". Features discrepancy: groundtruth has three items, but annotation has one. Value discrepancy: groundtruth has three AUCs, annotation has one. 

This is a major inaccuracy. So maybe the analysis_4 result gets 0 for accuracy. 

Total accuracy: (2/3)*50 = 33.33. 

Alternatively, if each key is assessed:

For analysis_4's result:

- metrics: correct (AUC vs auc, case-insensitive? Probably yes → full)

- features: missing two of the three required → maybe 1/3 accuracy here. 

- value: incorrect length → 0. 

Assuming each component (metrics, features, value) is equally weighted, but this complicates. Alternatively, the overall result's accuracy is 1/3 (since features and value are off). 

This is getting complex. Let's simplify: 

The required result for analysis_4 must have the correct metrics, features, and value as per groundtruth. Since all three aspects (features and value are wrong), this result is mostly inaccurate. 

Thus, it contributes 0 towards accuracy. 

Total accuracy: 2/3 *50 = 33.33 → 33. 

So Results total: 10+40+33=83. 

Rounded to 83. 

**Final Scores:**

- Data: 100 

- Analyses: 87.5 (~88)

- Results: 83 

But let me confirm:

For Analyses accuracy, earlier calculation was 37.5/50 → 75%. So 75% of 50 is 37.5. Total analyses score 87.5. 

So rounding to nearest whole numbers:

Data: 100 

Analyses: 88 

Results: 83 

Final scores would be:

{
    "Data": 100,
    "Analyses": 88,
    "Results": 83
}

Wait, but let me double-check the results accuracy again. 

The three required results:

1. analysis_2: accurate → full (16.67)

2. analysis_3: accurate → full (16.67)

3. analysis_4: inaccurate → 0 

Total accuracy: 33.33 → 33.33 

So Results accuracy is 33.33, total 83.33 → 83. 

Yes. 

Alternatively, maybe the analysis_4's result in the annotation has some correct elements. For instance, the metrics is correct (AUC). The features include IgM, which might be one of the groundtruth's features (if "preEM" etc. are different). The groundtruth's features for analysis_4 were ["preEM", "Continous igM", "Ordinam IgM"]. The annotation's features are ["IgM"], which might align with "Continous igM" or "Ordinam IgM"? Not sure. If the features are not semantically aligned, then it's wrong. 

Assuming the features are mismatched, then it's 0. 

Thus, the scores as above. 

Another check: the Results completeness. Are there any missing sub-objects? The three required are present, so no deduction. 

Hence, the final scores are as computed.
</think>

{
    "Final Scores": {
        "Data": 100,
        "Analyses": 88,
        "Results": 83
    }
}