Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component: data, analyses, and results. Each has its own structure, completeness, and accuracy to evaluate.

Starting with the **Data** section:

First, check the **structure**. Both groundtruth and annotation have arrays of objects with keys like id, omics, link, format, source, public_id. The structure seems consistent, so full 10 points here.

Next, **content completeness (40 points)**. Groundtruth has 5 data entries. The annotation has 15, which is way more. But some might be extra. Wait, looking closer, the groundtruth data includes PDC000358, PDC000360, etc., while the annotation has different PDC IDs like PDC000357, etc. Also, there are additional entries like data_7 to data_15 which aren't present in the groundtruth. So the annotation has many extra sub-objects beyond what's in groundtruth. Since extra sub-objects can incur penalties unless relevant, this is an issue. The groundtruth's data_4 and data_5 are WGS and RNA-seq from dbGaP with specific public IDs. In the annotation, data_7 and data_8 are Genomics and Transcriptomics from dbGaP with phs003152.v1.p1, which aligns with groundtruth's data_4 and 5. However, other entries like Proteomics from ProTrack Data Portal (data_9-14) and GEO data_15 are new. Since they're not in the groundtruth, these count as extras. The groundtruth doesn't mention these, so they shouldn't be included. The user mentioned that extra sub-objects may penalize if contextually irrelevant. Since these are additional datasets not in groundtruth, this reduces completeness. Also, some data entries in the groundtruth (like data_1 to data_6 in annotation) have different public IDs. For example, groundtruth data_1 has PDC000358, but annotation data_1 is PDC000357. That might indicate missing the correct ones. 

Wait, maybe the user said that sub-objects that are semantically equivalent even if not exact match can count. Let me see. The first three groundtruth entries are Proteomics with PDC000358, 360, 362. In the annotation, the first few data entries have PDC000357, 358, etc. So data_2 in annotation (PDC000358) matches groundtruth data_1's PDC000358? Wait, no: groundtruth data_1 is PDC000358, but in the annotation, data_1's PDC is 000357, so that's off by one. So perhaps some are shifted? Or maybe the IDs are different because of ordering, but the content is similar?

Wait the problem states: "data_id or analysis_id are only unique identifiers for sub-objects. When the same sub-objects are ordered differently, their IDs may vary. Therefore, scoring should focus on the sub-objects content, rather than using IDs to assess consistency". So IDs don't matter, just content. So for each groundtruth data entry, check if there's a corresponding entry in the annotation with same omics type and public_id. 

Looking at groundtruth data entries:

Groundtruth data_1: omics proteomics, PDC000358

In annotation data_2 has PDC000358. So that's a match. But groundtruth data_1's public_id is 358, which is in annotation's data_2. So perhaps the order is different but exists. Similarly, groundtruth data_2 is PDC000360, which is annotation data_4. Data_3 in groundtruth is PDC000362, which is annotation data_6. Data_4 in groundtruth is dbGaP phs003152.v1.p1 (WGS), which is annotation data_7 (Genomics). But Genomics vs WGS? Maybe not exact match. The omics types differ. Groundtruth says WGS, but annotation says Genomics. Is that considered a mismatch? The user said to prioritize semantic equivalence. Genomics might include WGS, but maybe it's too broad. Hmm. Similarly, RNA-seq (groundtruth data_5) is in annotation as data_8 (Transcriptomics). Transcriptomics could include RNA-seq, so maybe acceptable. 

So checking all groundtruth data entries:

Groundtruth data_1: PDC000358 (proteomics) → annotation data_2 (same ID and omics, so match)

Groundtruth data_2: PDC000360 → annotation data_4 (match)

Groundtruth data_3: PDC000362 → annotation data_6 (match)

Groundtruth data_4: dbGaP phs003152 (WGS) → annotation data_7 (Genomics). Here, omics type differs (WGS vs Genomics). Unless Genomics is considered a superset, but maybe this counts as incomplete.

Groundtruth data_5: RNA-seq from dbGaP → annotation data_8 (Transcriptomics). Transcriptomics often includes RNA-seq, so that's okay.

Thus, all groundtruth data entries except possibly data_4 (if WGS vs Genomics is a mismatch) are present. If data_4 is considered missing because Genomics isn't the same as WGS, then that's a problem. Alternatively, if Genomics is acceptable, then all are covered. 

Assuming Genomics is close enough (maybe WGS is part of Genomics), then all 5 groundtruth data entries are present in the annotation. However, the annotation has many extra entries (data_7-15), which may add penalty. 

The completeness score is about missing sub-objects. Since all are present (assuming semantic match), but there are extra entries, the penalty would be for the extra? The instructions say: "Extra sub-objects may also incur penalties depending on contextual relevance." Since the extra data entries (like data_9-15) are additional datasets not in groundtruth, they are irrelevant and thus penalized. 

The groundtruth had 5 data entries. The annotation has 15, so 10 extra. Each extra could deduct points. But how much? The completeness is out of 40. For each missing, you lose points, but since none are missing (if we consider all groundtruth are there), but the extras might lead to deduction. 

Alternatively, maybe the presence of extra entries doesn't directly affect completeness score unless they replace necessary ones. Wait, the instruction says: "Deduct points for missing any sub-object. Note: ... Extra sub-objects may also incur penalties depending on contextual relevance."

So the completeness is mainly about missing required ones, but extras can also take away points. Since all required are present (assuming data_4 is okay), but there are extras which are not in groundtruth, so those are irrelevant, hence penalized. 

How many points? The total completeness is 40. Suppose each missing would deduct 8 (since 5 items, 40/5=8 per item). But since none are missing, but there are extras. Maybe each extra deducts 0.5 points? Not sure. Alternatively, maybe the penalty for extra is proportional. Since there are 10 extra, maybe deduct 10*(40/(total possible?)). Maybe the extra adds to the total count, making it harder to score. Alternatively, the instructions might mean that having extra is a separate penalty, like 5 points off for adding unnecessary. 

This part is a bit ambiguous. Maybe better to think that since all required are present (assuming data_4 is okay), completeness is full except for the extra entries. But the problem states "extra sub-objects may also incur penalties". So maybe the completeness score is reduced due to the presence of extra entries. Since the annotation has more entries, but the groundtruth is the standard, adding extra could be seen as incorrect. 

Alternatively, maybe the presence of extra entries is allowed but penalizes the completeness. Since the user didn't specify exact penalty, perhaps deduct 20% of completeness (so 8 points) for adding 10 extra entries. 

Alternatively, since the groundtruth has 5, and the annotation has 15, that's tripling the data entries. But only 5 are needed. The excess might mean that the annotator added a lot of incorrect data, so the completeness is low. 

Hmm, this is tricky. Let's assume that the completeness is mostly about missing required items, but the extra adds a small penalty. Since all required are present (except maybe data_4's omics type?), then completeness is 40 minus some for the extras. Let's say 10 points penalty (so 30/40). 

Then **content accuracy (50 points)**. For each existing sub-object, check key-values. 

Take groundtruth data_1: 

omics: proteomics (annotation data_2 has Proteomics, which is same, case difference doesn't matter. So accurate.

public_id matches (PDC000358 in both).

source: Proteomic Data Commons, same as annotation. Link in groundtruth is empty; annotation has a link. But does the link matter? The instructions say to focus on semantic equivalence. Since the link is part of the content, if groundtruth's link is empty but annotation provides one, that's an improvement? Or is it a discrepancy? The problem says "content accuracy" looks at key-value pairs. If the groundtruth has empty link, but the annotation filled it, is that correct? Or is it wrong because the groundtruth didn't have it?

Hmm. The user says "content accuracy evaluates the accuracy of matched sub-object’s key-value pairs". Since the groundtruth's link is empty, but the annotation provides a link, that's an addition not present in groundtruth. Does that count as inaccurate? The instructions say to prioritize semantic equivalence. Since the link wasn't specified in groundtruth, providing one might not be incorrect, but perhaps the annotation shouldn't add info not in groundtruth? 

Alternatively, since the groundtruth left it blank, the annotation's inclusion of a link is extra and might be considered inaccurate. Or maybe the link is actually correct, so it's okay. Since the user says "you must account for potential differences in wording while semantic equivalence", perhaps the link is acceptable if it's the correct one. However, without knowing if the provided link is correct, but since it's in the groundtruth as empty, maybe it's better to deduct points for adding a link that wasn't there. 

Alternatively, maybe the link is part of the data and the annotation correctly filled it in, so it's accurate. Since the public_id is correct, perhaps the link is just additional info. But the problem says to base on groundtruth as reference. Since groundtruth's link is empty, the annotation's link is incorrect. So that's an inaccuracy. 

This complicates things. Let's proceed step by step for each data entry:

For each of the 5 groundtruth data entries:

Check each field:

1. **omics**: Check if matches. For groundtruth data_4 (WGS vs annotation data_7's Genomics). If considered different, that's a mistake. Similarly for others. 

2. **public_id**: Must match exactly? Like PDC numbers. 

3. **source**: Must match. 

4. **link**: Groundtruth has empty strings; annotation has links. So discrepancy here.

For accuracy, each key-value pair's correctness contributes. 

Let me go through each:

Groundtruth data_1 (PDC000358):

Annotation data_2 has:

omics: Proteomics (matches, case difference ok)

public_id: PDC000358 (exact match)

source: same.

link: provided (vs groundtruth's empty). So this field is incorrect. 

Thus, for this sub-object, 3/4 fields are correct (except link). Assuming each key contributes equally, maybe 25% inaccuracy here (since 4 keys, one wrong). 

Similarly, data_2 (PDC000360):

Annotation data_4 has same omics, public_id, source. Link is present but groundtruth has none. Same issue. 

Same for data_3 (PDC000362): annotation data_6's link is present but not in groundtruth. 

Groundtruth data_4 (WGS, dbGaP phs...):

Annotation data_7 has omics: Genomics vs WGS → discrepancy here. 

public_id: same (phs003152.v1.p1). 

source: same (dbGaP). 

link: provided (vs groundtruth's empty). 

So omics is wrong here (unless Genomics is acceptable as a broader term). If considered wrong, that's another error. 

Groundtruth data_5 (RNA-seq, dbGaP phs...):

Annotation data_8 has omics: Transcriptomics. Since RNA-seq is part of transcriptomics, maybe acceptable. 

public_id matches. 

source same. 

link provided. 

So omics is okay. 

Thus, for accuracy:

Each sub-object's accuracy is calculated. 

For each of the 5, check how many keys are correct:

1. data_1 (annotation data_2):

- omics: correct (case-insensitive)
- public_id: correct
- source: correct
- link: incorrect (groundtruth empty, annotation has link)

So 3/4 correct → 75% accuracy here.

2. data_2 (annotation data_4):

Same as above. 3/4 → 75%

3. data_3 (annotation data_6):

Same → 3/4

4. data_4 (annotation data_7):

- omics: WGS vs Genomics → incorrect (assuming not semantically equivalent)
- public_id: correct
- source: correct
- link: incorrect (present when groundtruth is empty)

→ 2/4 → 50%

5. data_5 (annotation data_8):

- omics: Transcriptomics vs RNA-seq → acceptable? Maybe yes → correct
- public_id: correct
- source: correct
- link: present but groundtruth empty → incorrect (so 3/4 → 75%

Total per sub-object:

Sub-objs 1-3: 75% each (3/4)

Sub-obj4: 50%

Sub-obj5: 75%

Total for all 5:

Average: (3*75 + 50 +75)/5 = (225 + 50 +75)/5 = 350 /5 = 70%. 

Since accuracy is out of 50 points, 70% of 50 is 35. But maybe each sub-object contributes equally. Let me think differently: each sub-object's accuracy contributes to the total. 

Each of the 5 sub-objects has 4 keys. Total keys across all 5: 20. 

Number of correct keys:

For data_1: 3

data_2: 3

data_3:3

data_4:2 (omics wrong, link wrong; public and source correct)

data_5:3 (omics is ok?)

Wait data_5's omics in groundtruth is RNA-seq, annotation says Transcriptomics. 

Is RNA-seq a type of transcriptomics? Yes, so that's acceptable. So omics is correct. 

Thus, data_5 has 3 correct (omics, public, source), link wrong → 3/4.

So total correct keys: 3+3+3+2+3=14

Total keys: 20 → 14/20 = 70% → 35/50.

But maybe each key is worth 1 point (since 50 total). 20 keys, so each is worth 2.5 points (50/20=2.5). 

Correct keys:14 → 14 *2.5 = 35. So accuracy score is 35/50. 

Also, the other data entries in the annotation beyond the groundtruth's are not counted here because they're extra. 

Additionally, the omics in data_4 (Genomics vs WGS) is a significant error, so maybe that's a bigger penalty. If the omics field is critical, perhaps that's more than just one wrong key. But according to the instructions, each key-value is treated equally. 

So total accuracy: 35.

So for Data:

Structure: 10/10

Completeness: Let's reassess. 

If all required data entries are present (with possible semantic matches), then completeness is full. The extra entries are penalized. The question is how much. 

The groundtruth requires 5 entries. The annotation has 15. The extra 10 may deduct points. The completeness is out of 40. The penalty for extra could be 10 points (e.g., 10% penalty for each extra beyond the required? But 10 extra would be too much). Alternatively, since completeness is about missing required, but the presence of extra is a separate issue. 

The instructions say "Deduct points for missing any sub-object. Note: Sub-objects in annotation result that are similar but not total identical to groundtruth may still qualify as matches..." So missing sub-objects are penalized. The extra are penalized based on context. 

Maybe the completeness is full (40) because all required are present (even with some inaccuracies?), but the extra cause a deduction. The problem states "Extra sub-objects may also incur penalties depending on contextual relevance." So if they are irrelevant, they reduce the score. 

Suppose each extra sub-object beyond the groundtruth's count deducts 2 points (40 divided by 5 is 8 per item, so maybe 0.5 per extra). 10 extra would deduct 5, so 35/40. Or maybe a flat 20% deduction (8 points). 

Alternatively, the maximum penalty for extra is up to half the completeness score. 

This is unclear, but to proceed, let's say the presence of 10 extra entries (which are not in the groundtruth) means that the annotator added unnecessary data, which is a significant error. Hence, deduct 10 points from completeness (40-10=30). 

Thus, Data total: 10 +30 +35 = 75/100. 

Wait, structure (10), completeness (30), accuracy (35). Total 75. 

Now moving to **Analyses** section:

First, structure. Groundtruth's analyses have id, analysis_name, analysis_data (array or single string). The annotation's analyses have id, analysis_name, analysis_data (array), and a label. The groundtruth doesn't have a 'label' key. The presence of an extra key (label) breaks the structure? The structure requires correct JSON structure and key-value pairs. Since the groundtruth doesn't have 'label', but the annotation does, that's an invalid key. So structure score is reduced. 

Original structure in groundtruth: each analysis has id, analysis_name, analysis_data. The annotation adds 'label', which is not in groundtruth. Thus, the structure is incorrect. 

Structure score: 10 points minus penalty for invalid key. Maybe deduct 2 points (since adding a key is structural error). So 8/10. 

Content completeness (40 points). Groundtruth has 13 analyses (analysis_1 to analysis_13). Annotation has only 1 analysis (analysis_1). That's a huge miss. All others are missing. So completeness is severely affected. 

The annotation is missing 12 sub-objects. Each missing would deduct (40/13 ≈ ~3.07 per missing). 12 missing → 12*3≈37 deducted. So 40-37=3. But since it's out of 40, maybe 3/40. 

However, the annotation's analysis_1 might correspond to one of the groundtruth's. Let's check:

Groundtruth analysis_1: differential mutation analysis, data_4.

Annotation analysis_1's name is "Proteogenomic Analysis for Treatment Response Prediction". Doesn't match. 

The analysis_data in groundtruth's analysis_1 uses data_4 (which in annotation is data_7). The analysis_data in annotation's analysis_1 includes multiple data entries (data_1-8, etc.), which might not correspond to any groundtruth analysis. 

Therefore, the annotation's analysis_1 doesn't semantically match any of the groundtruth's analyses. Hence, it's an extra, but there's nothing else. 

Thus, all groundtruth analyses are missing, so completeness score is 0/40. But wait, maybe some analyses could have been matched? Let me double-check:

Looking at all groundtruth analyses:

analysis_1: diff mutation analysis using data_4 (WGS). 

analysis_2: diff expr analysis using data_2 (proteomics). 

analysis_3: BRCA mutation predict response using analysis_9 (but analysis_9 is later in list, which is a protein panel). 

analysis_4: multivariate regression using analysis_1. 

analysis_5: genomic scars using data_4. 

analysis_6: predict platinum response using data_4. 

analysis_7-10: various protein panels using data_2/3/1. 

analysis_11: pathway analysis with multiple data sources. 

analysis_12: consensus clustering using analysis_11. 

analysis_13: immune infiltration using data_2,5, analysis_12. 

None of these are represented in the annotation's single analysis. 

Thus, all 13 analyses are missing → completeness is 0. 

However, the annotation has an extra analysis (analysis_1) which is not present in groundtruth. Since completeness is about missing required items, the extra doesn't help. So 0/40. 

Content accuracy (50 points). Since none of the groundtruth analyses are present, there are no matched sub-objects to assess accuracy. Thus, accuracy is 0/50. 

But wait, the instructions say "for sub-objects deemed semantically matched in the 'Content Completeness' section". Since none are matched, accuracy can't be scored here. So perhaps accuracy is 0. 

Thus, Analyses total: structure 8 + completeness 0 + accuracy 0 → 8/100. 

Now **Results** section:

Structure: Groundtruth results have analysis_id, metrics, value, features (array). The annotation's results have analysis_id, metrics (as an object?), value as a number, and features array. 

Wait, groundtruth results:

Example entry:

{
    "analysis_id": "analysis_3",
    "metrics": "p value",
    "value": "p=0.001",
    "features": [...]
}

Annotation's results:

[{"analysis_id": "analysis_1", "metrics": {"AUC": 0.83}, "value": 0.35, "features": ["TAP1", "BCL2L1"]}]

Here, the metrics in groundtruth is a string ("p value"), whereas in annotation it's an object {"AUC":0.83}. The structure is different. The groundtruth expects metrics as a string, but the annotation uses an object. Also, the 'value' in groundtruth is a string like "p=0.001", but annotation has a number (0.35). 

This structural deviation would affect the structure score. The keys are correct (analysis_id, metrics, value, features), but the types and structures within them are wrong. 

Structure: The key names are correct, but the values' formats are incorrect. The structure requires the correct JSON structure, so nested objects where they shouldn't be. Thus, structure score would be less. Maybe deduct 5 points, leaving 5/10. 

Content completeness (40 points). Groundtruth has 11 results entries. The annotation has 1. Missing all but one. 

The single result in annotation references analysis_1 (which is the only analysis present). But in groundtruth, analysis_1 has a result (first entry). Let's see:

Groundtruth's first result (analysis_1):

{
    "analysis_id": "analysis_1",
    "metrics": "",
    "value": "",
    "features": ["TP53", "BRCA1", "BRCA2"]
}

The annotation's result for analysis_1 has metrics as an object and a numeric value. 

But does the annotation's result correspond to groundtruth's analysis_1's result? The features in groundtruth's analysis_1's result are genes, while the annotation's features are ["TAP1", "BCL2L1"]. Different features. 

So, the sub-object exists (analysis_1 has a result), but the content is incorrect. 

However, in terms of completeness, the groundtruth requires 11 results. The annotation only has 1, so 10 missing. Each missing would deduct (40/11 ≈ ~3.6 each). So 10*3.6 ≈36, so 40-36=4. 

But the one present might be a match? 

The groundtruth's analysis_1's result has empty metrics and value. The annotation's result has metrics and value filled. Since the groundtruth allows empty fields, but the annotation added values, is that acceptable? 

The problem states: "content completeness accounts for missing sub-objects". Since the sub-object (result for analysis_1) is present, but the other 10 are missing, so completeness is 1/11 → roughly 4/40. 

Content accuracy (50 points). For the matched sub-object (analysis_1's result):

Compare each field:

analysis_id: matches (both analysis_1).

metrics: Groundtruth has "", annotation has {"AUC":0.83}. So discrepancy. 

value: Groundtruth "" vs 0.35. 

features: Groundtruth has ["TP53",...] vs annotation's ["TAP1", ...]. Different features. 

Thus, all fields except analysis_id are incorrect. 

Metrics and value are critical. So accuracy for this sub-object is very low. 

Each key's accuracy contributes. Let's say 4 keys:

analysis_id: correct (1/4)

others: 0 → total 25% accuracy for this sub-object. 

But since there's only one sub-object, the total accuracy is 25% of 50 → 12.5. But since the other 10 results are missing, the accuracy is based only on this one. 

Alternatively, maybe each result's contribution to accuracy is weighted by their presence. Since only one is present, and it's 25% accurate, total accuracy is 12.5. 

Thus, Results total: structure 5 + completeness ~4 + accuracy ~12.5 → ~21.5, rounded to 20/100. 

But this is rough. Let me recalculate:

Completeness: 1 sub-object present out of 11 → (1/11)*40 ≈3.64 → ~4.

Accuracy: Only the one sub-object's accuracy. 

For that sub-object's four keys:

analysis_id: correct → 1/4 → 25%.

Thus, accuracy score is 25% of 50 → 12.5.

Total: 5+4+12.5=21.5 → 22.

Final scores:

Data: 75

Analyses: 8

Results: 22

But let me recheck Data's completeness. Earlier thought was 30/40 because of extra entries. But if all required are present (5 out of 5), then completeness is full (40). The penalty for extra entries might not apply because completeness is about missing, not extra. Wait, the instruction says: "Deduct points for missing any sub-object. Note: ... Extra sub-objects may also incur penalties depending on contextual relevance."

So the completeness is first about missing. Since none are missing, completeness is 40. Then, the extra entries may deduct points. How?

The "may also incur penalties" implies that extra can be penalized, but the primary deduction is for missing. 

Suppose the extra entries count as negative, but the maximum is 40. If the extra are not allowed, maybe they reduce the score. 

The problem states that the annotator should not add extra sub-objects unless they are contextually relevant. Since the groundtruth lists certain data entries, adding others (like ProTrack Data Portal entries) are likely irrelevant, hence penalized. 

The penalty for extra could be proportional. The annotation has 15 entries instead of 5. The extra 10. Since completeness is 40, maybe each extra deducts 0.5 points: 10 *0.5=5, so 35. 

Alternatively, the extra is 3x the needed, so deduct 20 points (half of 40), leading to 20. 

Alternatively, since the instructions don't specify exact penalties for extras, perhaps it's better to assume that the presence of all required gives full completeness (40), and extras are ignored unless they caused missing. 

Wait, the user said: "sub-objects in annotation similar but not identical may still qualify as matches". The annotator might have added extra but missed none. So maybe completeness is 40. The extra are just extra, not penalized. 

In that case, Data's completeness is 40. 

Then accuracy for Data was 35/50. 

Structure 10. 

Thus Data total: 10+40+35=85. 

Revisiting analyses: 

Structure was penalized because of the 'label' key. The groundtruth's analyses do not have 'label', so adding it is a structural error. The structure score would be reduced. 

The analysis objects must have the same keys as groundtruth. The presence of an extra key 'label' makes the structure incorrect. 

If the structure requires that all keys must be present and no extra keys allowed, then it's a severe error. Maybe deduct 5 points for adding an extra key. So structure: 10-5=5. 

But the structure also needs correct key-value pairs. The 'analysis_data' in the annotation's analysis_1 is an array of data IDs. Groundtruth sometimes has single strings or arrays. Since the structure allows arrays, that's okay. The 'label' is the main issue. 

So structure score: 5/10. 

Then analyses total: 5 (structure) + 0 (completeness) +0 (accuracy)=5. 

Results: 

Structure: The metrics and value formats are wrong. The groundtruth expects strings, but the annotation used objects and numbers. So structural errors here. 

For example, 'metrics' in groundtruth is a string, but in annotation it's an object. That's a structure violation. 

Thus, structure score might be 5/10 (some keys exist but their types are wrong). 

Completeness: as before, 4. 

Accuracy:12.5 → 12.5. 

Total results: 5+4+12.5=21.5 → 22. 

Final scores:

Data: 85 (structure 10, completeness40, accuracy35)

Analyses:5

Results:22

Final Scores: {

    "Data": 85,

    "Analyses": 5,

    "Results":22

}

But let me confirm once more:

For Data:

- Structure: correct (10)

- Completeness: all required present (40)

- Accuracy: 14/20 keys correct (70% of 50 →35)

Yes, so 85.

Analyses:

Structure: 5 (due to 'label')

Completeness:0 (missing all except none are present)

Accuracy:0 (no matched sub-objects)

Total 5.

Results:

Structure:5 (due to metrics and value types),

Completeness: ~4,

Accuracy: ~12.5,

Total ~22.

Final Scores accordingly.
</think>

"Final Scores": {
    "Data": 85,
    "Analyses": 5,
    "Results": 22
}