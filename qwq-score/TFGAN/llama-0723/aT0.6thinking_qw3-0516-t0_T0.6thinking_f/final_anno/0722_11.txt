Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component: Data, Analyses, and Results. Each has Structure, Content Completeness, and Content Accuracy scores.

First, let's tackle the **Data** component. 

**Groundtruth Data**:
- Two sub-objects: data_1 (Serum proteome data) and data_2 (Tissue transcriptome data). Both have empty link, format, source, and public_id.
- Keys present: id, omics, link, format, source, public_id.

**Annotation Result Data**:
- Two sub-objects: data_1 (Proteomics) and data_2 (Transcriptomics).
- For data_1, source is filled ("Proteomics Platform..."), others are null except omics.
- For data_2, all fields except omics are null.
- All required keys (id, omics, etc.) are present.

**Structure Check (10 points)**:
- Both have correct keys. Even though some values are null, the structure is intact. So full 10 points here.

**Content Completeness (40 points)**:
- Groundtruth has two data entries; Annotation also has two. Correct number. But check if the sub-objects match semantically.
- Data_1: "Serum proteome" vs "Proteomics" – close enough, so considered a match.
- Data_2: "Tissue transcriptome" vs "Transcriptomics" – also a match. So no deductions here. Full 40?

Wait, but maybe the source? Groundtruth has empty source but Annotation filled it for data_1. However, completeness is about presence of sub-objects, not their content. Since both have the required sub-objects, maybe no penalty. So 40.

But wait, the problem says "missing any sub-object" gets deduction. Since they have all required ones, so yes. So 40.

**Content Accuracy (50 points)**:
- Now checking key-value pairs for semantic accuracy.
- Data_1:
  - Omics: "Serum proteome data" vs "Proteomics". The latter is broader, but semantically correct since proteome data is part of proteomics. So acceptable.
  - Link, format, public_id are all empty in GT, and null in Annotation. That's okay.
  - Source in GT is empty, but Annotation filled it. Since the groundtruth doesn't require it, but the user's instruction says to check for discrepancies. Wait, the accuracy is about whether the Annotation's values align with GT's. Since GT's source is empty, but Annotation provided a source, is that an error? Or since it's allowed to have more info? Hmm, the instructions say "content accuracy" evaluates discrepancies in key-value pairs of matched sub-objects. Since GT didn't specify a source, but the Annotation added one, perhaps this is extra info but not a deduction because it's accurate (if the source is indeed correct). Alternatively, if the GT's source is empty, then providing it might be wrong. Wait, the problem states "content accuracy" is about the correctness of the Annotation's content relative to the groundtruth. If the GT's source is empty, then adding a source would be incorrect. But maybe the groundtruth might have omitted it but the Annotation correctly filled it? Not sure. Since the groundtruth's source is empty, but the Annotation's entry has a valid source, maybe this is an extra detail not present in GT. But since the key is present, maybe it's allowed. Wait, the problem says to focus on semantic equivalence. If the Annotation's source is correct, even if GT didn't mention it, maybe it's okay? Or since the GT didn't include it, it shouldn't be there? Hmm, tricky. Since the question says "deductions based on discrepancies in key-value pair semantics". The GT didn't have a source, so adding one may not align with GT's data. So that's a discrepancy. So minus some points here. Let's see:

- Data_1:
  - Omics: okay (5/5)
  - Link: both empty (okay)
  - Format: both empty (okay)
  - Source: GT empty vs Annotation has value → discrepancy. Maybe -2?
  - Public_id: same as above (both empty)
  
Total for Data_1: Maybe minor deductions here, like -2.

Data_2:
- Omics: "Tissue transcriptome" vs "Transcriptomics" → okay (5/5)
- Other fields all empty vs null → okay (since null is same as empty). So no issues here.

Total accuracy points: 50 minus 2 = 48?

Wait, maybe the source is allowed to be filled. Since the groundtruth may have omitted it but the Annotation added a correct source. If the source is actually correct, then maybe it's okay. But without knowing real data, we have to assume based on given inputs. Since the GT didn't specify, perhaps the Annotation shouldn't add it unless it's correct. But since the problem doesn't state that the Annotation can't have more info, maybe it's acceptable. Hmm. Alternatively, since the GT didn't have the source, the Annotation adding it is an extra, which isn't penalized under completeness, but for accuracy, since it's different, maybe it's a mistake. I think the safest is to consider that the Annotation's source is incorrect because GT had none, so deducting for that.

So Data_1: source discrepancy, maybe 1 point off (total 49?), but let me think again. Total possible 50 for accuracy. Let's say the source is an issue, so minus 2 (so 48). Then Data's total accuracy is 48.

Total Data Score: 10 + 40 + 48 = 98? Wait, no, the three sections are separate: structure (10), completeness (40), accuracy (50). So total max 100. So Data's total would be 10+40+48=98? Wait, no: each component is scored separately. Wait the task says "each object with max 100, split into structure (10), completeness (40), accuracy (50)". So Data's total is sum of those three parts. So yes, 10 + 40 + 48 = 98. Okay.

Now moving to **Analyses**.

**Groundtruth Analyses**:
- 4 analyses: PCA, Spearman, differential expr, ROC.
- Each has analysis_data links to data_1 and/or data_2. For example, analysis_4 uses data_1 alone.
- Keys: id, analysis_name, analysis_data. Some have order like data_2 first in differential.

**Annotation Result Analyses**:
- 8 analyses listed. Names include Proteotranscriptomic, Case-control, Diff expr, Multivariate, PSM, RCS, ROC, Nomogram.
- Analysis_1: analysis_data includes both data_1 and data_2. 
- Analysis_2 uses both data_1 and data_2.
- Analysis_3 uses data_2 only (differs from GT's differential analysis which used both)
- Analysis_4 uses both data_1 and data_2 (multivariate logistic regression)
- Analysis_5 (PSM) uses data_1
- Analysis_6 (RCS) uses data_1
- Analysis_7 (ROC) uses data_1 (matches analysis_4 in GT)
- Analysis_8 references other analyses (analysis_5,6,7) instead of data directly.

First, **Structure (10 points)**:
Check if each sub-object has correct keys. The GT has id, analysis_name, analysis_data. The Annotation adds a "label" field which isn't in GT. However, the structure criteria only require correct keys as per the object's definition. Wait, looking back at the task details: "structure" is about JSON structure and key-value pairs. The GT's analyses don't have "label", so if the Annotation includes it, does that affect structure? The problem says "correct JSON structure of each object and proper key-value pair structure". So if the Annotation has extra keys (like label), that's a structural error. Because the groundtruth doesn't have them, so the structure should mirror the groundtruth's keys. Thus, adding "label" is an extra key, making structure incorrect. Therefore, structure score deduction.

Each analysis in GT has id, analysis_name, analysis_data. The Annotation adds "label", which is not in GT's structure. Hence, the structure is flawed. So structure points: maybe deduct 2 points? Because all sub-objects have this extra key. So structure score would be 8 (out of 10)? Or since the structure requires the correct keys as per groundtruth, any deviation (extra keys) is a problem. So full structure points can't be given. How many points exactly? Maybe 5? Hmm, the instructions aren't precise, but since the structure must match the groundtruth's structure, any extra key would break it. So maybe 0? But that's harsh. Alternatively, maybe the presence of all required keys (id, analysis_name, analysis_data) is sufficient, but extra keys are allowed? The problem says "proper key-value pair structure", so perhaps extra keys are not allowed. So structure is incorrect, so maybe 5/10? Or 0? Hmm, tricky. Let me think. The structure part says to focus on correct JSON structure and key-value pairs. The Annotation added an extra key (label) which isn't in groundtruth. So the structure is not matching. Hence, structure score is 0? Or maybe half. Maybe the presence of the necessary keys is primary. Let's assume that the required keys (id, analysis_name, analysis_data) are there, so structure is okay except for the extra key. Maybe deduct 2 points (so 8/10). Let's proceed with 8/10 for structure.

Next, **Content Completeness (40 points)**:
Groundtruth has 4 analyses. Annotation has 8. Need to check which ones correspond.

Compare each groundtruth analysis to Annotation's sub-objects:

GT Analysis_1: PCA analysis using data_1 & data_2. In Annotation, Analysis_1 is "Proteotranscriptomic analysis" which uses both data. Semantically, "PCA" is a type of Proteotranscriptomic analysis? Maybe. The names differ, but the purpose might overlap. Need to see if they're considered equivalent. Alternatively, maybe not. "PCA analysis" is a specific technique, whereas "Proteotranscriptomic analysis" is a broader category. So perhaps not a direct match. Hmm, this could be a discrepancy. 

GT Analysis_2: Spearman correlation analysis (using both data). In Annotation, Analysis_1 has Spearman correlation as a metric in its results, but the analysis name is Proteotranscriptomic analysis. The actual analysis name in GT is "Spearman correlation analysis", which isn't present in Annotation's analysis names except possibly in results (but the analysis_name in the analyses list is different). So GT's analysis_2 (Spearman) is missing in the Annotation's analyses. 

GT Analysis_3: Differential expression analysis using data_2 and data_1 (order doesn't matter). In Annotation, Analysis_3 is "Differential expression analysis" but only uses data_2. The GT used both, but the Annotation only uses one. So is this a match? The name matches, but the data used differs. So maybe partial match, but since the data sources differ, maybe it's not considered equivalent. 

GT Analysis_4: ROC analysis using data_1. In Annotation, Analysis_7 is "Receiver Operating Characteristic (ROC) analysis" using data_1. That's a direct match. 

So, let's count how many GT analyses are properly covered in the Annotation's analyses:

- GT Analysis_1 (PCA): Not matched directly. Annotation's Proteotranscriptomic might involve PCA but not explicitly named. So not counted.
- GT Analysis_2 (Spearman): Not present as an analysis name in Annotation's analyses list. The Spearman is mentioned in a result's metrics, but the analysis itself isn't listed as an analysis_name in the analyses array. So missing.
- GT Analysis_3 (Diff expr): Partially matched (name same but data mismatched), so maybe not counted as correct.
- GT Analysis_4 (ROC): Matched (Analysis_7).

Thus, only 1 out of 4 GT analyses are fully matched. The rest are either missing or not semantically equivalent. 

Therefore, for Content Completeness, each missing sub-object (analysis) would deduct points. Since GT has 4, and Annotation only has 1 correct (assuming Analysis_7 counts), the missing are 3. So 40 - (3*(40/4))? Wait, the problem says "deduct points for missing any sub-object". The total points for completeness is 40, divided equally among the 4 GT sub-objects? Or per missing sub-object, 10 points each (since 40/4=10). So missing 3 would be -30, leaving 10? But then Annotation has extra analyses (beyond GT's count), which may also be penalized. 

Wait the problem says: "Extra sub-objects may also incur penalties depending on contextual relevance." So having extra analyses beyond what's in GT might lose points. 

GT has 4, Annotation has 8. The extra 4 may be penalized. But first, the missing ones are 3 (since only 1 matched). So for missing: 3 missing → 3*10=30 points off from 40, so 10 left. Then, the extra sub-objects (the other 4 analyses beyond the GT's count) might also cost points. How much? The problem states: "extra sub-objects may also incur penalties depending on contextual relevance". The instruction isn't clear on exact deductions, but maybe each extra is a small penalty, say 2 points each. 4 extras *2=8, so total 10-8=2? But that might be too harsh. Alternatively, the problem might consider that extra sub-objects only lose points if they are not relevant. But since the Annotation's analyses include valid analyses related to the article, maybe they are contextually relevant, so not penalized. But the problem says "may also incur penalties", so it's up to interpretation. 

Alternatively, maybe the completeness score is based purely on missing GT sub-objects. The extra ones don't add, but also don't subtract unless irrelevant. Since the Annotation's analyses include new ones that the GT didn't have (like multivariate regression, PSM, etc.), these could be considered valid analyses not present in GT. Since the GT is the reference, any extra that aren't in GT's list might be considered unnecessary but not necessarily wrong. The problem says "extra sub-objects may also incur penalties...", so possibly, each extra deducts a point. 

Assuming the extra 4 analyses deduct 4 points (1 per extra), total completeness would be 40 - 30 (for missing) -4 (extras)= 6? That seems low, but maybe. Alternatively, maybe the penalty for extra is less. Let me think again.

The problem says "Note: Sub-objects in annotation result that are similar but not totally identical to groundtruth may still qualify as matches. Thoroughly analyze each groundtruth sub-object's semantic correspondence before determining equivalency."

So for content completeness, each GT sub-object must be present in Annotation. If a GT sub-object has no corresponding Annotation sub-object (even if Annotation has extras), it's a penalty. 

In this case, only Analysis_4 (ROC) in GT is matched (Analysis_7 in Annotation). The others are missing. So 3 missing → 3*(40/4)=30 points off, leaving 10. The extra analyses are allowed but may not help. So completeness score is 10. But maybe I'm being too strict. Let's see if any other GT analyses are partially matched.

Looking again:

GT Analysis_3: Differential expression analysis using data_2 and data_1. The Annotation's Analysis_3 is Differential Expression analysis but uses only data_2. The analysis name is same, but data is different. The data is critical here. So if the analysis_data is part of the sub-object's key-values, then the mismatch in data sources would mean it's not a match. Hence, not counted. 

GT Analysis_2: Spearman is not present as an analysis name in the Annotation's analyses array. It appears in a result's metrics, but that's different. So missing.

GT Analysis_1: PCA is missing as an analysis name. 

Thus, indeed only 1 out of 4. So completeness is 10/40. But maybe the Proteotranscriptomic analysis (Analysis_1) in Annotation is considered a superset that includes PCA, so maybe it counts as a match? The analysis name is different, but the data used is correct. If the purpose of PCA is part of Proteotranscriptomic analysis, maybe it's a broader term. But the task requires semantic equivalence. Since "PCA analysis" is a specific method, and the Annotation's name is broader, they might not be considered equivalent. So still 1.

Hence, Content Completeness is 10.

**Content Accuracy (50 points)**:

Now, for the matched analyses (only Analysis_4/GT and Analysis_7/Annotation):

GT Analysis_4: analysis_name "ROC analysis", analysis_data [data_1].

Annotation Analysis_7: analysis_name "Receiver Operating Characteristic (ROC) analysis", analysis_data [data_1]. The name is longer but semantically equivalent. The data is correct. So this sub-object is accurate. 

Other sub-objects in Annotation that don't correspond to GT's analyses won't contribute to accuracy since they weren't counted in completeness. 

But wait, the accuracy is for the sub-objects that were deemed semantically matched in completeness. So only the matched ones (Analysis_7) are considered here. 

For that one:

- analysis_name: "ROC" vs "Receiver Operating Characteristic (ROC)" → equivalent. Good.
- analysis_data: both use data_1. Perfect match.

So full marks for that one's accuracy. Since it's the only one contributing, the accuracy score is 50? But since there's only one sub-object contributing, and it's perfect, yes. However, the problem says "for sub-objects deemed semantically matched in the 'Content Completeness' section". Since only one was matched, the 50 points are based on that one's accuracy. Since it's perfect, 50/50.

However, maybe the other analyses in Annotation (like Analysis_3, which is a diff expr analysis but with wrong data) are not considered because they weren't counted in completeness. So yes, accuracy is 50.

Total Analyses Score: Structure (8) + Completeness (10) + Accuracy (50) = 68. But wait, structure was 8, completeness 10, accuracy 50 → total 68. Hmm, but maybe I miscalculated structure.

Wait, earlier thought on structure: the analyses in the Annotation have an extra "label" key not present in groundtruth. So the structure is invalid. How much to deduct? If all analyses have this extra key, then structure is incorrect. Suppose the structure requires that all keys are exactly as in groundtruth. So the presence of an extra key in every analysis makes the structure wrong. Hence, structure score is 0? Or maybe 5? Let me reevaluate structure.

Structure is about correct JSON structure and proper key-value pairs. The groundtruth analyses have keys id, analysis_name, analysis_data. The Annotation adds "label" to each analysis. That's an extra key. The problem says "structure should focus solely on the correct JSON structure of each object and proper key-value pair structure in sub-objects". The extra key means the structure isn't correct. So structure score is 0. Because the key set doesn't match. Hence, structure score is 0.

Then, recalculating:

Structure: 0

Completeness: 10

Accuracy:50

Total: 60.

Hmm, that's worse. Which is right? The problem says "structure" is about the JSON structure and key-value pairs. The structure is wrong because of extra keys. So probably 0/10 for structure.

Thus, total Analyses score would be 0+10+50=60.

Hmm, but maybe the presence of the required keys (id, analysis_name, analysis_data) is sufficient for structure, even if there's an extra key. The problem says "proper key-value pair structure". Maybe as long as the required keys are present, structure is okay. The extra keys might be allowed as additional info, but the problem specifies that the structure should follow groundtruth's. So likely, structure is incorrect, hence 0.

Alternatively, if the extra keys are allowed (since the problem didn't mention that extra keys are prohibited), then structure is okay. But the task says "structure accounts for 10 points: verify correct JSON structure of each object and proper key-value pair structure in sub-objects. Do not score on content of key-value pairs." So the structure is about having the correct keys. Since the Annotation added "label", which isn't in groundtruth, the structure is incorrect. Thus, structure is 0.

So total Analyses: 60.

Now onto **Results**.

**Groundtruth Results**:
- 3 sub-objects linked to analyses_2, 3, 4.
- Each has analysis_id, metrics, features, value.

**Annotation Results**:
- 9 sub-objects linked to various analyses (analysis_1 to analysis_8).
- Let's compare each GT result to see if they exist in the Annotation.

First, list GT results:

1. analysis_id: analysis_2 (Spearman) → metrics: correlation, features: IGHM, value [0.56, p<0.001].
2. analysis_id: analysis_3 (Diff expr) → metrics: log2FC, features: IGHM, value [2.64, p<0.001].
3. analysis_id: analysis_4 (ROC) → metrics: auc, features: [preEM, Continous igM, Ordinam IgM], value with three entries.

Check each in Annotation's results:

1. **GT Result 1**: analysis_2 (Spearman) → in Annotation's results, look for analysis_id: analysis_2. The Annotation has a result for analysis_2 with metrics "AUC" and features ["IgM"]. Doesn't match GT's metrics (correlation) or features (IGHM vs IgM). So not a match.

2. **GT Result 2**: analysis_3 (Diff expr) → in Annotation's results, analysis_3 has metrics "null" and features ["activated GPCR", ...]. No match to log2FC or IGHM.

3. **GT Result 3**: analysis_4 (ROC) → in Annotation, analysis_7 (which corresponds to GT's analysis_4) has results. Looking for analysis_id analysis_7. The Annotation's analysis_7 result has metrics "AUC", features ["serum IgM"], value 0.81. The GT's analysis_4's result had features like preEM etc., so not a direct match. 

Wait, let me check the Annotation results:

Looking at Annotation's results for analysis_4 (multivariate logistic regression):

There's a result with analysis_id analysis_4, metrics "p-value", value "<0.001", features ["serum IgM"]. Not matching GT's analysis_4 result.

The Annotation's analysis_7 (ROC) has a result with analysis_id analysis_7, metrics AUC, value 0.81, features serum IgM. The GT's analysis_4's result had features including preEM, Continous igM, etc. So not matching.

Thus, none of the GT results are directly present in the Annotation's results. 

**Structure (10 points)**:
Check if the results' sub-objects have the correct keys. GT's keys: analysis_id, metrics, features, value. The Annotation's results have these keys (except sometimes "value" is a single string, but GT's value is an array). Also, in some cases, features is an array (like in GT's third result) versus a string (e.g., "IGHM"). The keys are present but types may differ. 

Wait, structure is about the existence of keys, not their data types. For example, if features is a string vs array, that's content, not structure. So structure-wise, the keys are present. So structure is okay. So 10/10.

**Content Completeness (40 points)**:
GT has 3 results. The Annotation has 9 results. Need to see how many GT results are present in Annotation.

None of the three GT results are present in the Annotation's results. 

Thus, all 3 are missing. Each missing deducts 40/3 ≈13.33 points each? Wait, the problem says "deduct points for missing any sub-object". The total is 40, so per missing sub-object, 40/3 ≈13.33, but since it's 3 missing, 3*(40/3)=40, so 0 left. But since there are more than GT's count, maybe extra penalty? 

Wait, the completeness is based on the GT's sub-objects. Each missing GT sub-object deducts (40/3)*number_missing. Since all 3 are missing, 40 - (3*(40/3)) = 0. So completeness score is 0. But the Annotation has extra results (9 vs 3). The problem says extra may incur penalties. Each extra might take off 1 point, so 6 extra → 6 points off. Total 0 -6 = -6? But can't go below 0. So just 0.

**Content Accuracy (50 points)**:

Since none of the GT results are matched in Annotation, there are no sub-objects to score for accuracy. Thus, 0/50.

Total Results Score: 10 (structure) + 0 + 0 = 10.

Wait, but maybe I missed something. Let's recheck:

GT Result 1: analysis_2 (Spearman) → in Annotation, there's a result with analysis_1 (Proteotranscriptomic) having metrics "Spearman correlation" and features IGHM. The analysis_id here is analysis_1, not analysis_2. So even if the content matches (Spearman correlation on IGHM), the analysis_id is wrong. Since the analysis_id refers to a different analysis in the Annotation (analysis_1 vs GT's analysis_2), it's not considered a match. 

Similarly, GT Result 2 (analysis_3's log2FC on IGHM) might be in Annotation's analysis_1's second result (metrics log2FC, features IGHM). Analysis_id here is analysis_1 again. So even if the content matches, the analysis_id is wrong, so not counted.

GT Result 3 (analysis_4's ROC with specific features) is not present. 

Thus, indeed, no matches. So the scores hold.

Final Scores:

Data: 98

Analyses: 60 (if structure 0, completeness 10, accuracy 50)

Results: 10

Wait, but let me confirm the analyses structure again. If the analyses' structure is 0 because of the extra key "label", then yes. Otherwise, if structure allows extra keys, then structure is 10. But the problem states that structure must follow the groundtruth's keys. Hence 0.

Thus, Final Scores as per my calculation would be:

{
    "Data": 98,
    "Analyses": 60,
    "Results": 10
}

But I might have made errors. Let me cross-verify once more.

For Data's accuracy, the source in data_1 was an issue. Since GT's source is empty, but Annotation added a source, is that a discrepancy? If the source is actually correct (as per the article), then it's okay. Since we don't have the article, but the groundtruth didn't include it, maybe the Annotation shouldn't add it. So the deduction there is valid. So Data's accuracy was 48 (50-2). So Data total 10+40+48=98.

Analyses' structure: 0. Completeness: 10 (only 1 out of 4 matched). Accuracy:50 (that one was perfect). So 60 total.

Results: 10 (structure okay, completeness 0, accuracy 0) → 10.

Yes. So the final scores would be as above.
</think>

"Final Scores": {
    "Data": 98,
    "Analyses": 60,
    "Results": 10
}

### Scoring Breakdown:

#### **Data Component**
- **Structure (10/10)**: All required keys (`id`, `omics`, `link`, `format`, `source`, `public_id`) are present in both sub-objects. The `null` values in the annotation do not affect structure validity.
- **Content Completeness (40/40)**: Both sub-objects (`data_1` and `data_2`) are present in the annotation and semantically match the groundtruth (e.g., "Proteomics" vs. "Serum proteome data").
- **Content Accuracy (48/50)**:  
  - Minor deduction (-2): The `source` field in `data_1` of the annotation introduces a value not present in the groundtruth (which had an empty `source`). While the source may be correct, the groundtruth explicitly omits it, leading to a slight discrepancy.

---

#### **Analyses Component**
- **Structure (0/10)**: The annotation introduces an extra `label` key in all analyses, deviating from the groundtruth structure (which only includes `id`, `analysis_name`, and `analysis_data`). This violates the required structure.
- **Content Completeness (10/40)**:  
  - Only one analysis (`ROC analysis`/`analysis_4` in groundtruth and `analysis_7` in the annotation) is fully matched.  
  - Three groundtruth analyses (`PCA`, `Spearman correlation`, and `differential expression`) are missing.  
  - Four extra analyses (e.g., `Nomogram`, `Multivariate logistic regression`) are included but not penalized due to contextual relevance.
- **Content Accuracy (50/50)**:  
  - The matched `ROC analysis` (`analysis_7`) has accurate `analysis_name` ("Receiver Operating Characteristic (ROC)") and correct `analysis_data` (`data_1`).

---

#### **Results Component**
- **Structure (10/10)**: All required keys (`analysis_id`, `metrics`, `features`, `value`) are present in sub-objects. Type mismatches (e.g., `features` as arrays vs. strings) affect content accuracy, not structure.
- **Content Completeness (0/40)**: None of the three groundtruth results are semantically matched in the annotation. Key mismatches include analysis IDs, metrics (e.g., `correlation` vs. `AUC`), and features (e.g., `IGHM` vs. `serum IgM`).
- **Content Accuracy (0/50)**: No groundtruth results are accurately represented in the annotation due to missing semantic matches.  

---

### Final Scores:
```json
{
    "Data": 98,
    "Analyses": 60,
    "Results": 10
}
```