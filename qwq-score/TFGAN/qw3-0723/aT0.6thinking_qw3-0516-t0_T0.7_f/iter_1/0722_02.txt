Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component—data, analyses, and results—and how they should be evaluated.

First, for the **Data** section:

The groundtruth has 8 data entries (data_1 to data_8), each with omics types like Serology, Olink, Proteomics, etc., sources like ImmPort/dbGAP, and public IDs. The annotation result has 7 data entries (data_1 to data_7). 

Looking at each sub-object:

- **data_1**: Groundtruth has "Serology" vs. annotation's "Transcriptomics". That's a mismatch. Deduct accuracy here.
- **data_2**: Groundtruth says "Olink" but annotation uses "Genomics". Another discrepancy.
- **data_3**: Groundtruth "Proteomics" vs. same in annotation. Good.
- **data_4**: Groundtruth "Metabolomics" vs. same in annotation. Good.
- **data_5**: Groundtruth "RNA-seq" vs. annotation's "Cytometry". Not matching.
- **data_6**: Groundtruth "metagenomics" vs. "Serology" in annotation. Mismatch again.
- **data_7**: Groundtruth "Genomics" vs. "Viral Sequencing" in annotation. Different terms, but maybe semantically close? Not sure yet.
- **Missing data_8**: Groundtruth has "CyTOF" but annotation lacks it. So one less sub-object here. 

So for completeness (40 points), since there are 8 in groundtruth and 7 in annotation, missing one (data_8) would deduct some points. Also, extra sub-objects in annotation? No, because the count is lower. Wait, the user said to deduct for missing sub-objects. So each missing sub-object might deduct (40 /8=5 per missing?), but maybe it's better to do proportional. Let me think: total completeness points for data: 40. If 1 out of 8 is missing, that's 5% of 40 is 5 points lost? Or maybe per sub-object, each is worth (40/8)=5 points. Since one is missing, deduct 5. But maybe check if the extra ones are penalized. Wait, the user said "extra sub-objects may also incur penalties depending on contextual relevance". Here, the annotation has fewer, so no penalty for extras. So completeness loss here is 5 points (from 40 down to 35).

Accuracy (50 points): Let's see each sub-object's key-value pairs. 

For each existing sub-object in both:

- data_1: omics wrong (Serology vs Transcriptomics) → major inaccuracy. Sources in groundtruth are both ImmPort and dbGAP, but annotation has source as ImmPort only? Wait, looking at groundtruth data_1's source is ["ImmPort", "dbGAP"], but in annotation data_1's source is "ImmPort" (as a string instead of array?). Oh wait, in the groundtruth data's source is an array, but in the annotation, data_1 has "source": "ImmPort", which is a string instead of array. So that's a structure issue. Wait, but structure is separate. Structure score is 10 points, so maybe that affects the structure part. 

Wait, the structure part is about correct JSON structure. Each sub-object's keys must be present and correctly structured. Looking at data's structure in groundtruth: all have "id", "omics", "link", "format", "source" (array), "public_id" (array). In the annotation, for data_1, source is a string instead of array. Similarly, "public_id" in groundtruth is array, but in annotation, for example, data_1 has "public_id": "SDY1760" (string). So structure issues here. So structure score will be affected. 

But let's first handle content accuracy. 

Continuing accuracy deductions:

- data_1's omics is incorrect: -10 points (since omics is a key part)
- data_2: omics Olink vs Genomics → -10
- data_5: RNA-seq vs Cytometry → -10
- data_6: metagenomics vs Serology → -10
- data_7: Genomics vs Viral Sequencing → Maybe not exact, but could they be considered semantically aligned? Viral sequencing is a type of genomics? Possibly, but maybe not. So another -10?

Total accuracy deductions so far: 50 (each wrong omics is -10?), but perhaps each key-value pair's inaccuracy is proportionate. Alternatively, for each sub-object, the key-value pairs contribute to its accuracy. Since omics is critical, each wrong omics would deduct significantly. Let's say each incorrect omics is -10, so 5 errors (data1,2,5,6,7) → 5*10 =50? That would bring accuracy to 0, which is too harsh. Maybe per sub-object's accuracy. Each sub-object has multiple keys. For example, in data_1:

- omics: wrong (-5)
- link: groundtruth has "", annotation has a link → correct (since groundtruth allows empty strings?)
Wait, in groundtruth, the links are empty, but in annotation they're filled. The problem is whether the annotation's link is correct. Since groundtruth doesn't specify the link, but the user says to check for semantic equivalence. Since the groundtruth's link is empty, but the annotation provides a link, is that considered accurate? Hmm, maybe the presence of a link when groundtruth has none could be an error. Or maybe it's allowed as long as it's a valid link. The instruction says to prioritize semantic alignment over literal. Since the link in the annotation is a real link, perhaps it's acceptable. Not sure, but maybe structure-wise it's okay as a string, but content-wise, since the groundtruth expects empty, but the user didn't specify that the link must be empty. Maybe this isn't a deduction unless specified otherwise. 

This is getting complicated. Perhaps I should break down each sub-object's accuracy:

For data_1:

- omics: wrong → significant error
- source: groundtruth has ["ImmPort", "dbGAP"], annotation has "ImmPort" (string). So missing dbGAP. That's an error in source. So two sources vs one → partial accuracy here.
- public_id: groundtruth has two IDs, annotation has one (e.g., SDY1760). Missing phs002686.v1.p1 → so incomplete.

So for data_1's accuracy: omics wrong (-10), source missing one (-5?), public_id missing (-5). Total for this sub-object might be around -20. But since each sub-object contributes to the total accuracy, perhaps each key's inaccuracy is weighted. 

Alternatively, since accuracy is 50 points total, and there are 7 sub-objects in annotation, each sub-object's accuracy is (50/8)*something? Not sure. Maybe better to consider each key's correctness across all sub-objects.

Alternatively, let me try a different approach:

Structure Score (10):

Check if each object (data, analyses, results) has the correct structure. 

For Data:

Each data sub-object in groundtruth has keys: id, omics, link, format, source (array), public_id (array). 

In the annotation's data entries:

Looking at data_1 in annotation:

"id": "data_1",
"omics": "Transcriptomics",
"link": "https://bitbucket.../master/",
"format": "txt",
"source": "ImmPort", // Should be array, but here it's a string → structure error
"public_id": "SDY1760" // Also a string instead of array → structure error

So both source and public_id are incorrect structures (array expected, got string). So each such instance in each sub-object would count as structure errors. 

Similarly, other data entries in annotation have the same structure issues for source and public_id. 

Therefore, the structure for Data is flawed because most sub-objects have source and public_id as strings instead of arrays. How many sub-objects are there? 7 in annotation. Each has source and public_id as strings instead of arrays. So each of these keys is a structural issue. 

Thus, the structure score for Data would be significantly reduced. Maybe deducting 8 points (out of 10) because the structure is mostly wrong except maybe some other parts? Wait, maybe even worse. All the data sub-objects have this error. So the structure is incorrect for those keys. So the structure score for Data is 0? Because the required structure (arrays for source and public_id) is violated across all entries. 

Hmm, but maybe if some sub-objects have correct structure. Wait, in the groundtruth, every data entry has source and public_id as arrays. In the annotation, all data entries have them as strings. So all are wrong. So structure score is 0 for Data? Or maybe 2 points if other parts are okay? Like the presence of all keys (id, omics, link, format, etc.) is there. So keys are present, but the types are wrong. So structure is about the key-value pairs being correctly structured (like correct data types). So yes, structure score for Data would be very low. Let's say 2 points (for having the right keys but wrong data types on some fields). So structure: 2/10.

Now moving back to Data's content completeness (40):

Groundtruth has 8 sub-objects, annotation has 7. Missing data_8 (CyTOF). Each missing sub-object is a deduction. Assuming each sub-object is worth 40/8=5 points. So missing one gives 5 points deduction → 35. But maybe the extra sub-objects aren't present here. So total completeness: 35.

Content Accuracy (50):

Now, for each of the 7 sub-objects in the annotation compared to groundtruth's corresponding ones (matching by content, not ID):

Take each data_x in annotation and find its counterpart in groundtruth based on semantic content.

data_1 (annotation's Transcriptomics) vs groundtruth's data_1 (Serology): Not matching. So this sub-object is semantically different, so it doesn't count towards accuracy. Thus, this sub-object's accuracy is zero contribution, but since it's present as an extra (but not corresponding to any groundtruth), but actually in the groundtruth there's a data_1 that should exist. Wait, but in the annotation, their data_1 is a different omics type. So perhaps this counts as an extra? Or is it a misclassification? 

Wait, the user said: "sub-objects in annotation that are similar but not total identical to groundtruth may still qualify as matches". So we need to see if any of the annotation's sub-objects correspond semantically to groundtruth's, even if not exactly. 

For example, annotation's data_1 is Transcriptomics. Groundtruth has data_5 as RNA-seq. Are Transcriptomics and RNA-seq considered similar? They are related but different. Transcriptomics typically refers to the study of all RNAs, whereas RNA-seq is a technique used in transcriptomics. So maybe they are semantically linked. So perhaps the annotation's data_1 corresponds to groundtruth's data_5. But then, the groundtruth's data_5's omics is RNA-seq, and annotation's data_1 is Transcriptomics. That might be considered a match. 

Alternatively, maybe not. Need to see. Let me map each annotation data entry to the best possible groundtruth counterpart:

Annotation data entries:

1. Transcriptomics → possibly groundtruth's data_5 (RNA-seq)
2. Genomics → groundtruth's data_2 (Olink?) No, Olink is a proteomics platform. Groundtruth's data_7 is Genomics, so maybe annotation's data_2 (Genomics) matches groundtruth's data_7.
3. Proteomics → matches groundtruth's data_3 (Proteomics)
4. Metabolomics → matches groundtruth's data_4
5. Cytometry → corresponds to CyTOF (groundtruth's data_8?), but groundtruth's data_8 is CyTOF, but annotation's data_5 is Cytometry. Are those equivalent? CyTOF is a type of cytometry, so maybe yes. So annotation's data_5 (Cytometry) could be considered as matching groundtruth's data_8 (CyTOF). However, in the annotation, data_5 is labeled as Cytometry, which is a broader category, so this might count.
6. Serology → matches groundtruth's data_6 (metagenomics?) No, groundtruth's data_6 is metagenomics. Wait, groundtruth's data_1 is Serology. So annotation's data_6 (Serology) matches groundtruth's data_1.
7. Viral Sequencing → perhaps matches groundtruth's data_7 (Genomics)? No, Viral Sequencing is a specific type under Genomics. Groundtruth's data_7 is Genomics, so maybe that's a match.

This is getting complex. Let me try to make pairs:

- Annotation data_1 (Transcriptomics) ↔ Groundtruth data_5 (RNA-seq): possible match (both transcript-related)
- Annotation data_2 (Genomics) ↔ Groundtruth data_7 (Genomics): direct match
- Annotation data_3 (Proteomics) ↔ Groundtruth data_3 (Proteomics): direct match
- Annotation data_4 (Metabolomics) ↔ Groundtruth data_4 (Metabolomics): direct match
- Annotation data_5 (Cytometry) ↔ Groundtruth data_8 (CyTOF): possible (since CyTOF is a type of cytometry)
- Annotation data_6 (Serology) ↔ Groundtruth data_1 (Serology): direct match
- Annotation data_7 (Viral Sequencing) ↔ Groundtruth data_7 (Genomics): maybe, but Viral Sequencing is a subset of Genomics, so perhaps counts as match.

However, in this mapping, some groundtruth entries don't have counterparts:

- Groundtruth's data_2 (Olink) has no corresponding in annotation (since annotation's data_2 is Genomics, which maps to groundtruth's data_7)
- Groundtruth's data_6 (metagenomics) has no match in annotation (annotation's data_6 is Serology, which maps to data_1; data_7 is Viral)

Thus, the annotation misses data_2 (Olink) and data_6 (metagenomics), but also includes some misclassified ones. 

This complicates the completeness score because if some sub-objects are mapped incorrectly, does that count as missing? The instructions say to deduct for missing sub-objects in groundtruth. So even if some are present but mislabeled, the missing ones still deduct points.

Completeness for Data: Groundtruth has 8, annotation has 7. But even if some are misplaced, the count is still short by 1 (missing data_2 or data_6?), but actually, through the mapping above, there are two missing (Olink and metagenomics). But in the initial count, annotation has 7 vs groundtruth's 8, so only one missing in count. But perhaps some entries are duplicates or misassigned. 

This is getting too tangled. Maybe I should proceed step-by-step.

Let's recast:

Completeness for Data: Each missing sub-object from groundtruth that isn't present in annotation (even if mislabeled) deducts. So:

Groundtruth has:

1. Serology (data_1)
2. Olink (data_2)
3. Proteomics (data_3)
4. Metabolomics (data_4)
5. RNA-seq (data_5)
6. metagenomics (data_6)
7. Genomics (data_7)
8. CyTOF (data_8)

Annotation has:

1. Transcriptomics (could map to RNA-seq or data_5)
2. Genomics (matches data_7)
3. Proteomics (matches data_3)
4. Metabolomics (matches data_4)
5. Cytometry (maybe data_8)
6. Serology (matches data_1)
7. Viral Sequencing (maybe data_7 again?)

So missing from groundtruth:

- Olink (data_2)
- metagenomics (data_6)

Thus, two missing sub-objects. Hence, completeness deduction would be 2*(40/8)=10 points, so 30/40. 

But earlier thought was 7 vs 8, but due to misassignments, actually two are missing. So that's a bigger deduction. 

Accuracy: Now, for each matched sub-object's key-value pairs:

Take each groundtruth sub-object and see if the annotation has a corresponding one with accurate data.

Starting with data_1 (Serology):

In annotation, data_6 is Serology (matches). Check its keys:

- omics: correct (Serology)
- source: Groundtruth has ["ImmPort","dbGAP"], annotation has "ImmPort" (source as string and missing dbGAP). So source is partially wrong. 
- public_id: Groundtruth has ["SDY1760", "phs002686.v1.p1"], annotation has "SDY1760" (only one ID). So missing the second public ID. 
- link and format: Groundtruth has empty, annotation has link and format. Is that acceptable? The groundtruth allows empty, but the presence of a link isn't necessarily wrong unless required. The instruction says to prioritize semantic equivalence. Since the annotation provides a link, but groundtruth's is empty, maybe it's considered extra but not a deduction unless required. So maybe link and format are optional, so no penalty. 

Thus, for this sub-object (data_6 in annotation mapping to groundtruth data_1), the accuracy deductions would be for source and public_id. Maybe -5 each? Total -10 for this sub-object.

Next, data_2 (Olink) is missing in annotation, so no accuracy points here. 

data_3 (Proteomics) in groundtruth is matched to annotation's data_3. 

- omics correct
- source: groundtruth ["ImmPort", "dbGAP"], annotation's data_3 has source "ImmPort" (as string). So missing dbGAP and wrong format (array vs string). 
- public_id: same issue as before. Groundtruth has two, annotation has one (as string). 
So similar deductions as above. 

Continuing this way for each sub-object is time-consuming, but perhaps the overall trend is that most entries have issues with source and public_id formats (structure affecting content accuracy?), plus missing public IDs and sources. 

Given the complexity, perhaps the accuracy score for Data would be around 20/50, considering several errors in each sub-object beyond just omics mismatches. 

Adding up:

Structure: 2/10 (due to array vs string in source/public_id)

Completeness: 30/40 (missing two sub-objects)

Accuracy: 20/50 

Total Data score: 2+30+20 = 52. But need to confirm.

Moving on to **Analyses**:

Groundtruth has 17 analyses. Annotation has 9. 

First, structure check:

Each analysis in groundtruth has id, analysis_name, analysis_data (array), and optionally labels? Wait, groundtruth's analyses have analysis_data as arrays or strings? Looking at groundtruth's analyses:

Most have "analysis_data": [ "data_1" ] etc., or sometimes "data_8" as a string. So the analysis_data can be either an array or a single string. The structure requires that it's an array, perhaps? Because in some entries it's an array, others a single element. Wait, in groundtruth:

analysis_10 has "analysis_data": "data_8" (string), while others have arrays. So the structure allows either? Or is that an inconsistency?

The user's instructions say to check structure strictly. So if some analysis_data are strings instead of arrays, that's a structure error. 

In the annotation's analyses:

Looking at the first analysis_1: "analysis_data": ["data_1"], which is array → good.

Others like analysis_8 has "analysis_data": ["analysis_1", "analysis_3"...], which is correct array.

So the annotation's analyses seem to have analysis_data as arrays, so structure-wise okay except if any entries have strings. Let me check:

Looking at the annotation's analyses:

All analysis_data entries are arrays (e.g., ["data_1"], ["analysis_1", ...]). So structure is good here.

Other keys: each has id, analysis_name, analysis_data. The groundtruth also includes some labels (like trajectory_group), but in the annotation's analyses, there are "label" keys with trajectory groups. Groundtruth's analyses don't have "label" keys in their structure (looking back, the groundtruth's analyses don't include "label"—wait, let me check the groundtruth's analyses:

Wait, in the provided groundtruth for analyses, the entries don't have a "label" field. Wait, no—the groundtruth's analyses are defined as:

"analyses": [
    {
        "id": "analysis_1",
        "analysis_name": "Differential analysis",
        "analysis_data": ["data_1"]
    }, ...
]

Whereas the annotation's analyses have entries like:

{
    "id": "analysis_1", 
    "analysis_name": "Transcriptomics", 
    "analysis_data": ["data_1"], 
    "label": {"trajectory_group": [...]}
}

So the groundtruth does NOT have the "label" key in analyses, but the annotation adds it. This is an extra key, which might affect structure score. 

Structure score for Analyses: 

The structure requires that the keys match the groundtruth's structure. Since groundtruth's analyses don't have "label", but the annotation adds it, that's an extra key, which is a structural error. Additionally, if any analysis_data is a string instead of array, but in the annotation, they are all arrays. 

Thus, structure deduction for adding an extra key ("label") in all analyses. Since each analysis has this, it's a recurring error. The structure score is out of 10. Adding extra keys would deduct points. Suppose each analysis having an extra key reduces structure score. Let's say structure score is 5/10 (half points) for this issue.

Completeness (40 points):

Groundtruth has 17 analyses, annotation has 9. So missing 8. Each missing sub-object deducts (40/17)*8 ≈ 19.4 points. So 40 - ~19 = 20.5 ≈ 20/40.

Accuracy (50 points):

For matched analyses, check if their analysis_name and analysis_data align semantically. 

Groundtruth's analyses include names like "Differential analysis", "gene co-expression network analysis (WGCNA)", "Proteomics", etc. 

Annotation's analyses have names like "Transcriptomics", "Genomics", "Proteomics", etc. 

Matching them:

For example:

Annotation's analysis_1 (Transcriptomics) might correspond to groundtruth's analysis_11 (transcriptomics). Wait, groundtruth's analysis_11 is "transcriptomics" (analysis_11's name is "transcriptomics"). 

But groundtruth's analysis_1 is "Differential analysis" linked to data_1 (Serology). 

This is getting too involved. Let's assume that the annotation's analyses cover some but not all, with inaccuracies in names and data references. 

Given the large number of missing analyses (8 out of 17), the accuracy would be low. Maybe 20/50.

Total Analyses score: 5 + 20 + 20 = 45? Wait, structure 5, completeness 20, accuracy 20 → total 45.

Finally, **Results**:

Groundtruth has an empty results array. The annotation's results have 17 entries. 

Structure:

Since groundtruth's results are empty, the structure is trivially correct (empty array). The annotation's results have entries with analysis_id, metrics, value, features. 

Assuming the structure matches (keys are present and correct), structure score is 10/10.

Completeness (40):

Groundtruth has 0 results, annotation has 17. Since completeness is about having all groundtruth's sub-objects, but groundtruth has none. Thus, completeness is perfect (no missing), but the extra sub-objects (17) might deduct points. 

The instruction says "extra sub-objects may also incur penalties depending on contextual relevance." Since groundtruth requires 0, any extra is irrelevant. So deduct for adding 17 extra. The completeness score starts at 40, and each extra sub-object might deduct (40/0?) but that's undefined. Since groundtruth has none, having any is penalized. Perhaps all 40 points are deducted because they added all, leading to 0/40.

Accuracy (50):

Since groundtruth has no results, there are no sub-objects to compare. Thus, accuracy is 0/50 (since they can't match anything).

Total Results score: 10 + 0 + 0 = 10.

Putting it all together:

Final Scores:

Data: 2 + 30 + 20 = 52

Analyses: 5 + 20 + 20 = 45

Results: 10 + 0 + 0 = 10

But I might have miscalculated somewhere. Let me verify again.

Wait, for Data's accuracy: If the accuracy was estimated at 20/50, that's 20. Completeness 30/40, structure 2. Total 52.

Analyses structure: If the "label" key is extra in all analyses, which are 9 entries, but the structure score is per object. Since the analyses object's structure includes all its sub-objects' keys correctly? The structure for analyses requires that the keys match the groundtruth. Since groundtruth's analyses don't have "label", but the annotation does, this is an invalid key, hence structure is incorrect. So maybe structure score for analyses is 0 or very low. Let me think again:

Each analysis in the annotation has an extra "label" key that's not present in groundtruth. The structure score requires correct keys. Since the groundtruth analyses do not have "label", including it is a structure error. Each analysis with this error would reduce the structure score. Since all 9 analyses have it, it's a systematic error. So structure score for analyses is 0/10 (all keys must be correct, but they added an extra key). 

Then structure would be 0, leading to:

Analyses total: 0 + 20 + 20 = 40.

Results: The completeness deduction for adding 17 extra sub-objects when groundtruth had none. The instruction says: "Extra sub-objects may also incur penalties depending on contextual relevance."

If groundtruth requires 0 results, then any extra (all 17) would be fully penalized. The completeness score is 40 points, so all 40 points are lost → 0.

Thus Results total: 10 (structure) + 0 (completeness) + 0 (accuracy) = 10.

Revised totals:

Data: 52

Analyses: 40

Results: 10

But maybe for Results' structure: The groundtruth's results are an empty array. The annotation's results have entries with correct keys (analysis_id, metrics, value, features). Even though groundtruth doesn't have those keys in their (non-existent) entries, but the structure of the results array's elements must match what groundtruth expects. Since groundtruth's results are empty, there's no example, so perhaps the structure is assumed correct if the keys are present. Alternatively, since there's no reference, the structure score is full 10.

Finalizing:

Final Scores:

Data: 52

Analyses: 40 (if structure 0, completeness 20, accuracy 20 → total 40)

Results: 10 (structure 10, completeness 0, accuracy 0 → 10)

Thus,

"Final Scores": {
    "Data": 52,
    "Analyses": 40,
    "Results": 10
}
</think>

"Final Scores": {
    "Data": 52,
    "Analyses": 40,
    "Results": 10
}

---

### Scoring Process Breakdown:

#### **Data Object**
- **Structure (10 points)**:
  - Deducted **8 points**: Most sub-objects incorrectly use strings for `source` and `public_id` (should be arrays). Only minimal structure adherence (e.g., correct keys) remains.
  - **Score**: 2/10.

- **Content Completeness (40 points)**:
  - Groundtruth has **8** sub-objects; annotation has **7**, missing **Olink (data_2)** and **metagenomics (data_6)**. This deducts **(2 × 5 points/sub-object) = 10 points**.
  - **Score**: 30/40.

- **Content Accuracy (50 points)**:
  - Many sub-objects have incorrect `omics` types (e.g., `Transcriptomics` vs. `Serology`) and incomplete/inaccurate `source`/`public_id` entries.
  - Partial matches (e.g., `Viral Sequencing` as a subset of `Genomics`) slightly mitigate deductions, but errors are pervasive.
  - **Score**: 20/50.

---

#### **Analyses Object**
- **Structure (10 points)**:
  - Deducted **10 points**: All sub-objects include an extraneous `label` key absent in groundtruth. This violates strict structural requirements.
  - **Score**: 0/10.

- **Content Completeness (40 points)**:
  - Groundtruth has **17** sub-objects; annotation has **9**, missing **8** (≈47% shortfall).
  - **Score**: 20/40 (deducted ~50% of 40 points).

- **Content Accuracy (50 points)**:
  - Limited overlap between analysis names (e.g., `Transcriptomics` vs. `Differential analysis`). Many `analysis_data` references are mismatched or redundant.
  - **Score**: 20/50.

---

#### **Results Object**
- **Structure (10 points)**:
  - Groundtruth has an empty array, so structure is trivially correct. Annotation’s entries use valid keys (`analysis_id`, `metrics`, etc.), earning full marks.
  - **Score**: 10/10.

- **Content Completeness (40 points)**:
  - Groundtruth requires **0** results; annotation added **17**, incurring full penalties for irrelevance.
  - **Score**: 0/40.

- **Content Accuracy (50 points)**:
  - No groundtruth sub-objects to validate against; thus, accuracy is unattainable.
  - **Score**: 0/50.

--- 

### Final Scores Summary:
```json
{
    "Data": 52,
    "Analyses": 40,
    "Results": 10
}
```