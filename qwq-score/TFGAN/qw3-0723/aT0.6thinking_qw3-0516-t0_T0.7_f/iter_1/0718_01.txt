Okay, I need to score the provided annotation results against the groundtruth according to the given criteria. Let me start by understanding each component—data, analyses, and results—and how they should be evaluated.

First, for the **Data** section:

The groundtruth has one data entry with keys like omics, link, format, source, public_id. The annotation has two data entries. Let me check each part.

**Structure (10 points):**
- Groundtruth uses "omics" as RNA-seq, while the annotation uses "Transcriptomics". Are these considered equivalent? Since RNA-seq is a type of transcriptomics, maybe they're semantically the same. But the key names are the same, so structure-wise it's okay. The other keys like link, format, source, public_id are present in both. So structure looks good. Full 10 points.

**Content Completeness (40 points):**
Groundtruth has one sub-object. Annotation has two. Need to check if the extra sub-object (data_2) is relevant. Data_2 includes "Processed Data" format and same source but no link. The groundtruth doesn't mention processed data, so maybe this is an extra. However, the first data entry in annotation matches the groundtruth's data_1 except for some values. Since there's an extra sub-object (data_2), which isn't in the groundtruth, this might deduct points. But since the user said extra sub-objects may incur penalties depending on relevance. If data_2 is part of the data mentioned in the article, maybe acceptable, but since groundtruth didn't include it, perhaps it's an extra. So deduct 10 points (for adding an extra). But the first sub-object exists, so maybe only the extra is penalized. Wait, the groundtruth requires all sub-objects in the groundtruth to be present. Since the annotation has two instead of one, but the first is correct, so missing none, but added one. The penalty for extra? Maybe 10 points off. Hmm, the instructions say deduct points for missing any sub-object. But for extra sub-objects, it depends on context. Since the groundtruth has only one, but the user's annotation added another, which might not be present in the groundtruth, so probably deduct 10 points (40 -10=30).

Wait, but maybe the second data entry in the annotation corresponds to something else? Like the groundtruth's data_1 is the raw RNA-seq, and the second is processed. Maybe the groundtruth's data_1 is supposed to have a link? The groundtruth's data_1 has empty link and public_id, while the annotation filled in some details. But the main point is whether all required sub-objects are present. The groundtruth's data array has one element. The annotation has two. Since the groundtruth didn't have a second data entry, adding one could be a mistake. So yes, deduct 10 points here.

**Content Accuracy (50 points):**

For the existing data_1 in annotation:
- Omics: Groundtruth says RNA-seq vs Transcriptomics. Is that accurate? RNA-seq is a technique under transcriptomics, so maybe the annotation is more general. But the key should match. The key name is "omics", so the value is part of the data. Since the groundtruth's "omics" is RNA-seq, the annotation's Transcriptomics is a broader term. This might be a discrepancy. So maybe deduct some points here.
- Link: Groundtruth has empty, annotation provides a URL. That's better, so accurate.
- Format: Groundtruth fastq vs FASTQ (case difference). Probably acceptable, so okay.
- Source: Groundtruth empty vs Samsung Medical Center. The groundtruth might have intended to leave it blank, but if the correct source is known, then accurate. But since the groundtruth has it empty, maybe it's better to leave it blank. So this is a discrepancy. Deduct points for source.
- Public_id: Both empty, so okay.

So for data_1's accuracy, the omics term and source have issues. Let's see how much to deduct. Maybe 5 points for omics term mismatch, and 5 for source. Total deduction 10, so 50-10=40. 

But wait, the public_id is okay. The link is better. So maybe overall, 50 minus 10 (for omics and source). Then for the second data entry (data_2), since it's an extra, maybe its presence affects accuracy? Or since it's an extra sub-object, the accuracy is only about the existing ones. Since we already penalized completeness, here maybe just focus on the existing data_1. So total accuracy for data would be 40.

Total Data Score: Structure (10) + Completeness (30) + Accuracy (40) = 80?

Wait let me recalculate:

Structure: 10

Completeness: 40 -10 (extra sub-object) =30

Accuracy: 50 - (say 10 for the omics and source) → 40. 

Total 80.

Now moving to **Analyses**:

Groundtruth has analyses from analysis_1 to analysis_6. Let me count them: 6 analyses. The annotation has analysis_1 to analysis_7: 7 analyses.

Check each part:

**Structure (10 points):**
Each analysis has id, analysis_name, analysis_data, etc. The keys look consistent between groundtruth and annotation. Some have labels, others don't. For example, in the groundtruth, analysis_2 has label with sample_type, while the annotation's analysis_1 has label with Group. The key names are different (sample_type vs Group) but the content might align (IMCD, HC etc). The structure of having analysis_data as an array of strings is maintained. The keys seem properly named. So structure is okay. 10 points.

**Content Completeness (40 points):**
Groundtruth has 6 analyses. Annotation has 7. Need to check if any of the groundtruth's analyses are missing.

Looking at the groundtruth analyses:

- analysis_1: RNA-seq, data [data_1]
- analysis_2: Differential expr, data [analysis_1], sample_type IMCD vs HC
- analysis_3: same but NMCD vs HC
- analysis_4: IMCD vs NMCD
- analysis_5: Functional enrich on analysis_4
- analysis_6: Differential expr with three groups (IMCD,NMCD,HC)

Annotation's analyses:

- analysis_1: Transcriptomics (like RNA-seq?), data [data_1], Group [IMCD,NMCD,HC]
- analysis_2: Diff analysis, data [data_1], Group same
- analysis_3: PCA
- analysis_4: Pairwise correlation
- analysis_5: Func enrich on analysis_2
- analysis_6: Alternative splicing, data [data_1], Group IMCD vs HC
- analysis_7: Validation of CXCL1, data [data_2]

So comparing:

Groundtruth's analyses are focused on differential expressions between different groups and functional enrichment. The annotation includes some similar analyses but with different names. For example:

- Groundtruth analysis_1 (RNA-seq) vs annotation analysis_1 (Transcriptomics) – likely the same, but the name differs. But since they’re the initial data processing step, this might be acceptable. 

- Groundtruth analysis_2 (Diff expr IMCD vs HC) vs annotation analysis_2 (Diff analysis with group all three). Not exactly the same, but maybe the annotation's analysis_2 is covering multiple comparisons, similar to groundtruth's analyses 2-4. But groundtruth had separate analyses for each pairwise comparison, while the annotation combines them into one. This could mean the annotation is missing some specific analyses (the pairwise ones between IMCD-NMCD, etc.). Alternatively, if the diff analysis in annotation covers all groups, maybe it's considered as one analysis instead of three. That would mean the groundtruth has analyses 2,3,4 as separate, but the annotation has one, thus missing those. 

Similarly, the groundtruth analysis_6 compares all three groups, which the annotation's analysis_6 is a diff between IMCD and HC. So that's a discrepancy.

Additionally, the functional enrichment (groundtruth analysis_5) is present in annotation analysis_5, but it's linked to analysis_2 instead of analysis_4. So dependency might be wrong.

Moreover, the annotation includes PCA (analysis_3), pairwise correlation (4), alternative splicing (6), and validation (7), which aren't in the groundtruth. These are extras, but the groundtruth might require some of them?

The groundtruth's analyses are mainly differential expression between pairs and functional enrichment. The annotation's analyses include some different types (PCA, correlation, splicing), which may not be in the groundtruth. However, the problem states to deduct for missing sub-objects (groundtruth's analyses). So any analysis in groundtruth not present in the annotation counts as missing.

Let me list the groundtruth analyses and see which are missing:

Groundtruth analyses:

1. RNA-seq (base)
2. Diff expr IMCD vs HC
3. Diff expr NMCD vs HC
4. Diff expr IMCD vs NMCD
5. Func enrich on analysis_4
6. Diff expr all three groups (IMCD,NMCD,HC)

In annotation:

Analysis_1: Transcriptomics (like RNA-seq)
Analysis_2: Diff analysis (maybe covering all groups?)
Analysis_3: PCA
Analysis_4: Pairwise correlation
Analysis_5: Func enrich (on analysis_2)
Analysis_6: Alternative splicing (diff between IMCD and HC)
Analysis_7: Validation

So:

- The base RNA-seq (analysis_1 in groundtruth) is covered by analysis_1 in annotation (as Transcriptomics), so that's okay.

- The three pairwise differential analyses (groundtruth analyses 2-4) are not directly present in the annotation. The annotation's analysis_2 is a "diff analysis" but with Group [all three]. It might not capture the individual pairwise comparisons. Hence, groundtruth analyses 2,3,4 are missing. That's 3 missing sub-objects.

- Analysis_5 (func enrich on analysis_4) in groundtruth is present in annotation analysis_5 but linked to analysis_2 instead. The dependency is incorrect, but the analysis itself (func enrich) exists. So maybe the func enrich analysis is present but linked incorrectly. Does that count as missing? Since the analysis exists but connected to wrong data, it's still present but accuracy issue.

- Groundtruth analysis_6 (diff all three groups) is somewhat covered by the annotation's analysis_2 (if it's a three-way comparison), but the analysis_6 in annotation is a two-group comparison, so not exactly. So maybe the three-way comparison is missing.

Therefore, the groundtruth has analyses 2,3,4,6 possibly missing. That's four missing sub-objects. Each missing would deduct points. Since content completeness is per sub-object, each missing deducts 40/6 ~6.66 per missing? Or per missing sub-object, 40 divided by the number of groundtruth sub-objects (6), so each worth ~6.66. So 4 missing would deduct 26.66 points. But since it's integer, maybe approximate.

Alternatively, the total possible is 40. For each missing sub-object, deduct (40 /6)*number. So 4 missing: (4/6)*40 ≈ 26.66, so deduct 27, leaving 13. But maybe the user expects per missing sub-object a fixed amount. Alternatively, the instructions say deduct for missing any sub-object, so each missing sub-object gets a penalty. Maybe each missing sub-object deducts 40/6≈6.66 points. So 4 missing would be 4*6.66=26.64, so total completeness score 40-26.64≈13.36. That's rough. Alternatively, since the user says "deduct points for missing any sub-object"—maybe each missing sub-object deducts 10 points? Because 6 sub-objects would be 60, but total is 40. Maybe per missing, 40/6≈6.66.

But the annotation also has extra analyses (PCA, correlation, etc.) which may or may not penalize. The instruction says "Extra sub-objects may also incur penalties depending on contextual relevance." Since these are additional analyses not present in the groundtruth, maybe they don't add value but could be considered as extra. However, the completeness is about having all groundtruth sub-objects. So the penalty is for missing ones, not for extra unless they're irrelevant. The PCA, etc., might be valid but since not in groundtruth, they don't contribute to completeness but aren't penalized unless they're not relevant. Since the task is to match groundtruth, extra won't help but missing will hurt.

Thus, the completeness score is around 13.36, rounded to 13. But maybe I should think differently. Since there are 6 groundtruth analyses, and the annotation has 7 but misses 4 key ones (analyses 2-4 and 6?), the penalty would be higher. Maybe each missing deducts 10 points (since 6 sub-objects, 40 points total: 40/6 ≈6.66 each). So total deduction 4 * ~7 =28, so 40-28=12.

Hmm, this is getting complicated. Maybe better to count the number of missing groundtruth analyses:

Missing analyses (from groundtruth):

- analysis_2 (IMCD vs HC)
- analysis_3 (NMCD vs HC)
- analysis_4 (IMCD vs NMCD)
- analysis_6 (three groups)

That's four analyses missing. Each missing would take away (40/6)*4 = approx 26.66 points. So completeness score is 40 -26.66≈13.33.

Then, the annotation includes analysis_7 (validation) and others, but since those aren't in groundtruth, they don't affect completeness (only penalties if they're incorrect). So completeness is ~13.33.

**Content Accuracy (50 points):**

Now, evaluating the accuracy of the sub-objects that exist in both.

Starting with analysis_1 in both:

Groundtruth analysis_1: analysis_name "RNA-seq", data [data_1], no label.

Annotation analysis_1: "Transcriptomics", data [data_1], label {Group: all three}

The analysis name is different but related. Since RNA-seq is a type of transcriptomics, maybe acceptable. The data links are correct (data_1). The label adds a Group field, which wasn't in groundtruth's analysis_1. But groundtruth analysis_1's data_1's label might not have it. Not sure if that's an error. Maybe it's okay. So minor discrepancy in name, but acceptable. Maybe deduct 2 points.

Analysis_2 in groundtruth is differential between IMCD and HC. In annotation, analysis_2 is "Differential analysis" with Group all three. So the analysis is broader. The metrics may differ. Since the analysis in the annotation is a different scope (multi-group vs pairwise), this is a discrepancy. So this analysis's purpose differs, so accuracy is low here. Deduct more points.

Similarly, the groundtruth's analysis_5 (functional enrich on analysis_4) is linked to analysis_2 in the annotation. The dependency is wrong. That's a structural error in the analysis flow. Deduct points for that.

Also, the annotation has analysis_5's analysis_data pointing to analysis_2 instead of analysis_4 (which was supposed to be the pairwise comparison leading to enrichment). That's a dependency error, so accuracy loss.

Additionally, the annotation's analysis_6 is a differential analysis between IMCD and HC, but groundtruth's analysis_6 is a three-way comparison. So that's a different analysis; maybe not accurate.

Other analyses like PCA, correlation, splicing are not in the groundtruth, so their presence is extra but doesn't affect accuracy of existing matches.

Calculating the accuracy points:

There are overlapping analyses where the annotation has equivalent ones but not exactly matching all. Let's see:

- analysis_1: minor name difference → maybe 5 points off.
- analysis_2: major discrepancy in scope → 10 points?
- analysis_5: wrong dependency → 10 points.
- analysis_6: different comparison → 5 points.

Plus other inaccuracies. The total deductions could be 30 points, so 50-30=20.

Wait, but accuracy is per the semantically matched sub-objects. So only the analyses that are considered semantically equivalent to the groundtruth's analyses are evaluated.

For example, the annotation's analysis_2 might not be equivalent to groundtruth's analysis_2 (since it's a different comparison), so it might not count as a match. Thus, maybe only analysis_1 and analysis_5 (even with wrong dependency) are partially matched.

This complicates things. Let me try again:

For accuracy, we consider only the sub-objects that are semantically matched to the groundtruth. 

Which annotations correspond to groundtruth's analyses?

Groundtruth analysis_1 (RNA-seq) → annotation analysis_1 (Transcriptomics). Considered a match. So its accuracy is checked.

Groundtruth analysis_5 (Func enrich on analysis_4) → annotation analysis_5 (Func enrich on analysis_2). The analysis exists but dependency is wrong. So it's a match but accuracy is reduced due to wrong link.

Groundtruth analysis_6 (diff all three groups) → maybe none in the annotation. The closest is analysis_2, but that's not exact.

Other groundtruth analyses (2,3,4) are missing, so their accuracy isn't assessed because they're not present.

Thus, only analyzing the accuracy of the matched sub-objects (analysis_1 and analysis_5 in the annotation):

Analysis_1:

- analysis_name: RNA-seq vs Transcriptomics → acceptable, so no deduction.
- analysis_data: correct (data_1).
- label: added Group which wasn't in groundtruth's analysis_1. The groundtruth analysis_1's data's label might be in another place (like in data_1's label?), but since the analysis's own label wasn't specified, this addition might be an error. Deduct 5 points.

Analysis_5:

- analysis_data refers to analysis_2 instead of analysis_4 → wrong dependency. Deduct 10 points.

Total deductions: 15. So accuracy would be 50 -15=35?

But also, other aspects like metrics in results might affect, but I'm focusing on analyses here. Wait, the content accuracy for analyses is about the key-value pairs in the sub-object. For analysis_5's analysis_data being incorrect is a key-value error. The key 'analysis_data' has incorrect value (analysis_2 vs analysis_4). So that's a direct accuracy issue. Similarly, the label in analysis_1 might be unnecessary, but if the groundtruth's analysis_1 didn't have a label, then including it is an error (extra key?), which could deduct points.

Alternatively, if the groundtruth's analysis_1's data's label was elsewhere, maybe the analysis's label is okay. It's a bit ambiguous. Assuming the label addition is an error, maybe another 5 points.

Total deductions 20 → accuracy 30.

Hmm, this is tricky. Maybe the accuracy score ends up around 30-35. Let's tentatively say 30.

So total Analyses score: 10 (structure) +13 (completeness) +30 (accuracy)=53? Wait, 10+13=23 +30=53. But maybe my calculations are off. Alternatively, maybe I need to recast.

Alternatively, maybe the completeness was lower. If missing 4 analyses out of 6, that's 4/6 missed → 2/3 missing. So completeness is 40*(2/6)=~13.3, so 13.3. Structure 10, accuracy 30. Total 53.3≈53.

Now **Results**:

Groundtruth has results from analysis_ids 2,3,4,5,6. The results include various metrics like fold change, p-values, FDR, and features. The annotation's results are attached to analyses 2,5,7,4, etc.

Comparing:

Groundtruth Results:

- For analysis_2 (IMCD vs HC): fold_change, p-value, FDR.
- analysis_3 (NMCD vs HC): same metrics.
- analysis_4 (IMCD vs NMCD): metrics.
- analysis_5 (func enrich): features like GO terms.
- analysis_6 (three groups): features list.

Annotation Results:

- analysis_2 has Fold change, P value, FDR with features like ANGPT2 etc.
- analysis_5 has Biological process and pathway features.
- analysis_7 has validation metrics.
- analysis_4 has Pearson r.
- analysis_6 has alternative splicing info.

Structure (10 points):

Each result has analysis_id, metrics, value, features. The keys are consistent. The groundtruth's results have features as lists, and some metrics like "FDR-adjusted p-value" vs "False discovery rate (FDR)". These are semantically the same, so structure is okay. 10 points.

Content Completeness (40):

Groundtruth has 11 results. Let's count:

- analysis_2 (3 entries: fold, p, FDR)
- analysis_3 (3 entries)
- analysis_4 (3 entries)
- analysis_5 (2 entries)
- analysis_6 (1 entry)
Total: 3+3+3+2+1=12? Wait groundtruth shows 11? Let me recount:

Looking at the groundtruth results array:

There are 11 items listed. Let me count:

1. analysis_2, fold_change
2. analysis_2, p-value
3. analysis_2, FDR-adjusted p-value
4. analysis_3, fold...
5. analysis_3, p...
6. analysis_3, FDR...
7. analysis_4, fold...
8. analysis_4, p...
9. analysis_4, FDR...
10. analysis_5, ...
11. analysis_6, ...

Yes, 11.

The annotation has results:

- analysis_2: 3 entries (Fold change, P value, FDR)
- analysis_5: 2 entries (Biological process, Pathway)
- analysis_7: 4 entries (fold, P, r, comparison)
- analysis_4: 1 (pearson)
- analysis_6: 1 (splicing)
Total: 3+2+4+1+1=11.

Now, need to check which of the groundtruth's results are present.

Groundtruth results for analysis_2's three metrics (fold, p, FDR) are present in annotation as Fold change, P value, FDR. So that's a match.

Analysis_3's results in groundtruth (three entries) are not present in the annotation. The annotation has nothing for analysis_3's results.

Analysis_4's groundtruth results have fold, p, FDR. The annotation has pearson r instead. So missing fold etc.

Analysis_5's groundtruth has functional terms (GO terms). The annotation's analysis_5 has biological processes and pathways, which are similar. The exact terms might differ, but the content is about functions, so maybe counted as present. The groundtruth's analysis_5 results have features like "immune response...", which the annotation lists similar terms. So that's a match.

Analysis_6's groundtruth result has features list, while the annotation's analysis_6 says "no significant events", which is different. So that's a discrepancy.

Additionally, the annotation includes results for analysis_7 (which isn't in groundtruth's results), and analysis_4 has a different metric.

So missing results from groundtruth:

- All analysis_3's three results (missing)
- analysis_4's fold, p, FDR (replaced with pearson)
- analysis_6's features (annotation says no events)

Total missing: analysis_3 (3) + analysis_4 (3) + analysis_6 (1) → 7 missing sub-objects.

Out of 11 groundtruth results, 4 are present (analysis_2's 3, analysis_5's 2 (but one of the analysis_5's entries is pathway, which may count?), analysis_4's pearson is not a match. Wait analysis_5 has two entries in groundtruth: the first analysis_5 in groundtruth is the functional enrich with features being GO terms. The annotation's analysis_5 has two entries (biological process and pathway), which are similar, so maybe considered present. So analysis_5's two are present. analysis_2's three are present. analysis_4's pearson is not part of the groundtruth's expected results (they had fold etc.), so that's extra. analysis_6's entry is present but incorrect.

So the present results are analysis_2 (3), analysis_5 (2), analysis_7 (4), analysis_4 (1), analysis_6 (1). But the groundtruth required analysis_3 (3), analysis_4's original results (3), and analysis_6's features (1). The annotation didn't include analysis_3's results, so 3 missing. analysis_4's groundtruth results are missing (3), analysis_6's is replaced. So total missing: 3+3+1=7. Out of 11, so completeness is (11-7)/11 *40 → 4/11≈36.36. Wait no, the completeness is about missing sub-objects. Each missing deducts. So 40 points total. For each missing sub-object (7), deduct (40/11)*7≈25.45. So 40-25.45≈14.54. Approximately 15 points.

**Content Accuracy (50 points):**

Evaluating the accuracy of the present results that match semantically:

Analysis_2's metrics:

- Fold change: groundtruth has numerical values, annotation says ".2 or <0.5". This is a discrepancy in value representation (numeric vs range), so deduct points.
- P-value: groundtruth has numbers, annotation says "<0.01" → similar issue.
- FDR: "<0.05" vs groundtruth's numeric values. So all three metrics have value format discrepancies. Deduct for each metric.

Analysis_5's metrics:

- Biological process terms are similar but phrased differently. Groundtruth: "immune response (GO:0006955)" vs annotation's "immune response" without GO terms. Maybe acceptable. The pathway entries also match in concept. So minimal deduction.

Analysis_7's results are new (not in groundtruth), so their accuracy isn't scored here.

Analysis_4's pearson r is unrelated to groundtruth's expected fold change etc., so not accurate. But since it's an extra analysis, maybe not counted here.

Analysis_6's result states no significant events, conflicting with groundtruth's features list. So that's an error. Deduct for that.

Calculating deductions:

Analysis_2's three metrics: each has value format mismatch. Maybe 10 points each? No, total 50. Let's say each metric (3) deducts 5 → 15 total.

Analysis_6's inaccuracy: 5 points.

Analysis_5's slight discrepancies: maybe 2 points.

Total deductions: 22 → 50-22=28.

So accuracy: 28.

Total Results Score: 10 (structure) +15 (completeness) +28 (accuracy)=53.

Wait, structure was 10, completeness≈15, accuracy 28 → 53 total.

But let me verify:

Structure: 10

Completeness: ~14.5 (rounded to 15)

Accuracy: 28.

Total 10+15+28=53.

Putting it all together:

Data: 10 + 30 (wait earlier thought was 80? Wait let me go back.

Wait for Data:

Structure:10, Completeness: 40 -10 (for extra data_2) =30, Accuracy: 50 -10 (omics and source)=40 → total 80.

Analyses: 10+13.3+30=53.3 → 53.

Results: 53.

Final scores would be Data:80, Analyses:53, Results:53. But let me recheck:

Wait for Analyses' completeness: I had 4 missing out of 6 analyses. 40 points total. 4/6 missing → 40*(2/6)=~13.3.

Accuracy: maybe I said 30. So 10+13.3+30≈53.3→53.

Results' completeness was 14.5, accuracy 28. So total 52.5≈53.

Thus, the final scores would be:

"Final Scores": {
    "Data": 80,
    "Analyses": 53,
    "Results": 53
}

But maybe I made some miscalculations. Let me confirm:

Data:

- Structure:10 (correct)
- Completeness: 40 -10 (for extra sub-object) =30
- Accuracy: 50 - (10 for omics and source) =40 → Total 80.

Analyses:

- Structure:10
- Completeness: 40 - (4 missing * (40/6≈6.66)) → 4*6.66=26.64 → 40-26.64≈13.36
- Accuracy: 50 - (analysis_1:5 + analysis_5:10 + dependency error:10 + etc) → maybe 50 - 25=25 → 25+13.36+10=48.36? Hmm, perhaps my earlier calculation was off. If accuracy is 25, then total 10+13+25=48. Maybe I need to adjust.

Alternatively, the accuracy deductions for analyses:

Analysis_1's label addition (5), analysis_5's wrong dependency (10), and analysis_6's incorrect comparison (5). Total 20. So 50-20=30. So total 10+13+30=53.

Yes, so keeping that.

Results:

- Structure:10
- Completeness: 40 - (7 missing * (40/11≈3.64)) →7*3.64≈25.5 →40-25.5≈14.5 →15
- Accuracy:50 - (Analysis_2's value formatting:15, analysis_6:5, analysis_5:2 → total 22 → 28) → total 50-22=28 → 10+15+28=53.

So the final scores are Data:80, Analyses:53, Results:53.

But maybe the user wants integer scores, so rounding appropriately.
</think>

"Final Scores": {
    "Data": 80,
    "Analyses": 53,
    "Results": 53
}